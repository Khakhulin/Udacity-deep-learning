{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "        batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295208 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "yatheajtllatrskhmep invotlq  q pi t laf ga sahnpknylpwfl ggrqaneoe ugtpmtaf p s \n",
      "ynhvajhiovfb dowdvwpoewm d xxeewxi x pbpa  sim azf e edp tghb b dvel mxsehn e ke\n",
      "pntjis ebjdtcxto ja gpbootnceupoowghydoycijjtdnfy jkmw izminq  q vosuranleav tog\n",
      "efnuaicetxeprafgbymdnnls sdnftlteteiydgohjnpqsjh g yfsntlailngtvnznnmwraeuer swe\n",
      "i wcgkhznzsicgcobohnoqd cauhtu e eehlkfrfmixu rvlrspy  eq cscuclgcjosttr neyjvzc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.588530 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.246915 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.094018 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.996125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.929049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.904875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.858147 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.817738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.827896 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.827739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "giee romery with the sjedice as defrremic is and will americal aceipling zeno of\n",
      "ing perilev of dores of nuctrakuovinory in aolly vid groininily heasuthic mo of \n",
      "hile have when haried of the lating of ma atorions artain on noqulles nice saace\n",
      "lies the the the nature three seven his brids of e to thind to speld fiaded or t\n",
      "legis incesfibuts pose migfes notic oill minsterm to promited isels it ledery wi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.773581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.753957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.734725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.750867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1500: 1.734888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.747994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.713807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.676606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.647259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.698557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "vine of jeaged sckovemallys hindufil the bornain bungances porsed useng anstrel \n",
      "coling the eiglonoss in a dive the parouse mouse hanautity parted the amses mija\n",
      "jem heid bown discision often itserent cited resectors and tandticislad sucheish\n",
      "gorage the gutyursts condictine romield in no be devited mastom dose for pos gat\n",
      "ming caroil it slassing jost inkent zero four one retumunne desse is landugts th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.688800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.684880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.645900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400: 1.662937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.684959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.656358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.656332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.650387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.651811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.650454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "nizal langoit it is froy sounce was bould sevon fors repable officially eaulic d\n",
      "zer the farchabon recolds their one nine seven two zero one nine zero melicyoses\n",
      "x used theory that to first lape decouxtly in auscak two at innose atsinced to f\n",
      "is do gro sceet thereally reuin landstac dome cassing to with dublation puncil r\n",
      "tive two there wayed hussity of blime freazius maracian indian is cacks exa and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.626096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.647592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3300: 1.636553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.670837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.664399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.667626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.644710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.643552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.635254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.652572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "scy rophist inclasse seliding from nomels discovering todby strupate compbered a\n",
      "han one nine th end the peaces blatt he descryefts suppen butly in the burlys la\n",
      "ired lipp agastagle is discencus meir caye one johia a rume anderwaberable murni\n",
      "ver mass gamesed part quevered shoug one nine nine byhind the king by event dist\n",
      "zer and ninger ecognen alboin the georate fromszion wold metic priece to are the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4100: 1.631786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.636934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.616193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.606888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.615679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.614758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.624560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.627901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.630480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.608328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "gan mutholh called a cain through and the sage hall after litisunian howish musi\n",
      "galled between the bed than be israbling and psypheile of s swith was masis inde\n",
      "k in one nine six eight one nine one six three zero one six five years cominal p\n",
      "emer first the mikam by distriptance others that killed thig consifted ston five\n",
      "d the with main pocts for oftenray this parces for chyple gang ar often mughts w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.603532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.587737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.576345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.579209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.566709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.577041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.567916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.580292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.572056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.543825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "mill dare posedent an armet of cert seven two zero with samal the tevired an and\n",
      "af the war as dolas was all da pendital or racote restraish and sueg thewe on th\n",
      "n six jasables heablines by confinition the quanters ponitary markex on the many\n",
      "on amac brash umis treen one nine seven nine six three bholow is a conno than th\n",
      "s and arny since actival word states reflocitian heak mangy is game otceen press\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.565541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.537013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.544169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.540127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.558921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.591132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.577313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.581632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.572044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "p gen g in an and distranamor pernabial like and typical articule john tox sh th\n",
      "tha suckele such days pisw in the litelell and populate the chablisms of the mor\n",
      "pen from a hilled the replyntlening emperor milleally it series see brouly as th\n",
      "y and an emeriarch that five dregal saching it is notes iboly waves one zero zer\n",
      "zer geo claimed byughtrick ensirension on neight sinning the qutles whatt whel h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(27), Dimension(256)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296088 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "gnyruvqeoxko oiz z hw ilv bpb kx lidyvglnpfdergeahl  nfsnavviqloeqlvsatisknrtadr\n",
      "o drqklnk gz gqi uhn qiigo kudqiilajlvw  n lk nm wedhe eiecarhxsqhlnt zteojcaoef\n",
      "dvjectz fnh kirlialdyaxebmcjetm tfibcdxh k iba li vqtglbd uidsooojc cuc did e  e\n",
      "weadhedhwsnlhwtlcqrfiallfr if qncuitr ea tpbnrewsduoseiffzislcocsr kekeswkqrttly\n",
      "n sbe kan icp vcfnt v v xcid  ozniqtmosoj ejzpeim mgc agwt  joenefucpafgdzp joll\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.578006 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.241599 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.080559 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.993168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.997385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.921496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.896213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.874717 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.863912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.795488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "x astin time the biede of knotiper stages ay masses diyy the hresm the beto it c\n",
      "f was didpacomore of purbon etceol and for opter plepular avreiked to a noveb se\n",
      "ricise orrenure one one seven in foullalic guen mut one nine ofter molugagraing \n",
      "her has as woin the fishce erees the retyed the cantucate maboroly cotuence erea\n",
      "zed in thisoldwoum codnive tic as a pardicens aytelsco of in the pautef extecks \n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1100: 1.771558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1200: 1.792623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1300: 1.776950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1400: 1.749590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1500: 1.738979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1600: 1.724243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1700: 1.744537 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.706883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.715391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.719643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "mal amedia per protectional lindent showed and gill broods houlations to crablia\n",
      "jendacucty s twok daugise by the valct fing thser shaneage bald ada pherem tees \n",
      " quath vollows on kitt serouss frectional diecan these geart as an aselicated ca\n",
      "jeed albymalianca deone afterly deam korterdest jistion of he fast adea of low h\n",
      "mentsers ameriated alterentter mides ellapitions reserice be ansechesed enation \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.704503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.679675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.684407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2400: 1.686753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.704629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.678998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.694463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2800: 1.654941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.658249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.669374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "lical isolan celler arcilural compare the lands assail existed by or the seallin\n",
      "maned aunor ablected houchlored in five verter bus the dissuriture making s la t\n",
      "ffer four mathorization of where likely of twu coulling smacle servivizally remo\n",
      "ned it daierc the coulder and arterts of lecks is haltter thermour eight nine th\n",
      "figelo colonender one nine nine nine nine zero shing hulta and emplicaping bed t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.657744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.653052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.637909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.637971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3500: 1.629433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.634803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.634721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.632055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.620987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.621898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "chillowale have moracic becagarth ollogave inditheme tain medicions offertiially\n",
      "kions composbats swenberly address brigh war heasy marschled charsherazafas the \n",
      "le keeving eayal to cleasite butchade compiticiesmilm hebbolly of the sectors th\n",
      "fac is the e breatius simpute they edito appirily reganast that mased and a migl\n",
      "re three one the german also headshasels ricemm gruith found to diffine american\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.619685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4200: 1.609960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4300: 1.591466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.618586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4500: 1.626669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.630820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.597774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.583371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.599418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.623120 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "s aduative of gthinamation seccent gomania atposes the offscy r for autor sha mo\n",
      "kf evely are moxitin increas and neater a ennekhione in perlip the to s policion\n",
      "ed the each hole artiures entile one five zero sell two five in the themen of so\n",
      "ys it wotter both puldimal adchand percent for space of the estimgy computatiect\n",
      "ns actomal chant holving such the cidian regiols internota the sade incree year \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.635124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.634271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.593521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.580340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.607977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.569673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.576006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.597034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6000: 1.563030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "o countly and of troagus alo is a relates such as a quilat sudpup vari and show \n",
      "jon he sive estension light however this one nine zero zero est in s linder can \n",
      "th ocrewist that by empioration for related tanchis is ituation from me fighteld\n",
      "b were the skempon inadapped of his cantrick immootal humprish publiornes upodes\n",
      " most featives wille by macles andsce and gemer to zen the for the mode sezen pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6100: 1.585241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6200: 1.599419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.611827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6400: 1.639336 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.635514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.604675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.594729 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.573748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.572458 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.584984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "hes his terry a that prish in can the lefeet who junding incies mempose returned\n",
      "nge sient also at a buri than naturally feftical greek universy one nine six lif\n",
      "ken is d known burde of into word notecer riggh nafcer assed rengrayly amminia t\n",
      "ra filled construenter he reloger eloperes the hervy s manum curled theorits the\n",
      "ods and edeciated two the color example succks si publishe of them during offect\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 7100: 1.555530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7200: 1.592411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7300: 1.601814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7400: 1.578915 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7500: 1.562399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7600: 1.589043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7700: 1.579928 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7800: 1.581206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7900: 1.586072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8000: 1.564950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "================================================================================\n",
      "os samora s coner soon of the be time taxim jondaria a invieence febtiar on scet\n",
      "rair of alsyor the foles intension ths exalaton and for andificid p de mechanism\n",
      "zades and offendon of the alcoce o through nat no chure relational rylimbus vide\n",
      "phonal the some to kuing longs west years purnts of a pursuit nowleduen of ave t\n",
      "t ageious of several of nites two six seven elead downame which on the midnas yi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8100: 1.571132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8200: 1.582282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8300: 1.591929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8400: 1.569817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8500: 1.567689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8600: 1.551054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8700: 1.562797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 8800: 1.588976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 8900: 1.581452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9000: 1.586009 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "per consimerisma general drween anchmance if sull eight sions four seven fill wh\n",
      "re contrives which they of game india attayen polind explication unifour but ame\n",
      "eish in the cluited assertious sometimes within critives k frolloo marulata scot\n",
      "by and imple the k one eight and nocle reart teperots of the san griffer the was\n",
      "zed imbrisis minctor soud that ansible clao ziskmis sbarletdery the world raxamm\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9100: 1.590926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9200: 1.595583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9300: 1.590074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9400: 1.573143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9500: 1.592387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9600: 1.581841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9700: 1.577665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9800: 1.577118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9900: 1.580554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10000: 1.597251 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ing may went from and de the sajound out a and torad are grany of lewneng into w\n",
      "orser erlywara the demored tom use an amonging than laceman in the grapked by th\n",
      "s others after geodx struelian statforts the ground pachice work societatoospfic\n",
      "pian fegen be game by madicatimmans scate shortanes and the runtil supported its\n",
      "beh markg from the jec his creymons over famurayils hold removyble conditional f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 10100: 1.565126 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 10200: 1.590093 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10300: 1.584949 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10400: 1.582896 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10500: 1.573755 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10600: 1.583306 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10700: 1.562903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10800: 1.560848 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 10900: 1.555347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11000: 1.588585 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "roughs on the film brit perconstion to he sect form s r coduce cre often and qui\n",
      "z well verquesthic official rocumber under where has a legacy adent are offerabl\n",
      "ists augnrising its happown law overpoussia alen for spected theadine therefull \n",
      "ng anahaphies to actor show the diel those of troal intublishos fort three tho a\n",
      "pirited of some created unior the ong starcy mal yack and the but lease roy an o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11100: 1.587801 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11200: 1.579634 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11300: 1.541607 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11400: 1.569012 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11500: 1.562717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11600: 1.557078 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11700: 1.573717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11800: 1.580734 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11900: 1.601813 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12000: 1.597944 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "zed with two show allow processosia as a one nine zero zero prepequence pignorsa\n",
      "y the under pow reevishing mankex one seven chillows ugnotia year s parof it to \n",
      "vimians such is tilspos forch and leftitsour south some trittulsot of is atril l\n",
      "phanz idels without supromussed governind jurnes schalrone similating a between \n",
      "que involver on wassfal weater of histiclats reference of the with the like stro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12100: 1.575750 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12200: 1.590594 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12300: 1.569764 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12400: 1.566402 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12500: 1.560497 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12600: 1.551376 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12700: 1.589705 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12800: 1.581224 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 12900: 1.588941 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 13000: 1.549479 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "quers exportained town does of the weenned by one seven two rulation hera two co\n",
      "variex in denawion xpy in one thromengies for two did put of whomery the country\n",
      "d fine fullion clowesh force thinkfore central ugs a light into suppsorey all fi\n",
      "zen numbers and the invent at dickather martagers linking here war at show chour\n",
      "xard were lown numbers allahoph errisken suka ppuce studed in one nine seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13100: 1.593628 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13200: 1.593347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13300: 1.577060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13400: 1.592521 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13500: 1.603572 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13600: 1.579617 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13700: 1.578702 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13800: 1.590926 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13900: 1.632133 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14000: 1.615489 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "coneand theory do on hemed teachw and defins two zero zero in west a loss by the\n",
      "s rot national name in the nument shack booic recalas european pechured is parti\n",
      "m ith darined is consucunds carg these as questor statum and in corpore the empl\n",
      "ter milkning or a histam their rause ownersite one nine eight five four stat hal\n",
      "ishing mated out convlems mornenon from forg had fearcis lavach calannisult hurr\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14100: 1.596736 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14200: 1.573053 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14300: 1.580519 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14400: 1.595691 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14500: 1.600329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14600: 1.570035 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14700: 1.592610 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14800: 1.574893 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14900: 1.592432 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 15000: 1.586098 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "des as that it day ten inchodence is the untogrites and mains other anba first m\n",
      "rap purreating the meupples oft devolution yet over is parode islands it still r\n",
      "valur been particularly of the causes different of the triffed by industric nall\n",
      "nn norve dease to as also of computer finnistric commanncement production of its\n",
      "orkage ip our screepon on one nine four films ireland of these american in great\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15100: 1.573534 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15200: 1.575539 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15300: 1.570134 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15400: 1.576043 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15500: 1.600311 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15600: 1.593948 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15700: 1.570269 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15800: 1.611362 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15900: 1.617383 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16000: 1.577232 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "way are by simpley one connicife of eight nine inine were inceed s by s eck spec\n",
      "ners american rock though northan initatesely as of byology aber purrdegations o\n",
      "ley opyp that air that sources pression me concerfine two zero s autax socially \n",
      "k by the in mijornism is ethornism hur charge skater as one two is stromping ope\n",
      " faile promoties merslar institite the one four midribivies and toward dowing ol\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16100: 1.570450 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16200: 1.566831 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16300: 1.608270 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16400: 1.591806 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16500: 1.566013 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16600: 1.567127 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16700: 1.579081 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16800: 1.595272 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16900: 1.552456 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 17000: 1.557044 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "jicalling of serving of a williorical point edust clins are b overel as general \n",
      "chnorece bill an athapuria one cable clown contablepomnody contembrally dungh in\n",
      "jornwa of the low play storea ligg a wings tver sean pilined directle in eurlibe\n",
      "zer headly into s the and arrikans micronizalations in is the made in the been o\n",
      "ubluel gremack sociation over fasta absour the mocars and terman the cultural th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: Cause of high demension in bigram model, we will use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 20\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "        train_labels.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for inputs in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input_idx = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input_idx)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 27)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_batches.next()[1]).shape\n",
    "(train_batches.next()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299145 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "oylgmcepa z fpvtywiumju   erw iboay cqer y voa sdc nidk us ql n a rcu tpanm xoxe\n",
      "klhlvbez empyziikatzpuiorc iwc wisuirghrtljrlm isen vuely  i vaypb  eteehhzsjpil\n",
      "rzkaoww ndlpomgfeg eotyueoanetze kr fimw  dbcn ez   efoexuupletyqthm  kdyzlcccqi\n",
      "clrsgyfditku o ttvd gh e z iiejrek  reiiaknnhr ueu e hiosh yeal bk qlso iomvzco \n",
      "khcpsaa odtjissm rravggbeixsq lqoq x    jlwltsecxxgvyo ayroh fbsvee aia in etisz\n",
      "================================================================================\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 100: 2.386494 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 200: 2.050725 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 300: 1.958449 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 400: 1.878871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 500: 1.835760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 600: 1.831754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 700: 1.800248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 800: 1.785890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.776740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1000: 1.756033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "ns mattenish the jeprest betill stuagly recopitaran elephised ciethnem impledact\n",
      "ments hark zero subsout articty working instanch hotro aneted ec vied anderewark\n",
      "dest sepathends ha on cetress liesis apperies then with of the stund mintirely s\n",
      "ressiol trantive of a stider blet there styt ie apbirse sixi one nine eight eihp\n",
      " veries then newsgest prosulty orgagenstos present senize s s the sted intenitio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1100: 1.754460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1200: 1.748571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1300: 1.726775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1400: 1.726786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1500: 1.727561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.702806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.702149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.673698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.692209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.704837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "gas by fiam wheer boops above seven yark sall and name carvice are falson and pe\n",
      "sern is jager bepen but three is and the bora and name in a sharge than or sense\n",
      "wifrer lub every person indiscient civiliond leveld spelfon one five five five o\n",
      "js apports one with and seen and bressent of supprotent six canamia an cater lif\n",
      "ne he used about seven zero ze of draweverl the sirah undernace of the proveriol\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.687647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.677252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2300: 1.683569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2400: 1.657063 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.660905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.664901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.659565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2800: 1.686172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2900: 1.661634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3000: 1.686554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bell respectual take sty all peomber stagn of one thrby the mecry see groupn isl\n",
      "ond while not relique lesignas uply it no fark ana gene exi one ni sound minity \n",
      "er meam of mudyling tourining clearates mones generage only their bik ups propin\n",
      "ov sig be premicia pusse of in ho moer which he neatric from chang janding in ot\n",
      "gest is don of his awarder ciese is island unor to or ofterne of hilly were orip\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.668552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.657001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3300: 1.629718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.643223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.659296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.638719 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.652325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.662170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.675188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4000: 1.652607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "yual and old und nnturys oulltian as voll and coblurgrate intendentencoly world \n",
      "ful the most s hellva and election s bond m they anks bill is bochsails ainspeno\n",
      "bands to pennyfo mis dame in dadical laures his chers informed source a boody mi\n",
      "vicul fabq one all aptem john bradia the analy dister for ammere of caulism one \n",
      "vers and luend a seven one mit ogrover shost to five four isso notations would o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4100: 1.655894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4200: 1.664900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4300: 1.639053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4400: 1.660346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4500: 1.636049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4600: 1.631538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4700: 1.627726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4800: 1.633210 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.634437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5000: 1.623997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ching the percession be kearges as they evil gpuce ordhlitipe to trivity this kh\n",
      "saured to the courquent as asternered as popel on films and lee while zero zero \n",
      "nap refirs staracted by selvcity use in anustimple in the creat mooking the one \n",
      "nicousivels vatels a macipmda withbrorizenter one sm pxt at a cans faman commits\n",
      "dize and to a states hes pace to it adone the formanistianly the the majored the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5100: 1.612377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5200: 1.586064 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.566667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.566389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5500: 1.562027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.613164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5700: 1.580942 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.591175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.586076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.578914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "acshitle ms other ravidual subsmen published a mysting one two mistorical north \n",
      "quation educally to no commus is gremights where war one nine seensane with doon\n",
      "domoson historited and exploiswantaclates states a free fick state allizy sane s\n",
      "prous then presidents founds compleri japoubbles of this germandurche distica im\n",
      "rouse asterneger and remains themosal pregutioned rality the civilization portab\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.558227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.571269 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300: 1.567709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.597509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.557523 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.577094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.591322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.550663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6900: 1.558134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.567879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "t and abblecution and high promia and cogropan for rucotforates the memanuan rec\n",
      "ker a construation or pose barrani of these and schoor of its of the vardimed yi\n",
      "be destancing for eventatinvenger distinction some habyen in the commannitionslo\n",
      "yberved diromians by higher accords are marvist which erigary labvement one nine\n",
      "hosal to jadlar it seaver programateriding and charvi zerood douced rugh lopting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7100: 1.586794 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7200: 1.568544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7300: 1.559103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7400: 1.575451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 7500: 1.586957 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7600: 1.567902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7700: 1.564885 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7800: 1.537815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7900: 1.559151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8000: 1.570239 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "xic instructiogual with attingly about the possed one simmate stricton land shg \n",
      "hoot films had and sone their willania in the emperor sahnt van cassemates peapd\n",
      "gayans between the lays was catakes the rights if timoshics by the elow from pos\n",
      "fecturositate with a fustroco series over palarsisculus gide and cape between gr\n",
      "ked the certed a propentene amveromy uncha strease as propuge rard in the his po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis=1)\n",
    "      feed_dict[train_labels[i]] = batches[i+1]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_idx: feed.argmax(axis=1)})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_idx: b[0].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b:**  Change graph for bigrams model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['nar']\n",
      "['rch']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 32\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        print(embed.shape)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    print(len(embeds))\n",
    "    print(len(train_labels))\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289089 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.82\n",
      "================================================================================\n",
      "wzkgfrmne gvme  ehgfah ykezsac teejk r  eypo nyhtvia  xxngdwyioy t nmaaodilnnswvc\n",
      "tbiprrpjz   ztkpscan hh woawatxxheovklocr   lepnzbwva eemkaridb w  ahictra i qe z\n",
      "qcc rxvloagsuw ssaifevcjnltctcehwp ofenygtnfgzl o  zbso afcsr iuwnthczv fxofesdax\n",
      "ds bisawakfgy p  wfcpkeo     ujdt eukdoktvylrb j pg d xawdg jxe dchkfbukfvyedcsto\n",
      "ixthpk oxnqanbtvmea bwplx me gtqjpwbtn tfgoearcg xiwz lh  nps vfphah suqgdoxxtprn\n",
      "================================================================================\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 100: 2.302889 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 200: 2.027483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 1.912729 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 1.855559 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 500: 1.808225 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 600: 1.827677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 700: 1.835468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 800: 1.815775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 900: 1.796186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 1000: 1.801397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "ccuuooueouuhiiiuuiueioaieeeuuueuitueuaeeieeeuoleaouuueeiuaeheuiieuroeiiuueoueioeu\n",
      "bveoeeeeyeeeieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      " jeeuoaauaeaauuinaeeaaahooararaeoaieahaoaoa uaacaraoioelaaauaoearraelaaeleaariaoa\n",
      " voaauaaiaaaaauaaaeie eaiaaeeeaiaaaeoeaieielieaeaeeaeaeeaeeeieiaiieaeeeeeeoeeaaea\n",
      "s abiaooaahmmsihbofmisdudoiacamooatttotskcaftiioobsambmbsmbmrmieopmnebbaddpsimfok\n",
      "================================================================================\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1100: 1.776139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1200: 1.753704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 1300: 1.783106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1400: 1.755056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 1500: 1.748084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1600: 1.743098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1700: 1.736587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1800: 1.751845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1900: 1.730856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2000: 1.735253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "fy pm lnpmcpqtrrnrutnprnzn tnlnmprnrntlpwnetcprlcnpnsntrnntpcmlnpttppntr r rumpls\n",
      "hfr     eo i r  rr    io  ri i ii rro  rriier ro oriii i riiooioi  oorrr im  iiro\n",
      "zroeeieieoeeeoooeoeooeoooeoeooeeeooeeeeeoeeooooioiooeooeeieoeeiaeeeeeoooooooioooe\n",
      "nkeies e sei seeeeyi ee e  s  saeee eae se   nisiiieee   emees  oi i a  l ee eee \n",
      "tonbmm ammgmm nms s mmprirmrgs b r rs sbsombr  ml   mbi mmbrm rbyaisrmb  mmsndrms\n",
      "================================================================================\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 2100: 1.721529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2200: 1.720902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 2300: 1.749266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2400: 1.751861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2500: 1.726976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2600: 1.713863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 2700: 1.715788 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 2800: 1.716926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 2900: 1.730110 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 3000: 1.749124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "q limpgcnmsmeccwrmeodijsmwyaittmspbrmtacgptealhmhrsachlccltmhbheommrhyoiaoaihalsh\n",
      "us       i  s        iu         o    a              ii o       u         c       \n",
      "vwb    enr  o o     o       ro    h              i  o   o     ooo       i        \n",
      " innnnrmnnnnnnnnnnnnnnnnnnnnnnnnmnnnnnnnnnnnsnnsnnnnnnnnnnnnd nnnnnnntns nnnnjnnt\n",
      "fj    si c   m   o  uwm      u c wh rhr   ssu  ihs usoau m s   mss ssocsup s wu  \n",
      "================================================================================\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3100: 1.767456 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 3200: 1.753534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 3300: 1.741110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 3400: 1.738429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3500: 1.756102 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 3600: 1.756591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 3700: 1.784526 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 3800: 1.716554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 3900: 1.732743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 4000: 1.743809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "cjuaeoaaoooluiusruirluaauuuerousvuuuuuuluuuuuuluuuvfuruiuuiauusuuuuiur uuiuumuurm\n",
      "gs         t                                       i                             \n",
      "o eamavpaprsprdcvjuoiproaujhvftmrrfcorltadcbrtrlrmnmigtriwcsarlliadcemmrauhfpowsp\n",
      "gs    t           tt                                         f e                 \n",
      "oji oe aai iee iuuiaiouaaeuheeeieimooeeiieu a yaai sieiaa asiuuiaaeeuiua euueoeei\n",
      "================================================================================\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 4100: 1.745477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 4200: 1.728003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 4300: 1.745660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 4400: 1.752761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 8.14\n",
      "Average loss at step 4500: 1.744508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 4600: 1.737338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 4700: 1.701295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 4800: 1.736424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 4900: 1.739173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 5000: 1.724270 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ce mvrlplpmipr rpdlppnlprprmpprdnbrpnrlrdndpllsrrtnmdmdrrvdsrn mrpdpdrpsllvpsdrdl\n",
      "xdoeie errirerm em me reuemeuom  iu ime meee  im eeeiemmu im me  leieem   o ml mi\n",
      "kbimaaneaaaauaelaaltamaeaauaeaoaeuaaomealleura laaonloarraaaelaaaraornepaammaarar\n",
      "xmaesasaneaaaceeaalalsiaaaepaaaddeaasapasaaaaoaeeapaassdeaaaalareaaeaaaaaaadgpea \n",
      "mnoia eaensaoauoaoo auocaia  aaoooauioioaeiooooauouuooo sooaueoueoaopapwoaeoaaaeo\n",
      "================================================================================\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 5100: 1.703443 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5200: 1.678977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 5300: 1.694666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5400: 1.660934 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5500: 1.662414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 5600: 1.667233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5700: 1.654800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5800: 1.659663 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5900: 1.660582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 6000: 1.671689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "fg e  hhheho hhhh h  h hhh h  hhoh   hh hh h   heehhhhh    h   h hhse h hh eh hhh\n",
      "pd  aeaaeaeaeeaeeeeeaeaoeaeaioaoeeeoooaooieoiioooooeaieeaeoeoooeoooa oeeoooooeoao\n",
      " unnsspswsssnnsncnsnstpnssssssnnsgnsssnnscnnnssnmnnpswsnnknssnslsbnnnpnsnsnnwscsp\n",
      "ihao  aaeaaa  a a aaa  a    aaaa  a aa  ea   a aaaaae     aan   aa  a aac nara   \n",
      "vm  e  aeo    aa e   aeeae   aa     aa a eseaee i     eea  esea s me  ae sa ee ao\n",
      "================================================================================\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 6100: 1.674793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 6200: 1.687196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 6300: 1.694800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6400: 1.668747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6500: 1.634569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 6600: 1.645372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6700: 1.658347 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 6800: 1.664987 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6900: 1.665253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7000: 1.645361 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "ss isei eiiiieieeii  i ii i io iii eioiiieiii ef ae iiiei  ieaeieiiiiiie iuaiiiii\n",
      "illdiluyauudadadteyudiehrdayyelliauayiyr ds yaeyyy  uytaeuyiruttilrdayyuteeu uttu\n",
      "ux                        i     e              i            i                    \n",
      "cbilieeileaaibeebnlbmyylyyismbnlayiiilbyiiaeayueieililnriineleua aybyybiiiiioeola\n",
      "psi  oe             i e             yi     o      e                i         o   \n",
      "================================================================================\n",
      "Validation set perplexity: 7.70\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[0].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[1].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input0: b[0].argmax(axis=1),\n",
    "                                              sample_input1: b[1].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c:** Add dropout layer to output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "embedding_size = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.7)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        output_drop = tf.nn.dropout(output, 0.7)\n",
    "        outputs.append(output_drop)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibe']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.382030 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.43\n",
      "================================================================================\n",
      "ef o a rr mvm oi fcpj psesl imesr aw rc   j eu nrlhupsw kq xy vj k  ie w t t ll r\n",
      " ol et ze n k gao vhehonia  o qyebt ndfxt sa bd qfbx qphqd yg  j knoy evfnq arbmj\n",
      "myt w de k sfva ty  c aert n vx g myry a y qcj gea xq lme w t c fkr uuefg j  krkt\n",
      "bai it w nfii  i qi rt sg   a  suo qsg sa il mdneulrei zw io gw dv  o rk  phc  mn\n",
      "lceb e at i f g ebk   a ohim kdk  y lterfoa w wulq km nyq b ovj ks  j zk cagu mge\n",
      "================================================================================\n",
      "Validation set perplexity: 38.10\n",
      "Average loss at step 100: 2.626747 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.21\n",
      "Validation set perplexity: 10.53\n",
      "Average loss at step 200: 2.337547 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.57\n",
      "Validation set perplexity: 9.21\n",
      "Average loss at step 300: 2.235834 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 400: 2.171309 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 500: 2.144766 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 600: 2.117988 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.09\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 700: 2.073055 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 800: 2.049127 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 900: 2.060903 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 1000: 2.035523 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "================================================================================\n",
      " fum tratity forme jort end chars of ruce yuld in three for formench mative macko\n",
      "ffch pany hiks refs who ficts to bacter andrany is on and and gotack of apricat t\n",
      "bsolled formacald is fassontry i net caribars pact flesed catany three sumervalgi\n",
      "qbareme the nine six sought uss vaction of clan apardid fight worle ally miclen l\n",
      "fection the eimaimeter also poines aptatpletent he dophe soum of the nine to schw\n",
      "================================================================================\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 1100: 2.014350 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.70\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 1200: 2.018449 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 1300: 1.968035 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 1400: 1.984473 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 1500: 1.990996 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1600: 1.985853 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 1700: 1.970402 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 1800: 1.960118 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1900: 1.966145 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2000: 1.943224 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "================================================================================\n",
      "oque ald his the two six reqies one four the zero his ballso iresidanist by a tha\n",
      "ujurical s atuince eathis of doemp afferen imassol one the stives legor his of se\n",
      "fzactured me wous alaw smallop in taming divility opbed by onesaby who a in a his\n",
      "ds unite seven of ifertacity cifeate the in hod kyoysho she onal cator hover west\n",
      "cident a c the fish and of the atd five one six the agarcidylic the impermility w\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2100: 1.950524 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2200: 1.913135 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2300: 1.912210 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2400: 1.908296 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2500: 1.909459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2600: 1.891275 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2700: 1.906708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2800: 1.895041 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2900: 1.865176 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 3000: 1.902457 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "================================================================================\n",
      "ufficitations a john in are consitic game or amic shup procry they of ocary for h\n",
      "xpained is in yhard confultum leadds day mations abst that and his to chadt husen\n",
      "wcitical puers baruming trop for is a nine yaudheir the sip or shand crapolition \n",
      "hbanatios ralegic in havems infourting rusures and pad provater have to libration\n",
      "qjunitive tussucted treficiation breen moning by encially abiumiquies uriafties t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 3100: 1.901018 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3200: 1.864992 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3300: 1.846519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 3400: 1.860066 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3500: 1.860518 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 3600: 1.849644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 3700: 1.838713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3800: 1.845104 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3900: 1.868213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4000: 1.865225 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "================================================================================\n",
      "lz sking with dom stulgalic srectice usas eaderical reach move inled refpiendid n\n",
      "fbous the marrook ratematic of carrividaty into hat by arres of for group mant th\n",
      "  saven s freewis name rebristances inflay camply i  made that ity herum stence w\n",
      "ujary the cate of stari a hurrencaperal ny jon infort clikey vais who hich one ne\n",
      "qlead seven the fame duscurry and on gur or me capprive ex six tralave ssatoriati\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[-2].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[-1].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input1: b[1].argmax(axis=1),\n",
    "                                             sample_input0: b[0].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adic and om', 'ks journals', 'g inn g is ', 'itions were', 'antiprism w', 'ragon ball ', ' for his sh', 'eference in', 'dclapped th', 'prettanikee', 'ng some str', 'le on top o', ' as the tre', 'anks buildi', ' lead in a ', ' nine nine ', 'g in the u ', 'g the tourn', ' arcology n', 'unlike toda', 'mosomes vol', 'ial plasma ', 'h the use o', 'six zero s ', 'rgagni ital', 'n the late ', 'he victim c', 'an be const', 'ism secular', ' case the s', ' maker leo ', 'ended impro', 'ut failed t', 'sed on the ', 'ed at sea s', 'ense that o', 'f march fiv', 'tor kenneth', 'horror movi', 'es use the ', 'percard as ', 'ld reaching', 'was soon ca', 'g monster a', 'overnments ', 'ing machine', 'cannot say ', 'to young wh', ' eight four', 'author john', 'learly seen', 'ed to the a', 'cement of e', 'ion the rev', 'neural netw', 'press that ', 'underground', 'o this text', ' since one ', 'her modes g', ' for other ', ' a role dra', ' zero four ', 'ight zero s']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramsBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            first = self._text[self._cursor[b]]\n",
    "            if self._cursor[b] + 1 == self._text_size :\n",
    "                second = ' '\n",
    "            else :\n",
    "                second = self._text[self._cursor[b] + 1]\n",
    "            batch[b, char2id(first) * vocabulary_size + char2id(second)] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigramCharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c//vocabulary_size)  + id2char(c%vocabulary_size) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigramBatches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigramCharacters(b))]\n",
    "    s = [x[0::2] for x in s]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramsBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramsBatchGenerator(valid_text, 1, 1)\n",
    "print(bigramBatches2string(train_batches.next()))\n",
    "print(bigramBatches2string(train_batches.next()))\n",
    "\n",
    "print(bigramBatches2string(valid_batches.next()))\n",
    "print(bigramBatches2string(valid_batches.next()))\n",
    "print(bigramBatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramSample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] \n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.7)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        output_drop = tf.nn.dropout(output, 0.7)\n",
    "        outputs.append(output_drop)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = (tf.placeholder(tf.float32, shape=[1, bigram_size]))\n",
    "    \n",
    "    sample_input_embed=tf.nn.embedding_lookup(embeddings,  tf.argmax(sample_input, dimension=1))\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.625717 learning rate: 10.000000\n",
      "Minibatch perplexity: 754.25\n",
      "================================================================================\n",
      "qtfdcnxjjsvmempawjrugunscwgsfadoaxrmjcsznnnexwodlb rpwtteqzjvuaskqyohtssrotzracd \n",
      "eaoaqyqqqtthgbghovnoklqeujzmgsljimsizocdegdiiycmsgltosbkqsqe  juupuxlf mtzymc hox\n",
      "pl h tmhzalgenkfshotxbfzpkjtqdterylb  dvvmotmhdjkhvnrtmkfetqkdjmvtuvovzjgdxyhxxgq\n",
      "zxuajdunsupyjsorwm mrxvkqfnpkx ifaauxaetrezmbcgphoclrrmz jjhetvmwxsnpekjvsxadnpmh\n",
      "mtnjbcxy kakvbvacnvqbmazsybugafuu sydyhxaqtqhzvnizsxrnahyfzspkfikvrnkomhr gxevarv\n",
      "================================================================================\n",
      "Validation set perplexity: 646.48\n",
      "Average loss at step 100: 3.721513 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.81\n",
      "Validation set perplexity: 12.38\n",
      "Average loss at step 200: 2.405352 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.11\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 300: 2.259583 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 400: 2.117846 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.19\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 500: 2.069386 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 2.011264 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 700: 2.003808 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.988492 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.55\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.923224 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.901478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "================================================================================\n",
      "wu orch as and grate and suill eveld miss for fannisters exportal absecity trandi\n",
      "afer work facessuars own bile s a use or natiationally depted two zero zero four \n",
      "xl rusts three deftly deven and exported trem strorges are goternes gine sevent t\n",
      "galics at s provelogic decive three goous has by of the or khoough nine five ex s\n",
      "yorges to been exchose such it dep mstatory or which have mashary two oods truity\n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1100: 1.895293 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.935607 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.882132 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.879092 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1500: 1.881786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.887357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.821205 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.818994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1900: 1.846053 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2000: 1.826049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "cvory basque enylic in the capian senides schipictive growly which rosed the enkh\n",
      "iquently bis capistanica delearie excidifativem pan is of the the is weres decron\n",
      "ist maint by state two the probstruction factrificle was hero four seven have riv\n",
      "risition cal pross ances by win humaps expelions sas uppers the cedery is four ev\n",
      "on univers dey anatimple eding three the parped eight eight eight seven zero zero\n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.810442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.811508 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.801336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2400: 1.836576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2500: 1.802778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.808648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.818669 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.810343 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2900: 1.780158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3000: 1.795228 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "================================================================================\n",
      "my officiales speen is a verg countanic contraigo bouth the of the during dunk fo\n",
      "ectumentics of the clim of s rovialf progrenced by univer quila dypes base godern\n",
      "bvent scity systeting albumer of setern rooing which grosprit and it grouction am\n",
      "clopes six one one nine nine two zero m v ageneroce to ob see of of bluction and \n",
      "fs conts in teden to the pol cavitely namerive o for cause five or for ough dicia\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3100: 1.788626 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.820993 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3300: 1.791421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.804681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.763852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.769251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3700: 1.787372 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3800: 1.772152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3900: 1.770461 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.748770 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "hs objecth lember of colack the rivorptart who action and the ader smite for noma\n",
      "kqians region soung nonly nation rost four collitic er the and it systems mohor w\n",
      "sly radition however more a gavious felleorders so one nine seven years sundref s\n",
      "bmber one fagam commany one nine eight six four nine marian he ful many fer celle\n",
      "ek salepers estace the one five this in six seven ley rhook reea he which muminin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4100: 1.753341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.754904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4300: 1.758026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400: 1.751002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4500: 1.743036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4600: 1.737592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.741650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4800: 1.743987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.733613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5000: 1.713315 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "cond intensimic nonly overries and currel equipition amurary see and on it patemp\n",
      "ic tetch are four me one nine eight con the promicil hainst station of airn it me\n",
      "wish he titary selfronon of had to two two zero famic line were mitten to boditio\n",
      "pace one one nei two zeroo zero one nine six two shaude know by that noft iving i\n",
      "xrhancine the conside the depetest which the bourray by hest of spaces poxic dope\n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5100: 1.738318 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5200: 1.738346 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5300: 1.736808 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5400: 1.711470 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5500: 1.699603 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5600: 1.707417 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5700: 1.735175 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5800: 1.764228 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.757051 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6000: 1.720191 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "wzourt libreshed nation to and estates of one eight five one screes definces rund\n",
      "pnad god s have ganistelts about and water and civilica theres the broating the b\n",
      "cets be arve asseath which of and fishetized striber of him sun storives one one \n",
      "ualia invents relar ats of faction contenne one three his ease of ammunded goale \n",
      "lwas the essiple this from phich ascoitle recordal as any of has a cossible and i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6100: 1.715734 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6200: 1.719862 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6300: 1.737822 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6400: 1.694072 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.701996 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.713963 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.684008 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6800: 1.705068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6900: 1.721193 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.731440 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "ts known yorder to his operdian funderal five enclart the in facts with majortan \n",
      "wn and griverce bet the deen munsix eight zero zero zero five rifiet a partica sp\n",
      "wbands which an expertiates and rote pay a realation the sman ba at in councings \n",
      "ust in projemense the autoted examplee the game encomposer or preduces can and on\n",
      "kxs communess one two three zero maled namellable in lassan the equation to s str\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "\n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = bigramSample(bigram_random_distribution())\n",
    "          sentence = ''.join(bigramCharacters(feed)[0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = bigramSample(prediction)\n",
    "            #print(\"prediction: \\n feed {0}:{1} \\n\".format(bigramsCharacters(feed)[0][1], bigramsCharacters(feed)[0][3]))\n",
    "            sentence += bigramsCharacters(feed)[0][3]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text,1, 2)\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism']\n",
      "[' msihcrana']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class SequenceBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    \n",
    "    def _next_batches(self):\n",
    "        \"\"\" Generate a batches of appropriate size\"\"\"\n",
    "        batches = []\n",
    "        for step in range(self._num_unrollings):\n",
    "            batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "    \n",
    "    def _mirror(self, sequence):\n",
    "        \"\"\"Mirror every word in the sequnce \"\"\"\n",
    "        mirror_sentence = []\n",
    "        for word in sequence.split(' '):\n",
    "                mirror_sentence.append(''.join(reversed(word)))\n",
    "        return ' '.join(mirror_sentence)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate two next arrays of batches from the data.One for the encoder and another for the decoder.\n",
    "        The array consists of the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        enc_batches = self._next_batches()\n",
    "        dec_batches = self._next_batches()\n",
    "        for b in range(self._batch_size):\n",
    "            cursor = self._cursor[b]\n",
    "            sentence = self._text[cursor:cursor + self._num_unrollings]\n",
    "            mirrored = self._mirror(sentence)\n",
    "            for (i, (s, rev_s)) in enumerate(zip(sentence, mirrored)):\n",
    "                enc_batches[i][b, char2id(s)] = 1.0\n",
    "                dec_batches[i][b, char2id(rev_s)] = 1.0\n",
    "            self._cursor[b] = (cursor + self._num_unrollings) % self._text_size\n",
    "        return (enc_batches, dec_batches)\n",
    "        \n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "valid_batches = SequenceBatchGenerator(valid_text, 1, num_unrollings)\n",
    "train_batches = SequenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "enc, dec = valid_batches.next()\n",
    "print(batches2string(enc))\n",
    "print(batches2string(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stsi covda', 'yra nrevog', 'seh noitan', 'd retsanom', 'acar cnirp', 'drahc reab', 'lacigr nal', 'rof nessap', 'eht noitan', 'koot ecalp', 'reht llew ', 'neves xis ', 'hti a solg', 'ylbabor eb', 'ot ingocer', 'deviec eht', 'tnaci naht', 'citir fo t', 'thgi ni is', 's desuacnu', ' tsol sa i', 'ralullec i', 'e ezis fo ', ' mih a its', 'sgurd fnoc', ' ekat ot c', ' eht seirp', 'mi ot eman', 'd derrab a', 'dradnats f', ' hcus sa e', 'ez no eht ', 'e fo eht o', 'd revih no', 'y thgie am', 'eht dael c', 'se cissalc', 'ec eht non', 'la isylana', 'snomrom eb', 't ro ta el', ' deergasid', 'gni metsys', 'sepytb sab', 'segaugna t', 'r issimmoc', 'sse eno in', 'xun esus l', ' eht tsrif', 'iz tnecnoc', ' yteicos n', 'ylevitale ', 'skrowte hs', 'ro tihorih', 'lacitil ni', 'n tsom fo ', 'oodreksi r', 'ci eivrevo', 'ria nopmoc', 'mo mnca ca', ' nilretnec', 'e naht yna', 'lanoitoved', 'ed hcus ed']\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n"
     ]
    }
   ],
   "source": [
    "d = train_batches.next()\n",
    "print(batches2string(d[1]))\n",
    "print(batches2string(d[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    encoder_train_inputs = []\n",
    "    decoder_train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        encoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        decoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    # State\n",
    "    # Bias\n",
    "    \n",
    "    #Encoder\n",
    "    x_enc = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_enc = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_enc = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    #Decoder\n",
    "    x_dec = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_dec = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_dec = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    encoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    decoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    decoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell_encoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_enc)+ tf.matmul(o, m_enc) + bias_enc\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell_decoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_dec)+ tf.matmul(o, m_dec) + bias_dec\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop of encoder.\n",
    "    outputs = list()\n",
    "    for input_encoder in encoder_train_inputs:\n",
    "        encoder_output, encoder_state = lstm_cell_encoder(input_encoder, encoder_output, encoder_state)\n",
    "    \n",
    "    decoder_output = encoder_output    \n",
    "    decoder_state = encoder_state    \n",
    "    for input_decoder in decoder_train_inputs:\n",
    "        decoder_output, decoder_state = lstm_cell_decoder(input_decoder, decoder_output, decoder_state)\n",
    "        outputs.append(decoder_output)\n",
    "    \n",
    "    # Classifier.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_inputs = []\n",
    "    for _ in range(num_unrollings):\n",
    "        sample_inputs.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    \n",
    "    encoder_sample_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    encoder_sample_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    for input_encoder in sample_inputs:\n",
    "        encoder_sample_output, encoder_sample_state = lstm_cell_encoder(input_encoder,\n",
    "                                                            encoder_sample_output, encoder_sample_state)\n",
    "    \n",
    "    decoder_sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    decoder_sample_state = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    decoder_sample_output = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    sample_output, sample_state = lstm_cell_decoder(decoder_sample_input, decoder_sample_output, decoder_sample_state)\n",
    " \n",
    "    with tf.control_dependencies([sample_output,\n",
    "                                sample_state]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ructures a']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(valid_batches.next()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Average loss at step 0: 3.297160 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "Input:\n",
      " nihilism or anomie but rather a harmonious anti a\n",
      "Reverse input:\n",
      " msilihin ro eimona tub rehtar a inomrahsuo itna a\n",
      "Output:\n",
      "                                                  \n",
      "step\n",
      "Average loss at step 100: 2.682217 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.45\n",
      "Average loss at step 200: 2.454882 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.72\n",
      "================================================================================\n",
      "Input:\n",
      "uthoritarian society in place of what are regarded\n",
      "Reverse input:\n",
      "iratirohtuna yteicos ni ecalp fo tahw rae dedrager\n",
      "Output:\n",
      " seht eht  seht eht  seht eht  seht eht  seht eht \n",
      "Average loss at step 300: 2.351906 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.01\n",
      "Average loss at step 400: 2.276232 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.38\n",
      "================================================================================\n",
      "Input:\n",
      " as authoritarian political structures and coerciv\n",
      "Reverse input:\n",
      " sa rohtuanairati oplacitil tsserutcur adn vicreoc\n",
      "Output:\n",
      " sa si sa  sa si sa  sa si sa  sa si sa  si si sa \n",
      "Average loss at step 500: 2.238947 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.59\n",
      "Average loss at step 600: 2.208372 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "================================================================================\n",
      "Input:\n",
      "e economic instituti anarchism originated as a ter\n",
      "Reverse input:\n",
      "e cimonoce itutitsni msihcrana etanigirod sa a ret\n",
      "Output:\n",
      " eno enin  orez onin eno eno ie eno eno e eno eno \n",
      "Average loss at step 700: 2.156267 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Average loss at step 800: 2.098095 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "================================================================================\n",
      "Input:\n",
      "m of abuse first used against early working class \n",
      "Reverse input:\n",
      "m fo esuba tsrif esud tsniaga ylrae krowgni ssalc \n",
      "Output:\n",
      "eht eht raeht eht rathgie ot teht eht ranoitar tah\n",
      "Average loss at step 900: 2.089467 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Average loss at step 1000: 2.045354 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "================================================================================\n",
      "Input:\n",
      "radicals including the diggers of the english revo\n",
      "Reverse input:\n",
      "slacidar ignidulcn teh sreggid fo eht nehsilg over\n",
      "Output:\n",
      "yllaita a noitano ehe eht eht  eht eno esel ni a a\n",
      "Average loss at step 1100: 1.945742 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Average loss at step 1200: 1.874420 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "Input:\n",
      "lution and the sans culottes of the french revolut\n",
      "Reverse input:\n",
      "noitul dna eht snas settoluc of eht nerfhc tulover\n",
      "Output:\n",
      "noita dna  eht si eheht si ehte eht eno eht rehto \n",
      "Average loss at step 1300: 1.810557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 1400: 1.773435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "Input:\n",
      "ion whilst the term is still used in a pejorative \n",
      "Reverse input:\n",
      "noi tslihw eht mret si llits udes ni a pevitaroje \n",
      "Output:\n",
      "noit si ot thgie ehtsi a itartsdn dna taetaro itam\n",
      "Average loss at step 1500: 1.730570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 1600: 1.669779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "Input:\n",
      "way to describe any act that used violent means to\n",
      "Reverse input:\n",
      "yaw ot sedebirc yna tca taht udes neloivt snaem ot\n",
      "Output:\n",
      "ya eht tsoecirg ni etcat htiw des ni evot sno eht \n",
      "Average loss at step 1700: 1.613710 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 1800: 1.552456 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "Input:\n",
      " destroy the organization of society it has also b\n",
      "Reverse input:\n",
      " yortsed teh zinagronoita fo syteico ti sah osla b\n",
      "Output:\n",
      " yrotsed teh orez innoitac ot ytinif ehtsela hcus \n",
      "Average loss at step 1900: 1.495892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Average loss at step 2000: 1.512178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "Input:\n",
      "een taken up as a positive label by self defined a\n",
      "Reverse input:\n",
      "nee nekat pu sa a opevitis balle yb fles denifed a\n",
      "Output:\n",
      "eno enin rsa orp acue si a cilyl eno eht denidem a\n",
      "Average loss at step 2100: 1.490419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Average loss at step 2200: 1.455780 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.88\n",
      "================================================================================\n",
      "Input:\n",
      "narchists the word anarchism is derived from the g\n",
      "Reverse input:\n",
      "stsihcran eht drow amsihcran is devired morf eht g\n",
      "Output:\n",
      "seitartsi eht drow asih ram iee evis erifo eh eht \n",
      "Average loss at step 2300: 1.417098 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Average loss at step 2400: 1.404327 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "================================================================================\n",
      "Input:\n",
      "reek without archons ruler chief king anarchism as\n",
      "Reverse input:\n",
      "keer ohtiwtu snohcra relur ihcfe gnik namsihcra sa\n",
      "Output:\n",
      "eht orez wth taluoc  reht lliweht gniw amahcsir am\n",
      "Average loss at step 2500: 1.398887 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Average loss at step 2600: 1.371318 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.73\n",
      "================================================================================\n",
      "Input:\n",
      " a political philosophy is the belief that rulers \n",
      "Reverse input:\n",
      " a citilopla osolihpyhp si eht feileb htta srelur \n",
      "Output:\n",
      " a iloticalla oitsocytic eht s eht elihwtah retsul\n",
      "Average loss at step 2700: 1.360595 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.22\n",
      "Average loss at step 2800: 1.343471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.09\n",
      "================================================================================\n",
      "Input:\n",
      "are unnecessary and should be abolished although t\n",
      "Reverse input:\n",
      "era ecennuyrass dna dluohs eb dehsiloba hguohtla t\n",
      "Output:\n",
      "eht eno irsyard s taseluoht dnselba dlohthguol ta \n",
      "Average loss at step 2900: 1.329664 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.68\n",
      "Average loss at step 3000: 1.325979 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.84\n",
      "================================================================================\n",
      "Input:\n",
      "here are differing interpretations of what this me\n",
      "Reverse input:\n",
      "ereh era dgnireffi itaterpretnsnoi fo hwta siht em\n",
      "Output:\n",
      "ereht dra gnitrof i tnemertnu snoi fo httsa eht im\n",
      "Average loss at step 3100: 1.291150 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Average loss at step 3200: 1.309048 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.91\n",
      "================================================================================\n",
      "Input:\n",
      "ans anarchism also refers to related social moveme\n",
      "Reverse input:\n",
      "sna hcranamsi osla rsrefe ot rdetale coslai emevom\n",
      "Output:\n",
      "sna hcarafsma llops sretfo rofdetalec oflai emoced\n",
      "Average loss at step 3300: 1.293923 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.14\n",
      "Average loss at step 3400: 1.309673 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.64\n",
      "================================================================================\n",
      "Input:\n",
      "nts that advocate the elimination of authoritarian\n",
      "Reverse input:\n",
      "stn taht aetacovd hte tanimilenoi fo tuanairatiroh\n",
      "Output:\n",
      "stn taht aecite f tie inailem noi fo ta ratini oht\n",
      "Average loss at step 3500: 1.284384 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.59\n",
      "Average loss at step 3600: 1.283273 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.30\n",
      "================================================================================\n",
      "Input:\n",
      " institutions particularly the state the word anar\n",
      "Reverse input:\n",
      " itutitsnisno citrapylralu eht etats eht drow rana\n",
      "Output:\n",
      " ti tnisnosno aridnuyllaru eht etats eht dna row r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3700: 1.267704 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.73\n",
      "Average loss at step 3800: 1.256064 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.56\n",
      "================================================================================\n",
      "Input:\n",
      "chy as most anarchists use it does not imply chaos\n",
      "Reverse input:\n",
      "yhc sa somt sihcranast esu ti seod ton iylpm soahc\n",
      "Output:\n",
      "yce sa fo t sahcran se tsus ehsedo no tiylpm sa fo\n",
      "Average loss at step 3900: 1.258813 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.36\n",
      "Average loss at step 4000: 1.248739 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.46\n",
      "================================================================================\n",
      "Input:\n",
      " nihilism or anomie but rather a harmonious anti a\n",
      "Reverse input:\n",
      " msilihin ro eimona tub rehtar a inomrahsuo itna a\n",
      "Output:\n",
      " sinimil hro enom intub rehtar namrof iasuo na a a\n",
      "Average loss at step 4100: 1.240694 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.31\n",
      "Average loss at step 4200: 1.226048 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.31\n",
      "================================================================================\n",
      "Input:\n",
      "uthoritarian society in place of what are regarded\n",
      "Reverse input:\n",
      "iratirohtuna yteicos ni ecalp fo tahw rae dedrager\n",
      "Output:\n",
      "rot tirutcna etalcif enin ecalfo rah tahe deredar \n",
      "Average loss at step 4300: 1.214264 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.55\n",
      "Average loss at step 4400: 1.213556 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.52\n",
      "================================================================================\n",
      "Input:\n",
      " as authoritarian political structures and coerciv\n",
      "Reverse input:\n",
      " sa rohtuanairati oplacitil tsserutcur adn vicreoc\n",
      "Output:\n",
      " sa rof htnaitarop olacitis fosretcur a dn civerpm\n",
      "Average loss at step 4500: 1.213433 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Average loss at step 4600: 1.212699 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.56\n",
      "================================================================================\n",
      "Input:\n",
      "e economic instituti anarchism originated as a ter\n",
      "Reverse input:\n",
      "e cimonoce itutitsni msihcrana etanigirod sa a ret\n",
      "Output:\n",
      "e nommocc  etitnuocs naciremac enin retxd rae taht\n",
      "Average loss at step 4700: 1.218321 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.21\n",
      "Average loss at step 4800: 1.205003 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.49\n",
      "================================================================================\n",
      "Input:\n",
      "m of abuse first used against early working class \n",
      "Reverse input:\n",
      "m fo esuba tsrif esud tsniaga ylrae krowgni ssalc \n",
      "Output:\n",
      "m fo eb ts strof ehtd snaitan ylrae orpygni selac \n",
      "Average loss at step 4900: 1.209397 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.29\n",
      "Average loss at step 5000: 1.176852 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.17\n",
      "================================================================================\n",
      "Input:\n",
      "radicals including the diggers of the english revo\n",
      "Reverse input:\n",
      "slacidar ignidulcn teh sreggid fo eht nehsilg over\n",
      "Output:\n",
      "seilacir dgnidulcn teh egries  fo eht nehgils over\n",
      "Average loss at step 5100: 1.163284 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.24\n",
      "Average loss at step 5200: 1.157885 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.38\n",
      "================================================================================\n",
      "Input:\n",
      "lution and the sans culottes of the french revolut\n",
      "Reverse input:\n",
      "noitul dna eht snas settoluc of eht nerfhc tulover\n",
      "Output:\n",
      "noital dnu eht sna esetoluco af eht nerfhc elusnoc\n",
      "Average loss at step 5300: 1.143168 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.01\n",
      "Average loss at step 5400: 1.137439 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.47\n",
      "================================================================================\n",
      "Input:\n",
      "ion whilst the term is still used in a pejorative \n",
      "Reverse input:\n",
      "noi tslihw eht mret si llits udes ni a pevitaroje \n",
      "Output:\n",
      "noi siht i eht remafsi llits adet sa ni evit retfa\n",
      "Average loss at step 5500: 1.141342 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.89\n",
      "Average loss at step 5600: 1.156937 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.45\n",
      "================================================================================\n",
      "Input:\n",
      "way to describe any act that used violent means to\n",
      "Reverse input:\n",
      "yaw ot sedebirc yna tca taht udes neloivt snaem ot\n",
      "Output:\n",
      "yaw ot esoecirb dna tna taht cdes noilevt senom aw\n",
      "Average loss at step 5700: 1.139267 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "Average loss at step 5800: 1.130067 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.10\n",
      "================================================================================\n",
      "Input:\n",
      " destroy the organization of society it has also b\n",
      "Reverse input:\n",
      " yortsed teh zinagronoita fo syteico ti sah osla b\n",
      "Output:\n",
      " yrotsed teh orez ninoita fo aytiloc ehtsah sal ob\n",
      "Average loss at step 5900: 1.143452 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "Average loss at step 6000: 1.119517 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.79\n",
      "================================================================================\n",
      "Input:\n",
      "een taken up as a positive label by self defined a\n",
      "Reverse input:\n",
      "nee nekat pu sa a opevitis balle yb fles denifed a\n",
      "Output:\n",
      "nee nacit tup sa orpevitis ablll eb emos deniaed o\n",
      "Average loss at step 6100: 1.119511 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.15\n",
      "Average loss at step 6200: 1.112812 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.78\n",
      "================================================================================\n",
      "Input:\n",
      "narchists the word anarchism is derived from the g\n",
      "Reverse input:\n",
      "stsihcran eht drow amsihcran is devired morf eht g\n",
      "Output:\n",
      "stnacireh eht dna ornacirema is devired morf eht g\n",
      "Average loss at step 6300: 1.119907 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.26\n",
      "Average loss at step 6400: 1.111466 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.11\n",
      "================================================================================\n",
      "Input:\n",
      "reek without archons ruler chief king anarchism as\n",
      "Reverse input:\n",
      "keer ohtiwtu snohcra relur ihcfe gnik namsihcra sa\n",
      "Output:\n",
      "ereh owt otc noitart revil htoeh ni g nasicirem a \n",
      "Average loss at step 6500: 1.119468 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Average loss at step 6600: 1.141695 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.51\n",
      "================================================================================\n",
      "Input:\n",
      " a political philosophy is the belief that rulers \n",
      "Reverse input:\n",
      " a citilopla osolihpyhp si eht feileb htta srelur \n",
      "Output:\n",
      " itaciloppla oitalusylp eht hs elbif ehtta reluser\n",
      "Average loss at step 6700: 1.133633 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "Average loss at step 6800: 1.144356 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.81\n",
      "================================================================================\n",
      "Input:\n",
      "are unnecessary and should be abolished although t\n",
      "Reverse input:\n",
      "era ecennuyrass dna dluohs eb dehsiloba hguohtla t\n",
      "Output:\n",
      "era eno inyrass dna desuoh yb seilba dohhguoht y t\n",
      "Average loss at step 6900: 1.133428 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Average loss at step 7000: 1.134137 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.98\n",
      "================================================================================\n",
      "Input:\n",
      "here are differing interpretations of what this me\n",
      "Reverse input:\n",
      "ereh era dgnireffi itaterpretnsnoi fo hwta siht em\n",
      "Output:\n",
      "ereh era dgnireffi itneretar tsnoi fo htta eht isa\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001  \n",
    "summary_frequency = 100\n",
    "initial_step = np.zeros((batch_size, vocabulary_size)) # Equivalent of GO \n",
    "num_unrollings = 10\n",
    "train_batches = SequenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "\n",
    "def next_step(session, last_step, index):\n",
    "    (letter, _, do, ds) = last_step\n",
    "    feed_dict = {\n",
    "        decoder_sample_input: letter,\n",
    "        decoder_sample_output: do,\n",
    "        decoder_sample_state: ds,\n",
    "    }\n",
    "    do, ds, prev_let =  session.run([sample_output,\n",
    "                        sample_state,\n",
    "                        sample_prediction], feed_dict=feed_dict)\n",
    "    probabilities = prev_let[0].tolist()\n",
    "    s = sorted(probabilities, reverse=True)\n",
    "    prob = s[index]\n",
    "    index = probabilities.index(prob)\n",
    "    letter = np.zeros((1, vocabulary_size))\n",
    "    index = np.argmax(probabilities)\n",
    "    letter[0, index] = 1\n",
    "    logprob = np.log(prob)\n",
    "    return (letter, logprob, do, ds)\n",
    "                \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized\\n')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        (encoder_batches, decoder_batches) = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[encoder_train_inputs[i]] = encoder_batches[i]\n",
    "            feed_dict[train_labels[i]] = decoder_batches[i]\n",
    "            if i == 0:\n",
    "                feed_dict[decoder_train_inputs[i]] = initial_step\n",
    "            else:\n",
    "                feed_dict[decoder_train_inputs[i]] = decoder_batches[i - 1]\n",
    "\n",
    "        (_, l, predictions, lr) = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate],\n",
    "            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step == 100 :\n",
    "            print('step'.format(step))\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            summary = (step, mean_loss, lr)\n",
    "            print('Average loss at step %d: %f learning rate: %f' % summary)\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(decoder_batches)\n",
    "            perplexity = float(np.exp(logprob(predictions, labels)))\n",
    "            print('Minibatch perplexity: %.2f' % perplexity)\n",
    "        if step % (summary_frequency * 2) == 0:\n",
    "            # Generate some samples.\n",
    "            valid_sentence = []\n",
    "            output_sentence = []\n",
    "            input_sentence = []\n",
    "            for _ in range(5):\n",
    "                feed_dict_enc = {}\n",
    "\n",
    "                (e_batches, d_batches) = valid_batches.next()\n",
    "                \n",
    "                valid_sentence += batches2string(d_batches)\n",
    "                input_sentence += batches2string(e_batches)\n",
    "                \n",
    "                for i in range(num_unrollings):\n",
    "                    feed_dict_enc[sample_inputs[i]] = e_batches[i]\n",
    "\n",
    "                enc_output, enc_state = session.run([encoder_sample_output, encoder_sample_state],\n",
    "                                  feed_dict=feed_dict_enc)\n",
    "                N = 2\n",
    "                sequences = ()\n",
    "                for _ in range(N):\n",
    "                    letter = np.zeros((1, vocabulary_size))\n",
    "                    sequences += (((letter, 0, enc_output, enc_state), ), )\n",
    "\n",
    "                for _ in range(num_unrollings):\n",
    "                    new_sequences = ()\n",
    "                    for sequence in sequences:\n",
    "                        last_step = sequence[-1]\n",
    "                        for i_type in range(N):\n",
    "                            current_step = next_step(session, last_step, i_type)\n",
    "                            new_sequences += (sequence + (current_step, ), )\n",
    "\n",
    "                    sequences = new_sequences\n",
    "\n",
    "                sums = []\n",
    "                for (ind, sequence) in enumerate(sequences):\n",
    "                    logprob_sum = 0\n",
    "                    for step1 in sequence:\n",
    "                        logprob_sum  += step1[1]\n",
    "                    sums.append((ind, logprob_sum))\n",
    "\n",
    "                sums = sorted(sums, key=lambda s: s[1], reverse=True)\n",
    "                sequence = []\n",
    "                index = sums[0][0]\n",
    "                for step_ind in sequences[index]:\n",
    "                    sequence.append(step_ind[0])   \n",
    "                    \n",
    "                decoded_sequence =  sequence[1:]\n",
    "                \n",
    "                output_sentence += batches2string(decoded_sequence)\n",
    "            \n",
    "            print('Input:')\n",
    "            print(''.join(input_sentence))\n",
    "            print('Reverse input:')\n",
    "            print(''.join(valid_sentence))\n",
    "            print('Output:')\n",
    "            print(''.join(output_sentence))\n",
    "            print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Improve quallity of batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add seq2seq model from tf models library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
