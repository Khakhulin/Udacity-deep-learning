{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "        batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295208 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "yatheajtllatrskhmep invotlq  q pi t laf ga sahnpknylpwfl ggrqaneoe ugtpmtaf p s \n",
      "ynhvajhiovfb dowdvwpoewm d xxeewxi x pbpa  sim azf e edp tghb b dvel mxsehn e ke\n",
      "pntjis ebjdtcxto ja gpbootnceupoowghydoycijjtdnfy jkmw izminq  q vosuranleav tog\n",
      "efnuaicetxeprafgbymdnnls sdnftlteteiydgohjnpqsjh g yfsntlailngtvnznnmwraeuer swe\n",
      "i wcgkhznzsicgcobohnoqd cauhtu e eehlkfrfmixu rvlrspy  eq cscuclgcjosttr neyjvzc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.588530 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.246915 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.094018 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.996125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.929049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.904875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.858147 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.817738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.827896 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.827739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "giee romery with the sjedice as defrremic is and will americal aceipling zeno of\n",
      "ing perilev of dores of nuctrakuovinory in aolly vid groininily heasuthic mo of \n",
      "hile have when haried of the lating of ma atorions artain on noqulles nice saace\n",
      "lies the the the nature three seven his brids of e to thind to speld fiaded or t\n",
      "legis incesfibuts pose migfes notic oill minsterm to promited isels it ledery wi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.773581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.753957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.734725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.750867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1500: 1.734888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.747994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.713807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.676606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.647259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.698557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "vine of jeaged sckovemallys hindufil the bornain bungances porsed useng anstrel \n",
      "coling the eiglonoss in a dive the parouse mouse hanautity parted the amses mija\n",
      "jem heid bown discision often itserent cited resectors and tandticislad sucheish\n",
      "gorage the gutyursts condictine romield in no be devited mastom dose for pos gat\n",
      "ming caroil it slassing jost inkent zero four one retumunne desse is landugts th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.688800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.684880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.645900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400: 1.662937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.684959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.656358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.656332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.650387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.651811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.650454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "nizal langoit it is froy sounce was bould sevon fors repable officially eaulic d\n",
      "zer the farchabon recolds their one nine seven two zero one nine zero melicyoses\n",
      "x used theory that to first lape decouxtly in auscak two at innose atsinced to f\n",
      "is do gro sceet thereally reuin landstac dome cassing to with dublation puncil r\n",
      "tive two there wayed hussity of blime freazius maracian indian is cacks exa and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.626096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.647592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3300: 1.636553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.670837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.664399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.667626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.644710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.643552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.635254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.652572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "scy rophist inclasse seliding from nomels discovering todby strupate compbered a\n",
      "han one nine th end the peaces blatt he descryefts suppen butly in the burlys la\n",
      "ired lipp agastagle is discencus meir caye one johia a rume anderwaberable murni\n",
      "ver mass gamesed part quevered shoug one nine nine byhind the king by event dist\n",
      "zer and ninger ecognen alboin the georate fromszion wold metic priece to are the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4100: 1.631786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.636934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.616193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.606888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.615679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.614758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.624560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.627901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.630480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.608328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "gan mutholh called a cain through and the sage hall after litisunian howish musi\n",
      "galled between the bed than be israbling and psypheile of s swith was masis inde\n",
      "k in one nine six eight one nine one six three zero one six five years cominal p\n",
      "emer first the mikam by distriptance others that killed thig consifted ston five\n",
      "d the with main pocts for oftenray this parces for chyple gang ar often mughts w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.603532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.587737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.576345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.579209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.566709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.577041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.567916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.580292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.572056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.543825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "mill dare posedent an armet of cert seven two zero with samal the tevired an and\n",
      "af the war as dolas was all da pendital or racote restraish and sueg thewe on th\n",
      "n six jasables heablines by confinition the quanters ponitary markex on the many\n",
      "on amac brash umis treen one nine seven nine six three bholow is a conno than th\n",
      "s and arny since actival word states reflocitian heak mangy is game otceen press\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.565541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.537013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.544169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.540127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.558921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.591132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.577313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.581632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.572044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "p gen g in an and distranamor pernabial like and typical articule john tox sh th\n",
      "tha suckele such days pisw in the litelell and populate the chablisms of the mor\n",
      "pen from a hilled the replyntlening emperor milleally it series see brouly as th\n",
      "y and an emeriarch that five dregal saching it is notes iboly waves one zero zer\n",
      "zer geo claimed byughtrick ensirension on neight sinning the qutles whatt whel h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(27), Dimension(256)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296088 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "gnyruvqeoxko oiz z hw ilv bpb kx lidyvglnpfdergeahl  nfsnavviqloeqlvsatisknrtadr\n",
      "o drqklnk gz gqi uhn qiigo kudqiilajlvw  n lk nm wedhe eiecarhxsqhlnt zteojcaoef\n",
      "dvjectz fnh kirlialdyaxebmcjetm tfibcdxh k iba li vqtglbd uidsooojc cuc did e  e\n",
      "weadhedhwsnlhwtlcqrfiallfr if qncuitr ea tpbnrewsduoseiffzislcocsr kekeswkqrttly\n",
      "n sbe kan icp vcfnt v v xcid  ozniqtmosoj ejzpeim mgc agwt  joenefucpafgdzp joll\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.578006 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.241599 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.080559 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.993168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.997385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.921496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.896213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.874717 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.863912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.795488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "x astin time the biede of knotiper stages ay masses diyy the hresm the beto it c\n",
      "f was didpacomore of purbon etceol and for opter plepular avreiked to a noveb se\n",
      "ricise orrenure one one seven in foullalic guen mut one nine ofter molugagraing \n",
      "her has as woin the fishce erees the retyed the cantucate maboroly cotuence erea\n",
      "zed in thisoldwoum codnive tic as a pardicens aytelsco of in the pautef extecks \n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1100: 1.771558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1200: 1.792623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1300: 1.776950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1400: 1.749590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1500: 1.738979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1600: 1.724243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1700: 1.744537 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.706883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.715391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.719643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "mal amedia per protectional lindent showed and gill broods houlations to crablia\n",
      "jendacucty s twok daugise by the valct fing thser shaneage bald ada pherem tees \n",
      " quath vollows on kitt serouss frectional diecan these geart as an aselicated ca\n",
      "jeed albymalianca deone afterly deam korterdest jistion of he fast adea of low h\n",
      "mentsers ameriated alterentter mides ellapitions reserice be ansechesed enation \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.704503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.679675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.684407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2400: 1.686753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.704629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.678998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.694463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2800: 1.654941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.658249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.669374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "lical isolan celler arcilural compare the lands assail existed by or the seallin\n",
      "maned aunor ablected houchlored in five verter bus the dissuriture making s la t\n",
      "ffer four mathorization of where likely of twu coulling smacle servivizally remo\n",
      "ned it daierc the coulder and arterts of lecks is haltter thermour eight nine th\n",
      "figelo colonender one nine nine nine nine zero shing hulta and emplicaping bed t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.657744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.653052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.637909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.637971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3500: 1.629433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.634803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.634721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.632055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.620987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.621898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "chillowale have moracic becagarth ollogave inditheme tain medicions offertiially\n",
      "kions composbats swenberly address brigh war heasy marschled charsherazafas the \n",
      "le keeving eayal to cleasite butchade compiticiesmilm hebbolly of the sectors th\n",
      "fac is the e breatius simpute they edito appirily reganast that mased and a migl\n",
      "re three one the german also headshasels ricemm gruith found to diffine american\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.619685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4200: 1.609960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4300: 1.591466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.618586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4500: 1.626669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.630820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.597774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.583371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.599418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.623120 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "s aduative of gthinamation seccent gomania atposes the offscy r for autor sha mo\n",
      "kf evely are moxitin increas and neater a ennekhione in perlip the to s policion\n",
      "ed the each hole artiures entile one five zero sell two five in the themen of so\n",
      "ys it wotter both puldimal adchand percent for space of the estimgy computatiect\n",
      "ns actomal chant holving such the cidian regiols internota the sade incree year \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.635124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.634271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.593521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.580340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.607977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.569673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.576006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.597034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6000: 1.563030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "o countly and of troagus alo is a relates such as a quilat sudpup vari and show \n",
      "jon he sive estension light however this one nine zero zero est in s linder can \n",
      "th ocrewist that by empioration for related tanchis is ituation from me fighteld\n",
      "b were the skempon inadapped of his cantrick immootal humprish publiornes upodes\n",
      " most featives wille by macles andsce and gemer to zen the for the mode sezen pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6100: 1.585241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6200: 1.599419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.611827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6400: 1.639336 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.635514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.604675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.594729 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.573748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.572458 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.584984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "hes his terry a that prish in can the lefeet who junding incies mempose returned\n",
      "nge sient also at a buri than naturally feftical greek universy one nine six lif\n",
      "ken is d known burde of into word notecer riggh nafcer assed rengrayly amminia t\n",
      "ra filled construenter he reloger eloperes the hervy s manum curled theorits the\n",
      "ods and edeciated two the color example succks si publishe of them during offect\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 7100: 1.555530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7200: 1.592411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7300: 1.601814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7400: 1.578915 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7500: 1.562399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7600: 1.589043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7700: 1.579928 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7800: 1.581206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7900: 1.586072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8000: 1.564950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "================================================================================\n",
      "os samora s coner soon of the be time taxim jondaria a invieence febtiar on scet\n",
      "rair of alsyor the foles intension ths exalaton and for andificid p de mechanism\n",
      "zades and offendon of the alcoce o through nat no chure relational rylimbus vide\n",
      "phonal the some to kuing longs west years purnts of a pursuit nowleduen of ave t\n",
      "t ageious of several of nites two six seven elead downame which on the midnas yi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8100: 1.571132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8200: 1.582282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8300: 1.591929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8400: 1.569817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8500: 1.567689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8600: 1.551054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8700: 1.562797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 8800: 1.588976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 8900: 1.581452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9000: 1.586009 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "per consimerisma general drween anchmance if sull eight sions four seven fill wh\n",
      "re contrives which they of game india attayen polind explication unifour but ame\n",
      "eish in the cluited assertious sometimes within critives k frolloo marulata scot\n",
      "by and imple the k one eight and nocle reart teperots of the san griffer the was\n",
      "zed imbrisis minctor soud that ansible clao ziskmis sbarletdery the world raxamm\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9100: 1.590926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9200: 1.595583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9300: 1.590074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9400: 1.573143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9500: 1.592387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9600: 1.581841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9700: 1.577665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9800: 1.577118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9900: 1.580554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10000: 1.597251 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ing may went from and de the sajound out a and torad are grany of lewneng into w\n",
      "orser erlywara the demored tom use an amonging than laceman in the grapked by th\n",
      "s others after geodx struelian statforts the ground pachice work societatoospfic\n",
      "pian fegen be game by madicatimmans scate shortanes and the runtil supported its\n",
      "beh markg from the jec his creymons over famurayils hold removyble conditional f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 10100: 1.565126 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 10200: 1.590093 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10300: 1.584949 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10400: 1.582896 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10500: 1.573755 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10600: 1.583306 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10700: 1.562903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10800: 1.560848 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 10900: 1.555347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11000: 1.588585 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "roughs on the film brit perconstion to he sect form s r coduce cre often and qui\n",
      "z well verquesthic official rocumber under where has a legacy adent are offerabl\n",
      "ists augnrising its happown law overpoussia alen for spected theadine therefull \n",
      "ng anahaphies to actor show the diel those of troal intublishos fort three tho a\n",
      "pirited of some created unior the ong starcy mal yack and the but lease roy an o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11100: 1.587801 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11200: 1.579634 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11300: 1.541607 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11400: 1.569012 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11500: 1.562717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11600: 1.557078 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11700: 1.573717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11800: 1.580734 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11900: 1.601813 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12000: 1.597944 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "zed with two show allow processosia as a one nine zero zero prepequence pignorsa\n",
      "y the under pow reevishing mankex one seven chillows ugnotia year s parof it to \n",
      "vimians such is tilspos forch and leftitsour south some trittulsot of is atril l\n",
      "phanz idels without supromussed governind jurnes schalrone similating a between \n",
      "que involver on wassfal weater of histiclats reference of the with the like stro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12100: 1.575750 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12200: 1.590594 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12300: 1.569764 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12400: 1.566402 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12500: 1.560497 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12600: 1.551376 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12700: 1.589705 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12800: 1.581224 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 12900: 1.588941 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 13000: 1.549479 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "quers exportained town does of the weenned by one seven two rulation hera two co\n",
      "variex in denawion xpy in one thromengies for two did put of whomery the country\n",
      "d fine fullion clowesh force thinkfore central ugs a light into suppsorey all fi\n",
      "zen numbers and the invent at dickather martagers linking here war at show chour\n",
      "xard were lown numbers allahoph errisken suka ppuce studed in one nine seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13100: 1.593628 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13200: 1.593347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13300: 1.577060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13400: 1.592521 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13500: 1.603572 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13600: 1.579617 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13700: 1.578702 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13800: 1.590926 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13900: 1.632133 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14000: 1.615489 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "coneand theory do on hemed teachw and defins two zero zero in west a loss by the\n",
      "s rot national name in the nument shack booic recalas european pechured is parti\n",
      "m ith darined is consucunds carg these as questor statum and in corpore the empl\n",
      "ter milkning or a histam their rause ownersite one nine eight five four stat hal\n",
      "ishing mated out convlems mornenon from forg had fearcis lavach calannisult hurr\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14100: 1.596736 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14200: 1.573053 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14300: 1.580519 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14400: 1.595691 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14500: 1.600329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14600: 1.570035 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14700: 1.592610 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14800: 1.574893 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14900: 1.592432 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 15000: 1.586098 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "des as that it day ten inchodence is the untogrites and mains other anba first m\n",
      "rap purreating the meupples oft devolution yet over is parode islands it still r\n",
      "valur been particularly of the causes different of the triffed by industric nall\n",
      "nn norve dease to as also of computer finnistric commanncement production of its\n",
      "orkage ip our screepon on one nine four films ireland of these american in great\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15100: 1.573534 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15200: 1.575539 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15300: 1.570134 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15400: 1.576043 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15500: 1.600311 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15600: 1.593948 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15700: 1.570269 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15800: 1.611362 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15900: 1.617383 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16000: 1.577232 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "way are by simpley one connicife of eight nine inine were inceed s by s eck spec\n",
      "ners american rock though northan initatesely as of byology aber purrdegations o\n",
      "ley opyp that air that sources pression me concerfine two zero s autax socially \n",
      "k by the in mijornism is ethornism hur charge skater as one two is stromping ope\n",
      " faile promoties merslar institite the one four midribivies and toward dowing ol\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16100: 1.570450 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16200: 1.566831 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16300: 1.608270 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16400: 1.591806 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16500: 1.566013 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16600: 1.567127 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16700: 1.579081 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16800: 1.595272 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16900: 1.552456 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 17000: 1.557044 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "jicalling of serving of a williorical point edust clins are b overel as general \n",
      "chnorece bill an athapuria one cable clown contablepomnody contembrally dungh in\n",
      "jornwa of the low play storea ligg a wings tver sean pilined directle in eurlibe\n",
      "zer headly into s the and arrikans micronizalations in is the made in the been o\n",
      "ubluel gremack sociation over fasta absour the mocars and terman the cultural th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: Cause of high demension in bigram model, we will use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 20\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "        train_labels.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for inputs in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input_idx = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input_idx)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 27)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_batches.next()[1]).shape\n",
    "(train_batches.next()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299145 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "oylgmcepa z fpvtywiumju   erw iboay cqer y voa sdc nidk us ql n a rcu tpanm xoxe\n",
      "klhlvbez empyziikatzpuiorc iwc wisuirghrtljrlm isen vuely  i vaypb  eteehhzsjpil\n",
      "rzkaoww ndlpomgfeg eotyueoanetze kr fimw  dbcn ez   efoexuupletyqthm  kdyzlcccqi\n",
      "clrsgyfditku o ttvd gh e z iiejrek  reiiaknnhr ueu e hiosh yeal bk qlso iomvzco \n",
      "khcpsaa odtjissm rravggbeixsq lqoq x    jlwltsecxxgvyo ayroh fbsvee aia in etisz\n",
      "================================================================================\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 100: 2.386494 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 200: 2.050725 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 300: 1.958449 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 400: 1.878871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 500: 1.835760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 600: 1.831754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 700: 1.800248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 800: 1.785890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.776740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1000: 1.756033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "ns mattenish the jeprest betill stuagly recopitaran elephised ciethnem impledact\n",
      "ments hark zero subsout articty working instanch hotro aneted ec vied anderewark\n",
      "dest sepathends ha on cetress liesis apperies then with of the stund mintirely s\n",
      "ressiol trantive of a stider blet there styt ie apbirse sixi one nine eight eihp\n",
      " veries then newsgest prosulty orgagenstos present senize s s the sted intenitio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1100: 1.754460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1200: 1.748571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1300: 1.726775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1400: 1.726786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1500: 1.727561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.702806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.702149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.673698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.692209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.704837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "gas by fiam wheer boops above seven yark sall and name carvice are falson and pe\n",
      "sern is jager bepen but three is and the bora and name in a sharge than or sense\n",
      "wifrer lub every person indiscient civiliond leveld spelfon one five five five o\n",
      "js apports one with and seen and bressent of supprotent six canamia an cater lif\n",
      "ne he used about seven zero ze of draweverl the sirah undernace of the proveriol\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.687647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.677252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2300: 1.683569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2400: 1.657063 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.660905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.664901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.659565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2800: 1.686172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2900: 1.661634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3000: 1.686554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bell respectual take sty all peomber stagn of one thrby the mecry see groupn isl\n",
      "ond while not relique lesignas uply it no fark ana gene exi one ni sound minity \n",
      "er meam of mudyling tourining clearates mones generage only their bik ups propin\n",
      "ov sig be premicia pusse of in ho moer which he neatric from chang janding in ot\n",
      "gest is don of his awarder ciese is island unor to or ofterne of hilly were orip\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.668552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.657001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3300: 1.629718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.643223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.659296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.638719 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.652325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.662170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.675188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4000: 1.652607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "yual and old und nnturys oulltian as voll and coblurgrate intendentencoly world \n",
      "ful the most s hellva and election s bond m they anks bill is bochsails ainspeno\n",
      "bands to pennyfo mis dame in dadical laures his chers informed source a boody mi\n",
      "vicul fabq one all aptem john bradia the analy dister for ammere of caulism one \n",
      "vers and luend a seven one mit ogrover shost to five four isso notations would o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4100: 1.655894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4200: 1.664900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4300: 1.639053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4400: 1.660346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4500: 1.636049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4600: 1.631538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4700: 1.627726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4800: 1.633210 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.634437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5000: 1.623997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ching the percession be kearges as they evil gpuce ordhlitipe to trivity this kh\n",
      "saured to the courquent as asternered as popel on films and lee while zero zero \n",
      "nap refirs staracted by selvcity use in anustimple in the creat mooking the one \n",
      "nicousivels vatels a macipmda withbrorizenter one sm pxt at a cans faman commits\n",
      "dize and to a states hes pace to it adone the formanistianly the the majored the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5100: 1.612377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5200: 1.586064 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.566667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.566389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5500: 1.562027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.613164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5700: 1.580942 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.591175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.586076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.578914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "acshitle ms other ravidual subsmen published a mysting one two mistorical north \n",
      "quation educally to no commus is gremights where war one nine seensane with doon\n",
      "domoson historited and exploiswantaclates states a free fick state allizy sane s\n",
      "prous then presidents founds compleri japoubbles of this germandurche distica im\n",
      "rouse asterneger and remains themosal pregutioned rality the civilization portab\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.558227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.571269 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300: 1.567709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.597509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.557523 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.577094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.591322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.550663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6900: 1.558134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.567879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "t and abblecution and high promia and cogropan for rucotforates the memanuan rec\n",
      "ker a construation or pose barrani of these and schoor of its of the vardimed yi\n",
      "be destancing for eventatinvenger distinction some habyen in the commannitionslo\n",
      "yberved diromians by higher accords are marvist which erigary labvement one nine\n",
      "hosal to jadlar it seaver programateriding and charvi zerood douced rugh lopting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7100: 1.586794 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7200: 1.568544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7300: 1.559103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7400: 1.575451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 7500: 1.586957 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7600: 1.567902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7700: 1.564885 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7800: 1.537815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7900: 1.559151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8000: 1.570239 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "xic instructiogual with attingly about the possed one simmate stricton land shg \n",
      "hoot films had and sone their willania in the emperor sahnt van cassemates peapd\n",
      "gayans between the lays was catakes the rights if timoshics by the elow from pos\n",
      "fecturositate with a fustroco series over palarsisculus gide and cape between gr\n",
      "ked the certed a propentene amveromy uncha strease as propuge rard in the his po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis=1)\n",
    "      feed_dict[train_labels[i]] = batches[i+1]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_idx: feed.argmax(axis=1)})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_idx: b[0].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b:**  Change graph for bigrams model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['nar']\n",
      "['rch']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 32\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        print(embed.shape)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    print(len(embeds))\n",
    "    print(len(train_labels))\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289089 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.82\n",
      "================================================================================\n",
      "wzkgfrmne gvme  ehgfah ykezsac teejk r  eypo nyhtvia  xxngdwyioy t nmaaodilnnswvc\n",
      "tbiprrpjz   ztkpscan hh woawatxxheovklocr   lepnzbwva eemkaridb w  ahictra i qe z\n",
      "qcc rxvloagsuw ssaifevcjnltctcehwp ofenygtnfgzl o  zbso afcsr iuwnthczv fxofesdax\n",
      "ds bisawakfgy p  wfcpkeo     ujdt eukdoktvylrb j pg d xawdg jxe dchkfbukfvyedcsto\n",
      "ixthpk oxnqanbtvmea bwplx me gtqjpwbtn tfgoearcg xiwz lh  nps vfphah suqgdoxxtprn\n",
      "================================================================================\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 100: 2.302889 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 200: 2.027483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 1.912729 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 1.855559 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 500: 1.808225 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 600: 1.827677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 700: 1.835468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 800: 1.815775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 900: 1.796186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 1000: 1.801397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "ccuuooueouuhiiiuuiueioaieeeuuueuitueuaeeieeeuoleaouuueeiuaeheuiieuroeiiuueoueioeu\n",
      "bveoeeeeyeeeieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      " jeeuoaauaeaauuinaeeaaahooararaeoaieahaoaoa uaacaraoioelaaauaoearraelaaeleaariaoa\n",
      " voaauaaiaaaaauaaaeie eaiaaeeeaiaaaeoeaieielieaeaeeaeaeeaeeeieiaiieaeeeeeeoeeaaea\n",
      "s abiaooaahmmsihbofmisdudoiacamooatttotskcaftiioobsambmbsmbmrmieopmnebbaddpsimfok\n",
      "================================================================================\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1100: 1.776139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1200: 1.753704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 1300: 1.783106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1400: 1.755056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 1500: 1.748084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1600: 1.743098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1700: 1.736587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1800: 1.751845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1900: 1.730856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2000: 1.735253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "fy pm lnpmcpqtrrnrutnprnzn tnlnmprnrntlpwnetcprlcnpnsntrnntpcmlnpttppntr r rumpls\n",
      "hfr     eo i r  rr    io  ri i ii rro  rriier ro oriii i riiooioi  oorrr im  iiro\n",
      "zroeeieieoeeeoooeoeooeoooeoeooeeeooeeeeeoeeooooioiooeooeeieoeeiaeeeeeoooooooioooe\n",
      "nkeies e sei seeeeyi ee e  s  saeee eae se   nisiiieee   emees  oi i a  l ee eee \n",
      "tonbmm ammgmm nms s mmprirmrgs b r rs sbsombr  ml   mbi mmbrm rbyaisrmb  mmsndrms\n",
      "================================================================================\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 2100: 1.721529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2200: 1.720902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 2300: 1.749266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2400: 1.751861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2500: 1.726976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2600: 1.713863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 2700: 1.715788 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 2800: 1.716926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 2900: 1.730110 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 3000: 1.749124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "q limpgcnmsmeccwrmeodijsmwyaittmspbrmtacgptealhmhrsachlccltmhbheommrhyoiaoaihalsh\n",
      "us       i  s        iu         o    a              ii o       u         c       \n",
      "vwb    enr  o o     o       ro    h              i  o   o     ooo       i        \n",
      " innnnrmnnnnnnnnnnnnnnnnnnnnnnnnmnnnnnnnnnnnsnnsnnnnnnnnnnnnd nnnnnnntns nnnnjnnt\n",
      "fj    si c   m   o  uwm      u c wh rhr   ssu  ihs usoau m s   mss ssocsup s wu  \n",
      "================================================================================\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3100: 1.767456 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 3200: 1.753534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 3300: 1.741110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 3400: 1.738429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3500: 1.756102 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 3600: 1.756591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 3700: 1.784526 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 3800: 1.716554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 3900: 1.732743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 4000: 1.743809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "cjuaeoaaoooluiusruirluaauuuerousvuuuuuuluuuuuuluuuvfuruiuuiauusuuuuiur uuiuumuurm\n",
      "gs         t                                       i                             \n",
      "o eamavpaprsprdcvjuoiproaujhvftmrrfcorltadcbrtrlrmnmigtriwcsarlliadcemmrauhfpowsp\n",
      "gs    t           tt                                         f e                 \n",
      "oji oe aai iee iuuiaiouaaeuheeeieimooeeiieu a yaai sieiaa asiuuiaaeeuiua euueoeei\n",
      "================================================================================\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 4100: 1.745477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 4200: 1.728003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 4300: 1.745660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 4400: 1.752761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 8.14\n",
      "Average loss at step 4500: 1.744508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 4600: 1.737338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 4700: 1.701295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 4800: 1.736424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 4900: 1.739173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 5000: 1.724270 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ce mvrlplpmipr rpdlppnlprprmpprdnbrpnrlrdndpllsrrtnmdmdrrvdsrn mrpdpdrpsllvpsdrdl\n",
      "xdoeie errirerm em me reuemeuom  iu ime meee  im eeeiemmu im me  leieem   o ml mi\n",
      "kbimaaneaaaauaelaaltamaeaauaeaoaeuaaomealleura laaonloarraaaelaaaraornepaammaarar\n",
      "xmaesasaneaaaceeaalalsiaaaepaaaddeaasapasaaaaoaeeapaassdeaaaalareaaeaaaaaaadgpea \n",
      "mnoia eaensaoauoaoo auocaia  aaoooauioioaeiooooauouuooo sooaueoueoaopapwoaeoaaaeo\n",
      "================================================================================\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 5100: 1.703443 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5200: 1.678977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 5300: 1.694666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5400: 1.660934 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5500: 1.662414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 5600: 1.667233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5700: 1.654800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5800: 1.659663 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5900: 1.660582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 6000: 1.671689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "fg e  hhheho hhhh h  h hhh h  hhoh   hh hh h   heehhhhh    h   h hhse h hh eh hhh\n",
      "pd  aeaaeaeaeeaeeeeeaeaoeaeaioaoeeeoooaooieoiioooooeaieeaeoeoooeoooa oeeoooooeoao\n",
      " unnsspswsssnnsncnsnstpnssssssnnsgnsssnnscnnnssnmnnpswsnnknssnslsbnnnpnsnsnnwscsp\n",
      "ihao  aaeaaa  a a aaa  a    aaaa  a aa  ea   a aaaaae     aan   aa  a aac nara   \n",
      "vm  e  aeo    aa e   aeeae   aa     aa a eseaee i     eea  esea s me  ae sa ee ao\n",
      "================================================================================\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 6100: 1.674793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 6200: 1.687196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 6300: 1.694800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6400: 1.668747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6500: 1.634569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 6600: 1.645372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6700: 1.658347 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 6800: 1.664987 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6900: 1.665253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7000: 1.645361 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "ss isei eiiiieieeii  i ii i io iii eioiiieiii ef ae iiiei  ieaeieiiiiiie iuaiiiii\n",
      "illdiluyauudadadteyudiehrdayyelliauayiyr ds yaeyyy  uytaeuyiruttilrdayyuteeu uttu\n",
      "ux                        i     e              i            i                    \n",
      "cbilieeileaaibeebnlbmyylyyismbnlayiiilbyiiaeayueieililnriineleua aybyybiiiiioeola\n",
      "psi  oe             i e             yi     o      e                i         o   \n",
      "================================================================================\n",
      "Validation set perplexity: 7.70\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[0].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[1].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input0: b[0].argmax(axis=1),\n",
    "                                              sample_input1: b[1].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c:** Add dropout layer to output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "embedding_size = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.7)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        output_drop = tf.nn.dropout(output, 0.7)\n",
    "        outputs.append(output_drop)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibe']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa6ef9f79b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[-2].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[-1].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input1: b[1].argmax(axis=1),\n",
    "                                             sample_input0: b[0].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adic and om', 'ks journals', 'g inn g is ', 'itions were', 'antiprism w', 'ragon ball ', ' for his sh', 'eference in', 'dclapped th', 'prettanikee', 'ng some str', 'le on top o', ' as the tre', 'anks buildi', ' lead in a ', ' nine nine ', 'g in the u ', 'g the tourn', ' arcology n', 'unlike toda', 'mosomes vol', 'ial plasma ', 'h the use o', 'six zero s ', 'rgagni ital', 'n the late ', 'he victim c', 'an be const', 'ism secular', ' case the s', ' maker leo ', 'ended impro', 'ut failed t', 'sed on the ', 'ed at sea s', 'ense that o', 'f march fiv', 'tor kenneth', 'horror movi', 'es use the ', 'percard as ', 'ld reaching', 'was soon ca', 'g monster a', 'overnments ', 'ing machine', 'cannot say ', 'to young wh', ' eight four', 'author john', 'learly seen', 'ed to the a', 'cement of e', 'ion the rev', 'neural netw', 'press that ', 'underground', 'o this text', ' since one ', 'her modes g', ' for other ', ' a role dra', ' zero four ', 'ight zero s']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramsBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            first = self._text[self._cursor[b]]\n",
    "            if self._cursor[b] + 1 == self._text_size :\n",
    "                second = ' '\n",
    "            else :\n",
    "                second = self._text[self._cursor[b] + 1]\n",
    "            batch[b, char2id(first) * vocabulary_size + char2id(second)] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigramCharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c//vocabulary_size)  + id2char(c%vocabulary_size) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigramBatches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigramCharacters(b))]\n",
    "    s = [x[0::2] for x in s]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramsBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramsBatchGenerator(valid_text, 1, 1)\n",
    "print(bigramBatches2string(train_batches.next()))\n",
    "print(bigramBatches2string(train_batches.next()))\n",
    "\n",
    "print(bigramBatches2string(valid_batches.next()))\n",
    "print(bigramBatches2string(valid_batches.next()))\n",
    "print(bigramBatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramSample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] \n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.7)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        output_drop = tf.nn.dropout(output, 0.7)\n",
    "        outputs.append(output_drop)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = (tf.placeholder(tf.float32, shape=[1, bigram_size]))\n",
    "    \n",
    "    sample_input_embed=tf.nn.embedding_lookup(embeddings,  tf.argmax(sample_input, dimension=1))\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.625717 learning rate: 10.000000\n",
      "Minibatch perplexity: 754.25\n",
      "================================================================================\n",
      "qtfdcnxjjsvmempawjrugunscwgsfadoaxrmjcsznnnexwodlb rpwtteqzjvuaskqyohtssrotzracd \n",
      "eaoaqyqqqtthgbghovnoklqeujzmgsljimsizocdegdiiycmsgltosbkqsqe  juupuxlf mtzymc hox\n",
      "pl h tmhzalgenkfshotxbfzpkjtqdterylb  dvvmotmhdjkhvnrtmkfetqkdjmvtuvovzjgdxyhxxgq\n",
      "zxuajdunsupyjsorwm mrxvkqfnpkx ifaauxaetrezmbcgphoclrrmz jjhetvmwxsnpekjvsxadnpmh\n",
      "mtnjbcxy kakvbvacnvqbmazsybugafuu sydyhxaqtqhzvnizsxrnahyfzspkfikvrnkomhr gxevarv\n",
      "================================================================================\n",
      "Validation set perplexity: 646.48\n",
      "Average loss at step 100: 3.721513 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.81\n",
      "Validation set perplexity: 12.38\n",
      "Average loss at step 200: 2.405352 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.11\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 300: 2.259583 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 400: 2.117846 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.19\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 500: 2.069386 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 2.011264 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 700: 2.003808 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.988492 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.55\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.923224 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.901478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "================================================================================\n",
      "wu orch as and grate and suill eveld miss for fannisters exportal absecity trandi\n",
      "afer work facessuars own bile s a use or natiationally depted two zero zero four \n",
      "xl rusts three deftly deven and exported trem strorges are goternes gine sevent t\n",
      "galics at s provelogic decive three goous has by of the or khoough nine five ex s\n",
      "yorges to been exchose such it dep mstatory or which have mashary two oods truity\n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1100: 1.895293 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.935607 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.882132 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.879092 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1500: 1.881786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.887357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.821205 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.818994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1900: 1.846053 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2000: 1.826049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "cvory basque enylic in the capian senides schipictive growly which rosed the enkh\n",
      "iquently bis capistanica delearie excidifativem pan is of the the is weres decron\n",
      "ist maint by state two the probstruction factrificle was hero four seven have riv\n",
      "risition cal pross ances by win humaps expelions sas uppers the cedery is four ev\n",
      "on univers dey anatimple eding three the parped eight eight eight seven zero zero\n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.810442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.811508 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.801336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2400: 1.836576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2500: 1.802778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.808648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.818669 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.810343 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2900: 1.780158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3000: 1.795228 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "================================================================================\n",
      "my officiales speen is a verg countanic contraigo bouth the of the during dunk fo\n",
      "ectumentics of the clim of s rovialf progrenced by univer quila dypes base godern\n",
      "bvent scity systeting albumer of setern rooing which grosprit and it grouction am\n",
      "clopes six one one nine nine two zero m v ageneroce to ob see of of bluction and \n",
      "fs conts in teden to the pol cavitely namerive o for cause five or for ough dicia\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3100: 1.788626 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.820993 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3300: 1.791421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.804681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.763852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.769251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3700: 1.787372 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3800: 1.772152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3900: 1.770461 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.748770 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "hs objecth lember of colack the rivorptart who action and the ader smite for noma\n",
      "kqians region soung nonly nation rost four collitic er the and it systems mohor w\n",
      "sly radition however more a gavious felleorders so one nine seven years sundref s\n",
      "bmber one fagam commany one nine eight six four nine marian he ful many fer celle\n",
      "ek salepers estace the one five this in six seven ley rhook reea he which muminin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4100: 1.753341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.754904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4300: 1.758026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400: 1.751002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4500: 1.743036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4600: 1.737592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.741650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4800: 1.743987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.733613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5000: 1.713315 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "cond intensimic nonly overries and currel equipition amurary see and on it patemp\n",
      "ic tetch are four me one nine eight con the promicil hainst station of airn it me\n",
      "wish he titary selfronon of had to two two zero famic line were mitten to boditio\n",
      "pace one one nei two zeroo zero one nine six two shaude know by that noft iving i\n",
      "xrhancine the conside the depetest which the bourray by hest of spaces poxic dope\n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5100: 1.738318 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5200: 1.738346 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5300: 1.736808 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5400: 1.711470 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5500: 1.699603 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5600: 1.707417 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5700: 1.735175 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5800: 1.764228 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.757051 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6000: 1.720191 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "wzourt libreshed nation to and estates of one eight five one screes definces rund\n",
      "pnad god s have ganistelts about and water and civilica theres the broating the b\n",
      "cets be arve asseath which of and fishetized striber of him sun storives one one \n",
      "ualia invents relar ats of faction contenne one three his ease of ammunded goale \n",
      "lwas the essiple this from phich ascoitle recordal as any of has a cossible and i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6100: 1.715734 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6200: 1.719862 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6300: 1.737822 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6400: 1.694072 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.701996 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.713963 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.684008 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6800: 1.705068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6900: 1.721193 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.731440 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "ts known yorder to his operdian funderal five enclart the in facts with majortan \n",
      "wn and griverce bet the deen munsix eight zero zero zero five rifiet a partica sp\n",
      "wbands which an expertiates and rote pay a realation the sman ba at in councings \n",
      "ust in projemense the autoted examplee the game encomposer or preduces can and on\n",
      "kxs communess one two three zero maled namellable in lassan the equation to s str\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "\n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = bigramSample(bigram_random_distribution())\n",
    "          sentence = ''.join(bigramCharacters(feed)[0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = bigramSample(prediction)\n",
    "            #print(\"prediction: \\n feed {0}:{1} \\n\".format(bigramsCharacters(feed)[0][1], bigramsCharacters(feed)[0][3]))\n",
    "            sentence += bigramsCharacters(feed)[0][3]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text,1, 2)\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bababadalgharaghtakamminarronnkonnbronntonnerronntuonnthunntrovarrhounawnskawntoohoohoordenenthurnuk'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_text.split(),key=lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism']\n",
      "[' msihcrana']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class SequenceBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    \n",
    "    def _next_batches(self):\n",
    "        \"\"\" Generate a batches of appropriate size\"\"\"\n",
    "        batches = []\n",
    "        for step in range(self._num_unrollings):\n",
    "            batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "    \n",
    "    def _mirror(self, sequence):\n",
    "        \"\"\"Mirror every word in the sequnce \"\"\"\n",
    "        mirror_sentence = []\n",
    "        for word in sequence.split(' '):\n",
    "                mirror_sentence.append(''.join(reversed(word)))\n",
    "        return ' '.join(mirror_sentence)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate two next arrays of batches from the data.One for the encoder and another for the decoder.\n",
    "        The array consists of the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        enc_batches = self._next_batches()\n",
    "        dec_batches = self._next_batches()\n",
    "        for b in range(self._batch_size):\n",
    "            cursor = self._cursor[b]\n",
    "            sentence = self._text[cursor:cursor + self._num_unrollings]\n",
    "            mirrored = self._mirror(sentence)\n",
    "            for (i, (s, rev_s)) in enumerate(zip(sentence, mirrored)):\n",
    "                enc_batches[i][b, char2id(s)] = 1.0\n",
    "                dec_batches[i][b, char2id(rev_s)] = 1.0\n",
    "            self._cursor[b] = (cursor + self._num_unrollings) % self._text_size\n",
    "        return (enc_batches, dec_batches)\n",
    "        \n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "valid_batches = SequenceBatchGenerator(valid_text, 1, num_unrollings)\n",
    "train_batches = SequenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "enc, dec = valid_batches.next()\n",
    "print(batches2string(enc))\n",
    "print(batches2string(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stsi covda', 'yra nrevog', 'seh noitan', 'd retsanom', 'acar cnirp', 'drahc reab', 'lacigr nal', 'rof nessap', 'eht noitan', 'koot ecalp', 'reht llew ', 'neves xis ', 'hti a solg', 'ylbabor eb', 'ot ingocer', 'deviec eht', 'tnaci naht', 'citir fo t', 'thgi ni is', 's desuacnu', ' tsol sa i', 'ralullec i', 'e ezis fo ', ' mih a its', 'sgurd fnoc', ' ekat ot c', ' eht seirp', 'mi ot eman', 'd derrab a', 'dradnats f', ' hcus sa e', 'ez no eht ', 'e fo eht o', 'd revih no', 'y thgie am', 'eht dael c', 'se cissalc', 'ec eht non', 'la isylana', 'snomrom eb', 't ro ta el', ' deergasid', 'gni metsys', 'sepytb sab', 'segaugna t', 'r issimmoc', 'sse eno in', 'xun esus l', ' eht tsrif', 'iz tnecnoc', ' yteicos n', 'ylevitale ', 'skrowte hs', 'ro tihorih', 'lacitil ni', 'n tsom fo ', 'oodreksi r', 'ci eivrevo', 'ria nopmoc', 'mo mnca ca', ' nilretnec', 'e naht yna', 'lanoitoved', 'ed hcus ed']\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n"
     ]
    }
   ],
   "source": [
    "d = train_batches.next()\n",
    "print(batches2string(d[1]))\n",
    "print(batches2string(d[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    encoder_train_inputs = []\n",
    "    decoder_train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        encoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        decoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    # State\n",
    "    # Bias\n",
    "    \n",
    "    #Encoder\n",
    "    x_enc = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_enc = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_enc = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    #Decoder\n",
    "    x_dec = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_dec = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_dec = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    encoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    decoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    decoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell_encoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_enc)+ tf.matmul(o, m_enc) + bias_enc\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell_decoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_dec)+ tf.matmul(o, m_dec) + bias_dec\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop of encoder.\n",
    "    outputs = list()\n",
    "    for input_encoder in encoder_train_inputs:\n",
    "        encoder_output, encoder_state = lstm_cell_encoder(input_encoder, encoder_output, encoder_state)\n",
    "    \n",
    "    decoder_output = encoder_output    \n",
    "    decoder_state = encoder_state    \n",
    "    for input_decoder in decoder_train_inputs:\n",
    "        decoder_output, decoder_state = lstm_cell_decoder(input_decoder, decoder_output, decoder_state)\n",
    "        outputs.append(decoder_output)\n",
    "    \n",
    "    # Classifier.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    sample_inputs = []\n",
    "    for _ in range(num_unrollings):\n",
    "        sample_inputs.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    \n",
    "    encoder_sample_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    encoder_sample_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    for input_encoder in sample_inputs:\n",
    "        encoder_sample_output, encoder_sample_state = lstm_cell_encoder(input_encoder,\n",
    "                                                            encoder_sample_output, encoder_sample_state)\n",
    "    \n",
    "    decoder_sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    decoder_sample_state = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    decoder_sample_output = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    sample_output, sample_state = lstm_cell_decoder(decoder_sample_input, decoder_sample_output, decoder_sample_state)\n",
    " \n",
    "    with tf.control_dependencies([sample_output,\n",
    "                                sample_state]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ing class ']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(valid_batches.next()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Average loss at step 0: 3.294446 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "Input:\n",
      "narchists the word anarchism is derived from the g\n",
      "Reverse input:\n",
      "stsihcran eht drow amsihcran is devired morf eht g\n",
      "Output:\n",
      "                                                  \n",
      "================================================================================\n",
      "step\n",
      "Average loss at step 100: 2.661300 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.46\n",
      "Average loss at step 200: 2.403995 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.57\n",
      "Input:\n",
      "reek without archons ruler chief king anarchism as\n",
      "Reverse input:\n",
      "keer ohtiwtu snohcra relur ihcfe gnik namsihcra sa\n",
      "Output:\n",
      "s eht eht se eht eht eht eht oset eht ehe eht eht \n",
      "================================================================================\n",
      "Average loss at step 300: 2.296636 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.12\n",
      "Average loss at step 400: 2.223620 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.71\n",
      "Input:\n",
      " a political philosophy is the belief that rulers \n",
      "Reverse input:\n",
      " a citilopla osolihpyhp si eht feileb htta srelur \n",
      "Output:\n",
      " eht eht eeh eno ehteht eht eh eht eht eeh tseret \n",
      "================================================================================\n",
      "Average loss at step 500: 2.164369 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Average loss at step 600: 2.087451 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Input:\n",
      "are unnecessary and should be abolished although t\n",
      "Reverse input:\n",
      "era ecennuyrass dna dluohs eb dehsiloba hguohtla t\n",
      "Output:\n",
      "e eht era ser eht ersere eht eseruo eht sere eht r\n",
      "================================================================================\n",
      "Average loss at step 700: 2.052197 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.55\n",
      "Average loss at step 800: 2.005054 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Input:\n",
      "here are differing interpretations of what this me\n",
      "Reverse input:\n",
      "ereh era dgnireffi itaterpretnsnoi fo hwta siht em\n",
      "Output:\n",
      "en enin ezseit fo tsevif eht isi ni ti ieh owt of \n",
      "================================================================================\n",
      "Average loss at step 900: 1.835657 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Average loss at step 1000: 1.866709 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Input:\n",
      "ans anarchism also refers to related social moveme\n",
      "Reverse input:\n",
      "sna hcranamsi osla rsrefe ot rdetale coslai emevom\n",
      "Output:\n",
      "sno thgir sno thgir enine ot ienin enin sen detini\n",
      "================================================================================\n",
      "Average loss at step 1100: 1.812034 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Average loss at step 1200: 1.748579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Input:\n",
      "nts that advocate the elimination of authoritarian\n",
      "Reverse input:\n",
      "stn taht aetacovd hte tanimilenoi fo tuanairatiroh\n",
      "Output:\n",
      "yll ot a aellacir a e eht apsasti ot a alaitatsaps\n",
      "================================================================================\n",
      "Average loss at step 1300: 1.687172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 1400: 1.580146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Input:\n",
      " institutions particularly the state the word anar\n",
      "Reverse input:\n",
      " itutitsnisno citrapylralu eht etats eht drow rana\n",
      "Output:\n",
      " itsirocissso itsirpyllare fo  eht elacn si nacirp\n",
      "================================================================================\n",
      "Average loss at step 1500: 1.496431 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Average loss at step 1600: 1.366696 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Input:\n",
      "chy as most anarchists use it does not imply chaos\n",
      "Reverse input:\n",
      "yhc sa somt sihcranast esu ti seod ton iylpm soahc\n",
      "Output:\n",
      "yah fo sawa tsilacifst eht nacsedo fo tsyllo saw n\n",
      "================================================================================\n",
      "Average loss at step 1700: 1.284438 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.57\n",
      "Average loss at step 1800: 1.222812 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.22\n",
      "Input:\n",
      " nihilism or anomie but rather a harmonious anti a\n",
      "Reverse input:\n",
      " msilihin ro eimona tub rehtar a inomrahsuo itna a\n",
      "Output:\n",
      " yltirif tro terts othg retal  i itilid sih a irol\n",
      "================================================================================\n",
      "Average loss at step 1900: 1.157714 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.93\n",
      "Average loss at step 2000: 1.117800 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.62\n",
      "Input:\n",
      "uthoritarian society in place of what are regarded\n",
      "Reverse input:\n",
      "iratirohtuna yteicos ni ecalp fo tahw rae dedrager\n",
      "Output:\n",
      "irucirucitna redrocs ni elbam fo saw rufe desaecni\n",
      "================================================================================\n",
      "Average loss at step 2100: 1.041109 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.53\n",
      "Average loss at step 2200: 1.012413 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.60\n",
      "Input:\n",
      " as authoritarian political structures and coerciv\n",
      "Reverse input:\n",
      " sa rohtuanairati oplacitil tsserutcur adn vicreoc\n",
      "Output:\n",
      " sihtahcs anihsil illacitsi taseruccar odn crepmoc\n",
      "================================================================================\n",
      "Average loss at step 2300: 0.953023 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.74\n",
      "Average loss at step 2400: 0.930507 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.48\n",
      "Input:\n",
      "e economic instituti anarchism originated as a ter\n",
      "Reverse input:\n",
      "e cimonoce itutitsni msihcrana etanigirod sa a ret\n",
      "Output:\n",
      "e cinomero itidirpmi ylartsab  etticnirpd sa raeht\n",
      "================================================================================\n",
      "Average loss at step 2500: 0.862925 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.62\n",
      "Average loss at step 2600: 0.802169 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.08\n",
      "Input:\n",
      "m of abuse first used against early working class \n",
      "Reverse input:\n",
      "m fo esuba tsrif esud tsniaga ylrae krowgni ssalc \n",
      "Output:\n",
      "m fo elacs tsrif esud tnaisab ylrae lorggni syalb \n",
      "================================================================================\n",
      "Average loss at step 2700: 0.769591 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.01\n",
      "Average loss at step 2800: 0.677450 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.81\n",
      "Input:\n",
      "radicals including the diggers of the english revo\n",
      "Reverse input:\n",
      "slacidar ignidulcn teh sreggid fo eht nehsilg over\n",
      "Output:\n",
      "slamisam ignidnuhp teh sergnir fo eht nehsilo ures\n",
      "================================================================================\n",
      "Average loss at step 2900: 0.641022 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.08\n",
      "Average loss at step 3000: 0.592467 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.83\n",
      "Input:\n",
      "lution and the sans culottes of the french revolut\n",
      "Reverse input:\n",
      "noitul dna eht snas settoluc of eht nerfhc tulover\n",
      "Output:\n",
      "noitdu dam eht ssan stolepsi of eht effohc rutolpe\n",
      "================================================================================\n",
      "Average loss at step 3100: 0.557685 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.68\n",
      "Average loss at step 3200: 0.495115 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.54\n",
      "Input:\n",
      "ion whilst the term is still used in a pejorative \n",
      "Reverse input:\n",
      "noi tslihw eht mret si llits udes ni a pevitaroje \n",
      "Output:\n",
      "noi tsihw  eht mret si llisi odes ni ab evitrapec \n",
      "================================================================================\n",
      "Average loss at step 3300: 0.480793 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.60\n",
      "Average loss at step 3400: 0.425727 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.54\n",
      "Input:\n",
      "way to describe any act that used violent means to\n",
      "Reverse input:\n",
      "yaw ot sedebirc yna tca taht udes neloivt snaem ot\n",
      "Output:\n",
      "yaw ot edsevics yla tac taht udes enliogt namse ot\n",
      "================================================================================\n",
      "Average loss at step 3500: 0.395528 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.46\n",
      "Average loss at step 3600: 0.410241 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.39\n",
      "Input:\n",
      " destroy the organization of society it has also b\n",
      "Reverse input:\n",
      " yortsed teh zinagronoita fo syteico ti sah osla b\n",
      "Output:\n",
      " yortsed teh zignaronoita fo sytcieb to sah soll a\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3700: 0.358983 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.41\n",
      "Average loss at step 3800: 0.337327 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.35\n",
      "Input:\n",
      "een taken up as a positive label by self defined a\n",
      "Reverse input:\n",
      "nee nekat pu sa a opevitis balle yb fles denifed a\n",
      "Output:\n",
      "nee esadi pu a si apevitis balle s seht  denifed a\n",
      "================================================================================\n",
      "Average loss at step 3900: 0.505492 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.40\n",
      "Average loss at step 4000: 0.306693 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.33\n",
      "Input:\n",
      "narchists the word anarchism is derived from the g\n",
      "Reverse input:\n",
      "stsihcran eht drow amsihcran is devired morf eht g\n",
      "Output:\n",
      "stsihcran eht drow amsihcran is deviles morf eht g\n",
      "================================================================================\n",
      "Average loss at step 4100: 0.292764 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.29\n",
      "Average loss at step 4200: 0.295617 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.40\n",
      "Input:\n",
      "reek without archons ruler chief king anarchism as\n",
      "Reverse input:\n",
      "keer ohtiwtu snohcra relur ihcfe gnik namsihcra sa\n",
      "Output:\n",
      "keer ohtiwno slaihcr relur ihcfe gnia nomsihcar sa\n",
      "================================================================================\n",
      "Average loss at step 4300: 0.265446 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.35\n",
      "Average loss at step 4400: 0.257302 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.28\n",
      "Input:\n",
      " a political philosophy is the belief that rulers \n",
      "Reverse input:\n",
      " a citilopla osolihpyhp si eht feileb htta srelur \n",
      "Output:\n",
      " a citilopla osloihpyhp si eht etiweh tata srelur \n",
      "================================================================================\n",
      "Average loss at step 4500: 0.249219 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.26\n",
      "Average loss at step 4600: 0.245549 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.30\n",
      "Input:\n",
      "are unnecessary and should be abolished although t\n",
      "Reverse input:\n",
      "era ecennuyrass dna dluohs eb dehsiloba hguohtla t\n",
      "Output:\n",
      "era ecnenuyrass dna dluohs eb dehsilbuf hguohtla t\n",
      "================================================================================\n",
      "Average loss at step 4700: 0.232962 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.22\n",
      "Average loss at step 4800: 0.206974 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.34\n",
      "Input:\n",
      "here are differing interpretations of what this me\n",
      "Reverse input:\n",
      "ereh era dgnireffi itaterpretnsnoi fo hwta siht em\n",
      "Output:\n",
      "ereh era dgnireffi itatretcelbsnoi fo hwta siht em\n",
      "================================================================================\n",
      "Average loss at step 4900: 0.216018 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.22\n",
      "Average loss at step 5000: 0.199300 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.22\n",
      "Input:\n",
      "ans anarchism also refers to related social moveme\n",
      "Reverse input:\n",
      "sna hcranamsi osla rsrefe ot rdetale coslai emevom\n",
      "Output:\n",
      "sna hcranamsi osla rsrefe ot rdetael coslai emevom\n",
      "================================================================================\n",
      "Average loss at step 5100: 0.148260 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.17\n",
      "Average loss at step 5200: 0.137814 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.14\n",
      "Input:\n",
      "nts that advocate the elimination of authoritarian\n",
      "Reverse input:\n",
      "stn taht aetacovd hte tanimilenoi fo tuanairatiroh\n",
      "Output:\n",
      "stn taht aetapyno hte tanimilenoi fo tuanairtufrah\n",
      "================================================================================\n",
      "Average loss at step 5300: 0.137234 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.14\n",
      "Average loss at step 5400: 0.128142 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.15\n",
      "Input:\n",
      " institutions particularly the state the word anar\n",
      "Reverse input:\n",
      " itutitsnisno citrapylralu eht etats eht drow rana\n",
      "Output:\n",
      " ititutsnisno citrapyllaru eht etats eht draw ronk\n",
      "================================================================================\n",
      "Average loss at step 5500: 0.128944 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.14\n",
      "Average loss at step 5600: 0.136432 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.12\n",
      "Input:\n",
      "chy as most anarchists use it does not imply chaos\n",
      "Reverse input:\n",
      "yhc sa somt sihcranast esu ti seod ton iylpm soahc\n",
      "Output:\n",
      "yhc sa somt sihcranast esu ti seod ton iylpm sohac\n",
      "================================================================================\n",
      "Average loss at step 5700: 0.130830 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.12\n",
      "Average loss at step 5800: 0.135650 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.12\n",
      "Input:\n",
      " nihilism or anomie but rather a harmonious anti a\n",
      "Reverse input:\n",
      " msilihin ro eimona tub rehtar a inomrahsuo itna a\n",
      "Output:\n",
      " msihilfn ro eimona tub rehtar a inomrahsuo itna a\n",
      "================================================================================\n",
      "Average loss at step 5900: 0.130260 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.16\n",
      "Average loss at step 6000: 0.126544 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.10\n",
      "Input:\n",
      "uthoritarian society in place of what are regarded\n",
      "Reverse input:\n",
      "iratirohtuna yteicos ni ecalp fo tahw rae dedrager\n",
      "Output:\n",
      "irtarihpovna yteicos ni ecalp fo tahw rae dedrager\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6001  \n",
    "summary_frequency = 100\n",
    "initial_step = np.zeros((batch_size, vocabulary_size)) # Equivalent of GO \n",
    "num_unrollings = 10\n",
    "#train_batches = SequenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "\n",
    "def next_step(session, last_step, index):\n",
    "    (letter, _, do, ds) = last_step\n",
    "    feed_dict = {\n",
    "        decoder_sample_input: letter,\n",
    "        decoder_sample_output: do,\n",
    "        decoder_sample_state: ds,\n",
    "    }\n",
    "    do, ds, prev_let =  session.run([sample_output,\n",
    "                        sample_state,\n",
    "                        sample_prediction], feed_dict=feed_dict)\n",
    "    probabilities = prev_let[0].tolist()\n",
    "    s = sorted(probabilities, reverse=True)\n",
    "    prob = s[index]\n",
    "    index = probabilities.index(prob)\n",
    "    letter = np.zeros((1, vocabulary_size))\n",
    "    index = np.argmax(probabilities)\n",
    "    letter[0, index] = 1\n",
    "    logprob = np.log(prob)\n",
    "    return (letter, logprob, do, ds)\n",
    "                \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized\\n')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        (encoder_batches, decoder_batches) = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[encoder_train_inputs[i]] = encoder_batches[i]\n",
    "            feed_dict[train_labels[i]] = decoder_batches[i]\n",
    "            if i == 0:\n",
    "                feed_dict[decoder_train_inputs[i]] = initial_step\n",
    "            else:\n",
    "                feed_dict[decoder_train_inputs[i]] = decoder_batches[i - 1]\n",
    "\n",
    "        (_, l, predictions, lr) = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate],\n",
    "            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step == 100 :\n",
    "            print('step'.format(step))\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            summary = (step, mean_loss, lr)\n",
    "            print('Average loss at step %d: %f learning rate: %f' % summary)\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(decoder_batches)\n",
    "            perplexity = float(np.exp(logprob(predictions, labels)))\n",
    "            print('Minibatch perplexity: %.2f' % perplexity)\n",
    "        if step % (summary_frequency * 2) == 0:\n",
    "            # Generate some samples.\n",
    "            valid_sentence = []\n",
    "            output_sentence = []\n",
    "            input_sentence = []\n",
    "            for _ in range(5):\n",
    "                feed_dict_enc = {}\n",
    "\n",
    "                (e_batches, d_batches) = valid_batches.next()\n",
    "                \n",
    "                valid_sentence += batches2string(d_batches)\n",
    "                input_sentence += batches2string(e_batches)\n",
    "                \n",
    "                for i in range(num_unrollings):\n",
    "                    feed_dict_enc[sample_inputs[i]] = e_batches[i]\n",
    "\n",
    "                enc_output, enc_state = session.run([encoder_sample_output, encoder_sample_state],\n",
    "                                  feed_dict=feed_dict_enc)\n",
    "                N = 2\n",
    "                sequences = ()\n",
    "                for _ in range(N):\n",
    "                    letter = np.zeros((1, vocabulary_size))\n",
    "                    sequences += (((letter, 0, enc_output, enc_state), ), )\n",
    "\n",
    "                for _ in range(num_unrollings):\n",
    "                    new_sequences = ()\n",
    "                    for sequence in sequences:\n",
    "                        last_step = sequence[-1]\n",
    "                        for i_type in range(N):\n",
    "                            current_step = next_step(session, last_step, i_type)\n",
    "                            new_sequences += (sequence + (current_step, ), )\n",
    "\n",
    "                    sequences = new_sequences\n",
    "\n",
    "                sums = []\n",
    "                for (ind, sequence) in enumerate(sequences):\n",
    "                    logprob_sum = 0\n",
    "                    for step1 in sequence:\n",
    "                        logprob_sum  += step1[1]\n",
    "                    sums.append((ind, logprob_sum))\n",
    "\n",
    "                sums = sorted(sums, key=lambda s: s[1], reverse=True)\n",
    "                sequence = []\n",
    "                index = sums[0][0]\n",
    "                for step_ind in sequences[index]:\n",
    "                    sequence.append(step_ind[0])   \n",
    "                    \n",
    "                    decoded_sequence =  sequence[1:]\n",
    "                \n",
    "                output_sentence += batches2string(decoded_sequence)\n",
    "            \n",
    "            print('Input:')\n",
    "            print(''.join(input_sentence))\n",
    "            print('Reverse input:')\n",
    "            print(''.join(valid_sentence))\n",
    "            print('Output:')\n",
    "            print(''.join(output_sentence))\n",
    "            print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Improve quallity of batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add seq2seq model from tf models library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq with new batches\n",
    "We will use equal batches with 32 lenght\n",
    "To create eqaul sentence we will use padding(?) and eos(.), that means the end of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've redefined id2char and char2id for to add (eos, pad). Vocabulary size increased by 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z .  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 3\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '?':\n",
    "        return vocabulary_size - 1\n",
    "    elif char == '.':  \n",
    "        return vocabulary_size - 2\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid == vocabulary_size - 1:\n",
    "        return '?'\n",
    "    if dictid == vocabulary_size - 2:\n",
    "        return '.'\n",
    "    elif dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(27), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocate socia.??', 'when military governments fai.??', 'lleria arches national park p.??', ' abbeys and monasteries index.??', 'married urraca princess of ca.??', 'hel and richard baer h provid.??', 'y and liturgical language amo.??', 'ay opened for passengers in d.??', 'tion from the national media .??', 'migration took place during t.??', 'new york other well known man.??', 'he boeing seven six seven a w.??', 'e listed with a gloss coverin.??', 'eber has probably been one of.??', 'o be made to recognize single.??', 'yer who received the first ca.??', 'ore significant than in jerse.??', 'a fierce critic of the povert.??', ' two six eight in signs of hu.??', 'aristotle s uncaused cause so.??', 'ity can be lost as in denatur.??', ' and intracellular ice format.??', 'tion of the size of the input.??', 'dy to pass him a stick to pul.??', 'f certain drugs confusion ina.??', 'at it will take to complete a.??', 'e convince the priest of the .??', 'ent told him to name it fort .??', 'ampaign and barred attempts b.??', 'rver side standard formats fo.??', 'ious texts such as esoteric c.??', 'o capitalize on the growing p.??', 'a duplicate of the original d.??', 'gh ann es d hiver one nine ei.??', 'ine january eight march eight.??', 'ross zero the lead character .??', 'cal theories classical mechan.??', 'ast instance the non gm compa.??', ' dimensional analysis fundame.??', 'most holy mormons believe the.??', 't s support or at least not p.??', 'u is still disagreed upon by .??', 'e oscillating system example .??', 'o eight subtypes based on the.??', 'of italy languages the offici.??', 's the tower commission at thi.??', 'klahoma press one nine three .??', 'erprise linux suse linux ente.??', 'ws becomes the first daily co.??', 'et in a nazi concentration ca.??', 'the fabian society nehru wish.??', 'etchy to relatively stiff fro.??', ' sharman networks sharman s s.??', 'ised emperor hirohito to begi.??', 'ting in political initiatives.??', 'd neo latin most of these aut.??', 'th risky riskerdoo ricky rica.??', 'encyclopedic overview of math.??', 'fense the air component of ar.??', 'duating from acnm accredited .??', 'treet grid centerline externa.??', 'ations more than any other st.??', 'appeal of devotional buddhism.??', 'si have made such devices pos.??']\n",
      "[' anarchism originated as a t.???', 'phy is the belief that ruler.???']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=31\n",
    "\n",
    "class EqualSentenceBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        num_unrollings = np.random.randint(self._num_unrollings - 6, self._num_unrollings)\n",
    "        for step in range(num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        eos = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        eos[:,-2] = 1.\n",
    "        batches.append(eos)\n",
    "        pad = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        pad[:,-1] = 1.\n",
    "        for step in range(num_unrollings, self._num_unrollings):\n",
    "            batches.append(pad.copy())\n",
    "        return batches\n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = EqualSentenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = EqualSentenceBatchGenerator(valid_text, 2, num_unrollings)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['been taken up as a positive.????', 's nihilism or anomie but ra.????']\n",
      "['neeb nekat pu sa a evitisop.????', 's msilihin ro eimona tub ar.????']\n"
     ]
    }
   ],
   "source": [
    "def id2probs(idx):\n",
    "    probs = np.zeros(shape=vocabulary_size, dtype=np.float)\n",
    "    probs[idx] = 1.\n",
    "    return probs\n",
    "\n",
    "def chars2probs(characters):\n",
    "    ids = [char2id(c) for c in characters]\n",
    "    return map(id2probs, ids)\n",
    "\n",
    "# def string2lbatches(string):\n",
    "#     sent = ' '.join(map(lambda word: word[::-1], sent.split(' ')))\n",
    "#     return chars2probs('.'.join([sent, tail]))\n",
    "\n",
    "def string2labels(string):\n",
    "    body, tail = string.split('.')\n",
    "    body = ' '.join(map(lambda word: word[::-1], body.split(' ')))\n",
    "    return chars2probs('.'.join([body, tail]))\n",
    "\n",
    "def reverse_words(batches):\n",
    "    strings = batches2string(np.array(batches))\n",
    "    labels =[]\n",
    "    for i, s in enumerate(strings):\n",
    "        labels.append((list(string2labels(s))))\n",
    "    #labels = np.array(map(string2labels, strings))\n",
    "    labels = np.array(labels)\n",
    "    return labels.transpose((1,0,2))\n",
    "sent = valid_batches.next()\n",
    "rev_s = reverse_words(sent)\n",
    "print(batches2string(sent))\n",
    "print(batches2string(rev_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    encoder_train_inputs = []\n",
    "    decoder_train_inputs = []\n",
    "    train_labels = []\n",
    "    train_weights = []\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        encoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        decoder_train_inputs.append( #this one will be a labels\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "        train_weights.append(tf.placeholder(tf.float32, shape=1))\n",
    " \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    # State\n",
    "    # Bias\n",
    "    \n",
    "    #Encoder\n",
    "    x_enc = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_enc = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_enc = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    #Decoder\n",
    "    x_dec = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_dec = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_dec = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    encoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    decoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    decoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "      # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    w_decoder = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b_decoder = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell_encoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_enc)+ tf.matmul(o, m_enc) + bias_enc\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell_decoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_dec)+ tf.matmul(o, m_dec) + bias_dec\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop of encoder.\n",
    "    output_logits = list()\n",
    "    encoder_logits = list()\n",
    "    for input_encoder in encoder_train_inputs:\n",
    "        encoder_output, encoder_state = lstm_cell_encoder(input_encoder, encoder_output, encoder_state)\n",
    "\n",
    "    encoder_logits = tf.nn.xw_plus_b(encoder_output, w,b)\n",
    "    decoder_output = encoder_output    \n",
    "    decoder_state = encoder_state\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        decoder_output, decoder_state = lstm_cell_decoder(encoder_logits, decoder_output, decoder_state)\n",
    "        encoder_logits = tf.nn.xw_plus_b(decoder_output, w_decoder,b_decoder)\n",
    "        output_logits.append(encoder_logits)   \n",
    "    \n",
    "    train_labels = decoder_train_inputs\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(decoder_output),\n",
    "                                 saved_state.assign(decoder_state)]):\n",
    "        loss = tf.reduce_mean(\n",
    "              tf.nn.softmax_cross_entropy_with_logits(logits = tf.concat(output_logits[0], 0), labels=tf.concat(train_labels[0], 0))*train_weights[0])\n",
    "        for logits, labels, weight in zip(output_logits[1:], train_labels[1:], train_weights[1:]):\n",
    "            loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits, labels=labels)) * weight\n",
    "\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                1.0, global_step, 3000, 0.8, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = output_logits\n",
    "\n",
    "    sample_inputs = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        sample_inputs.append(tf.placeholder(tf.float32, shape=[1,vocabulary_size]))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    sample_output = saved_sample_output\n",
    "    sample_state = saved_sample_state\n",
    "    for input_sample_encoder in sample_inputs:\n",
    "        sample_output, sample_state = lstm_cell_encoder(input_sample_encoder, sample_output, sample_state)\n",
    "\n",
    "    sample_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    \n",
    "    sample_output_decoder = saved_sample_output\n",
    "    sample_state_decoder = sample_state\n",
    "    sample_inp = sample_logits\n",
    "    sample_output_logits = []\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        sample_output_decoder, sample_state_decoder = lstm_cell_decoder(\n",
    "            sample_inp, sample_output_decoder, sample_state_decoder)\n",
    "        sample_inp = tf.nn.xw_plus_b(sample_output_decoder, w_decoder, b_decoder)\n",
    "        sample_output_logits.append(sample_inp)\n",
    "\n",
    "    sample_prediction = sample_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Average loss at step 0: 3.414634 learning rate: 2.000000\n",
      " anarchism originated as a t.???\n",
      "   iiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "Average loss at step 600: 2.890870 learning rate: 2.000000\n",
      "Average loss at step 1200: 2.676989 learning rate: 2.000000\n",
      "erm of abuse first used agains.?\n",
      "ee       ???????????????????????\n",
      "Average loss at step 1800: 2.435932 learning rate: 2.000000\n",
      "Average loss at step 2400: 2.411158 learning rate: 2.000000\n",
      "t early working class radica.???\n",
      "e e a    t.?????????????????????\n",
      "Average loss at step 3000: 2.338820 learning rate: 2.000000\n",
      "Average loss at step 3600: 2.318368 learning rate: 2.000000\n",
      "ls including the diggers of .???\n",
      "ee ee                   oa t.???\n",
      "Average loss at step 4200: 2.302365 learning rate: 2.000000\n",
      "Average loss at step 4800: 2.304360 learning rate: 2.000000\n",
      "the english revolution and th.??\n",
      "e t ene   f.????????????????????\n",
      "Average loss at step 5400: 2.307390 learning rate: 1.800000\n",
      "Average loss at step 6000: 2.277037 learning rate: 1.800000\n",
      "e sans culottes of the fren.????\n",
      "e enii e ee             ee..????\n",
      "Average loss at step 6600: 2.263355 learning rate: 1.800000\n",
      "Average loss at step 7200: 2.237051 learning rate: 1.800000\n",
      "ch revolution whilst the ter.???\n",
      "no seiaa                 et..???\n",
      "Average loss at step 7800: 2.208667 learning rate: 1.800000\n",
      "Average loss at step 8400: 2.230630 learning rate: 1.800000\n",
      "m is still used in a pejo.??????\n",
      "s rei a seie         eeep.??????\n",
      "Average loss at step 9000: 2.183648 learning rate: 1.800000\n",
      "Average loss at step 9600: 2.180414 learning rate: 1.800000\n",
      "rative way to describe any a.???\n",
      "ene fo ent ent             s.???\n",
      "Average loss at step 10200: 2.161498 learning rate: 1.620000\n",
      "Average loss at step 10800: 2.706561 learning rate: 1.620000\n",
      "ct that used violent means to .?\n",
      "se??????????????????????????????\n",
      "Average loss at step 11400: 2.847484 learning rate: 1.620000\n",
      "Average loss at step 12000: 3.816777 learning rate: 1.620000\n",
      "destroy the organization of so.?\n",
      "sa                            .?\n",
      "Average loss at step 12600: 2.942928 learning rate: 1.620000\n",
      "Average loss at step 13200: 2.785579 learning rate: 1.620000\n",
      "ciety it has also been taken.???\n",
      "seiii eh  e                    ?\n",
      "Average loss at step 13800: 2.756677 learning rate: 1.620000\n",
      "Average loss at step 14400: 2.849133 learning rate: 1.620000\n",
      " up as a positive label by.?????\n",
      " ei en                          \n",
      "Average loss at step 15000: 2.861114 learning rate: 1.458000\n",
      "Average loss at step 15600: 2.836073 learning rate: 1.458000\n",
      " self defined anarchists .??????\n",
      " rae     ???????????????????????\n",
      "Average loss at step 16200: 2.832068 learning rate: 1.458000\n",
      "Average loss at step 16800: 2.939653 learning rate: 1.458000\n",
      "the word anarchism is derived.??\n",
      "eoi              ???????????????\n",
      "Average loss at step 17400: 2.884121 learning rate: 1.458000\n",
      "Average loss at step 18000: 3.092971 learning rate: 1.458000\n",
      " from the greek without a.??????\n",
      "   ?????????????????????????????\n",
      "Average loss at step 18600: 2.805852 learning rate: 1.458000\n",
      "Average loss at step 19200: 2.834504 learning rate: 1.458000\n",
      "rchons ruler chief king a.??????\n",
      "rhaee?il?leeeeeeeeeeeeeeeeeeeeee\n",
      "Average loss at step 19800: 2.877562 learning rate: 1.458000\n",
      "Average loss at step 20400: 2.709095 learning rate: 1.312200\n",
      "narchism as a political phil.???\n",
      "yn                          .???\n",
      "Average loss at step 21000: 2.384776 learning rate: 1.312200\n",
      "Average loss at step 21600: 2.387087 learning rate: 1.312200\n",
      "osophy is the belief that rule.?\n",
      "sni                           .?\n",
      "Average loss at step 22200: 2.349884 learning rate: 1.312200\n",
      "Average loss at step 22800: 2.329010 learning rate: 1.312200\n",
      "rs are unnecessary and sh.??????\n",
      "so eo                    .??????\n",
      "Average loss at step 23400: 2.312213 learning rate: 1.312200\n",
      "Average loss at step 24000: 2.296817 learning rate: 1.312200\n",
      "ould be abolished although the.?\n",
      "dloo ee                       .?\n",
      "Average loss at step 24600: 2.283717 learning rate: 1.312200\n",
      "Average loss at step 25200: 2.274150 learning rate: 1.180980\n",
      "re are differing interpretati.??\n",
      "ee ee                        .??\n",
      "Average loss at step 25800: 2.254999 learning rate: 1.180980\n",
      "Average loss at step 26400: 2.264836 learning rate: 1.180980\n",
      "ons of what this means anar.????\n",
      "sna eni                    .????\n",
      "Average loss at step 27000: 2.234881 learning rate: 1.180980\n",
      "Average loss at step 27600: 2.249216 learning rate: 1.180980\n",
      "chism also refers to related s.?\n",
      "siii                          .?\n",
      "Average loss at step 28200: 2.264594 learning rate: 1.180980\n",
      "Average loss at step 28800: 2.231029 learning rate: 1.180980\n",
      "ocial movements that advo.??????\n",
      "lnii                     .??????\n",
      "Average loss at step 29400: 2.232018 learning rate: 1.180980\n",
      "Average loss at step 30000: 2.228219 learning rate: 1.062882\n",
      "cate the elimination of au.?????\n",
      "eeia dnt                  .?????\n",
      "Average loss at step 30600: 2.223615 learning rate: 1.062882\n",
      "Average loss at step 31200: 2.213796 learning rate: 1.062882\n",
      "thoritarian institutions p.?????\n",
      "nni                       .?????\n",
      "Average loss at step 31800: 2.229596 learning rate: 1.062882\n",
      "Average loss at step 32400: 2.214225 learning rate: 1.062882\n",
      "articularly the state the.??????\n",
      "yllla                    .??????\n",
      "Average loss at step 33000: 2.193925 learning rate: 1.062882\n",
      "Average loss at step 33600: 2.204850 learning rate: 1.062882\n",
      " word anarchy as most anarchi.??\n",
      " seraae                      .??\n",
      "Average loss at step 34200: 2.188669 learning rate: 1.062882\n",
      "Average loss at step 34800: 2.181528 learning rate: 1.062882\n",
      "sts use it does not imply.??????\n",
      "stt eht eeia             .??????\n",
      "Average loss at step 35400: 2.176682 learning rate: 0.956594\n",
      "Average loss at step 36000: 2.174269 learning rate: 0.956594\n",
      " chaos nihilism or anomie .?????\n",
      " seee                     .?????\n",
      "Average loss at step 36600: 2.177165 learning rate: 0.956594\n",
      "Average loss at step 37200: 2.179741 learning rate: 0.956594\n",
      "but rather a harmonious an.?????\n",
      "tra eei                   .?????\n",
      "Average loss at step 37800: 2.186437 learning rate: 0.956594\n",
      "Average loss at step 38400: 2.169856 learning rate: 0.956594\n",
      "ti authoritarian society in.????\n",
      "ii eeae                    .????\n",
      "Average loss at step 39000: 2.171250 learning rate: 0.956594\n",
      "Average loss at step 39600: 2.162386 learning rate: 0.956594\n",
      " place of what are regarded a.??\n",
      " eaii                        .??\n",
      "Average loss at step 40200: 2.155767 learning rate: 0.860934\n",
      "Average loss at step 40800: 2.173234 learning rate: 0.860934\n",
      "s authoritarian political stru.?\n",
      "s nii                        c.?\n",
      "Average loss at step 41400: 2.156024 learning rate: 0.860934\n",
      "Average loss at step 42000: 2.158177 learning rate: 0.860934\n",
      "ctures and coercive economic.???\n",
      "serree                     a.???\n",
      "Average loss at step 42600: 2.137604 learning rate: 0.860934\n",
      "Average loss at step 43200: 2.153882 learning rate: 0.860934\n",
      " instituti anarchism origin.????\n",
      " giii                      .????\n",
      "Average loss at step 43800: 2.149031 learning rate: 0.860934\n",
      "Average loss at step 44400: 2.140117 learning rate: 0.860934\n",
      "ated as a term of abuse fir.????\n",
      "dett si e                 a.????\n",
      "Average loss at step 45000: 2.154344 learning rate: 0.774841\n",
      "Average loss at step 45600: 2.155169 learning rate: 0.774841\n",
      "st used against early work.?????\n",
      "ts dnia                  a.?????\n",
      "Average loss at step 46200: 2.137622 learning rate: 0.774841\n",
      "Average loss at step 46800: 2.141047 learning rate: 0.774841\n",
      "ing class radicals including .??\n",
      "gni saaa aaaaaaaaaaaaaaaaaaa .??\n",
      "Average loss at step 47400: 2.131721 learning rate: 0.774841\n",
      "Average loss at step 48000: 2.136472 learning rate: 0.774841\n",
      "the diggers of the english .????\n",
      "eht srit                   .????\n",
      "Average loss at step 48600: 2.136904 learning rate: 0.774841\n",
      "Average loss at step 49200: 2.131795 learning rate: 0.774841\n",
      "revolution and the sans cul.????\n",
      "noia                       .????\n",
      "Average loss at step 49800: 2.124491 learning rate: 0.774841\n",
      "Average loss at step 50400: 2.121335 learning rate: 0.697357\n",
      "ottes of the french revolu.?????\n",
      "setit e e e               .?????\n",
      "Average loss at step 51000: 2.125928 learning rate: 0.697357\n",
      "Average loss at step 51600: 2.128193 learning rate: 0.697357\n",
      "tion whilst the term is s.??????\n",
      "noit tnet                .??????\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52200: 2.137934 learning rate: 0.697357\n",
      "Average loss at step 52800: 2.115049 learning rate: 0.697357\n",
      "till used in a pejorative way .?\n",
      "lnit deia                     .?\n",
      "Average loss at step 53400: 2.126393 learning rate: 0.697357\n",
      "Average loss at step 54000: 2.129993 learning rate: 0.697357\n",
      "to describe any act that .??????\n",
      "ot enii                  .??????\n",
      "Average loss at step 54600: 2.116684 learning rate: 0.697357\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-de6782f224f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         (_, l, predictions, lr) = session.run(\n\u001b[1;32m     24\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 80001  \n",
    "summary_frequency = 600\n",
    "train_batches = EqualSentenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = EqualSentenceBatchGenerator(valid_text, 1, num_unrollings)\n",
    "initial_step = np.zeros((batch_size, vocabulary_size)) # Equivalent of GO \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized\\n')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        encoder_batches = train_batches.next()\n",
    "        decoder_batches = reverse_words(encoder_batches)\n",
    "        encoder_batches = encoder_batches[::-1]\n",
    "        weights = np.ones(len(encoder_batches))\n",
    "        weights = weights / weights.sum()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[encoder_train_inputs[i]] = encoder_batches[i]\n",
    "            feed_dict[decoder_train_inputs[i]] = decoder_batches[i]\n",
    "            feed_dict[train_weights[i]] = [weights[i]]\n",
    "\n",
    "        (_, l, predictions, lr) = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate],\n",
    "            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            summary = (step, mean_loss, lr)\n",
    "            print('Average loss at step %d: %f learning rate: %f' % summary)\n",
    "            mean_loss = 0\n",
    "        if step % (2*summary_frequency) == 0:\n",
    "            val_batches = valid_batches.next()\n",
    "            print(''.join(batches2string(val_batches)))\n",
    "            val_batches = val_batches[::-1]\n",
    "            feed_valid = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_valid[sample_inputs[i]] = val_batches[i]\n",
    "            val_prediction = session.run([sample_prediction], feed_dict=feed_valid)\n",
    "            val_prediction = val_prediction[0]\n",
    "            print(''.join(batches2string(np.array(val_prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Average loss at step 0: 3.429824 learning rate: 1.000000\n",
      " anarchism originated as a.?????\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "Average loss at step 600: 3.022642 learning rate: 1.000000\n",
      "Average loss at step 1200: 2.854655 learning rate: 1.000000\n",
      " term of abuse first used.??????\n",
      "  e                    ?????????\n",
      "Average loss at step 1800: 2.783953 learning rate: 1.000000\n",
      "Average loss at step 2400: 2.412181 learning rate: 1.000000\n",
      " against early working class .??\n",
      " sniee                      .???\n",
      "Average loss at step 3000: 2.324256 learning rate: 0.800000\n",
      "Average loss at step 3600: 2.280398 learning rate: 0.800000\n",
      "radicals including the digger.??\n",
      "seiiiiii siiai  e        .??????\n",
      "Average loss at step 4200: 2.238727 learning rate: 0.800000\n",
      "Average loss at step 4800: 2.205738 learning rate: 0.800000\n",
      "s of the english revolution an.?\n",
      " eit eht eni                  .?\n",
      "Average loss at step 5400: 2.176040 learning rate: 0.800000\n",
      "Average loss at step 6000: 2.151462 learning rate: 0.640000\n",
      "d the sans culottes of th.??????\n",
      "t eht enii               .??????\n",
      "Average loss at step 6600: 2.106481 learning rate: 0.640000\n",
      "Average loss at step 7200: 2.074638 learning rate: 0.640000\n",
      "e french revolution whilst .????\n",
      "e seisee snetros seiteo    .????\n",
      "Average loss at step 7800: 2.034680 learning rate: 0.640000\n",
      "Average loss at step 8400: 2.009459 learning rate: 0.640000\n",
      "the term is still used in a p.??\n",
      "eht seet so neieeiia    a  a.???\n",
      "Average loss at step 9000: 1.994478 learning rate: 0.512000\n",
      "Average loss at step 9600: 1.938465 learning rate: 0.512000\n",
      "ejorative way to describe.??????\n",
      "e roiorop snoros sniioa ..??????\n",
      "Average loss at step 10200: 1.921577 learning rate: 0.512000\n",
      "Average loss at step 10800: 1.915242 learning rate: 0.512000\n",
      " any act that used violent .????\n",
      " nni daa sraw eht seaa  a  .????\n",
      "Average loss at step 11400: 1.873774 learning rate: 0.512000\n",
      "Average loss at step 12000: 1.883673 learning rate: 0.409600\n",
      "means to destroy the organiza.??\n",
      "sooor no seitoow eht seii  a..??\n",
      "Average loss at step 12600: 1.834854 learning rate: 0.409600\n",
      "Average loss at step 13200: 1.834510 learning rate: 0.409600\n",
      "tion of society it has al.??????\n",
      "nooa fo saaaaaa no daa a..??????\n",
      "Average loss at step 13800: 1.813100 learning rate: 0.409600\n",
      "Average loss at step 14400: 1.803188 learning rate: 0.409600\n",
      "so been taken up as a positiv.??\n",
      "ht seof se aa seiea  aa  a  .???\n",
      "Average loss at step 15000: 1.795279 learning rate: 0.327680\n",
      "Average loss at step 15600: 1.771294 learning rate: 0.327680\n",
      "e label by self defined a.??????\n",
      "e lelia si seaa saaaaac s.??????\n",
      "Average loss at step 16200: 1.761947 learning rate: 0.327680\n",
      "Average loss at step 16800: 1.755424 learning rate: 0.327680\n",
      "narchists the word anarchis.????\n",
      "sttttttaf eht ttat dniiaa .?????\n",
      "Average loss at step 17400: 1.772066 learning rate: 0.327680\n",
      "Average loss at step 18000: 1.729832 learning rate: 0.262144\n",
      "m is derived from the greek w.??\n",
      "l ni seet oe seet eht teiet f.??\n",
      "Average loss at step 18600: 1.709690 learning rate: 0.262144\n",
      "Average loss at step 19200: 1.702102 learning rate: 0.262144\n",
      "ithout archons ruler chief kin.?\n",
      "ttito soi snotnes s eie n  eh.??\n",
      "Average loss at step 19800: 1.702604 learning rate: 0.262144\n",
      "Average loss at step 20400: 1.701825 learning rate: 0.262144\n",
      "g anarchism as a political ph.??\n",
      "g gaiiiaaia ac a laiaaaaa aaa.??\n",
      "Average loss at step 21000: 1.693904 learning rate: 0.209715\n",
      "Average loss at step 21600: 1.680845 learning rate: 0.209715\n",
      "ilosophy is the belief that.????\n",
      "taiattac sah eht setaae sa.?????\n",
      "Average loss at step 22200: 1.673032 learning rate: 0.209715\n",
      "Average loss at step 22800: 1.667225 learning rate: 0.209715\n",
      " rulers are unnecessary and.????\n",
      " seerof ena seiiiiieeee  ac.????\n",
      "Average loss at step 23400: 1.636337 learning rate: 0.209715\n",
      "Average loss at step 24000: 1.647314 learning rate: 0.167772\n",
      " should be abolished altho.?????\n",
      " eleter ra et elalaaaaa aa.?????\n",
      "Average loss at step 24600: 1.646383 learning rate: 0.167772\n",
      "Average loss at step 25200: 1.640974 learning rate: 0.167772\n",
      "ugh there are differing inte.???\n",
      "wre errht eno eninnnnee nii.????\n",
      "Average loss at step 25800: 1.628036 learning rate: 0.167772\n",
      "Average loss at step 26400: 1.628911 learning rate: 0.167772\n",
      "rpretations of what this me.????\n",
      "sroitorrerrr saht eh  sa  .?????\n",
      "Average loss at step 27000: 1.621736 learning rate: 0.134218\n",
      "Average loss at step 27600: 1.614721 learning rate: 0.134218\n",
      "ans anarchism also refers to .??\n",
      "snan gniiaanoa ena s rot oo .???\n",
      "Average loss at step 28200: 1.607280 learning rate: 0.134218\n",
      "Average loss at step 28800: 1.590589 learning rate: 0.134218\n",
      "related social movements that.??\n",
      "decaali saitoc snitec oo ttht.??\n",
      "Average loss at step 29400: 1.599554 learning rate: 0.134218\n",
      "Average loss at step 30000: 1.608425 learning rate: 0.107374\n",
      " advocate the elimination of.???\n",
      " etattaoa eht naiiiiiinee na.???\n",
      "Average loss at step 30600: 1.596561 learning rate: 0.107374\n",
      "Average loss at step 31200: 1.592763 learning rate: 0.107374\n",
      " authoritarian institutions p.??\n",
      " noitittttot ot sniiiirerro  .??\n",
      "Average loss at step 31800: 1.590312 learning rate: 0.107374\n",
      "Average loss at step 32400: 1.579005 learning rate: 0.107374\n",
      "articularly the state the wor.??\n",
      "ylrrerreero  eht etaat etht h.??\n",
      "Average loss at step 33000: 1.583391 learning rate: 0.085899\n",
      "Average loss at step 33600: 1.570317 learning rate: 0.085899\n",
      "d anarchy as most anarchis.?????\n",
      "d llaalaa  s taas saaa aaa.?????\n",
      "Average loss at step 34200: 1.582265 learning rate: 0.085899\n",
      "Average loss at step 34800: 1.569555 learning rate: 0.085899\n",
      "ts use it does not imply ch.????\n",
      "ht ses si tlaa dna snii na.?????\n",
      "Average loss at step 35400: 1.565609 learning rate: 0.085899\n",
      "Average loss at step 36000: 1.573446 learning rate: 0.068719\n",
      "aos nihilism or anomie but rat.?\n",
      "sris tnorrop ob elaa oo eht a.??\n",
      "Average loss at step 36600: 1.551881 learning rate: 0.068719\n",
      "Average loss at step 37200: 1.557082 learning rate: 0.068719\n",
      "her a harmonious anti authorit.?\n",
      "reht a saiiireroa eht aiiiiic.??\n",
      "Average loss at step 37800: 1.551178 learning rate: 0.068719\n",
      "Average loss at step 38400: 1.549788 learning rate: 0.068719\n",
      "arian society in place of what.?\n",
      "naiaa naitaoc fo eliha oo eah..?\n",
      "Average loss at step 39000: 1.556705 learning rate: 0.054976\n",
      "Average loss at step 39600: 1.542712 learning rate: 0.054976\n",
      " are regarded as authoritaria.??\n",
      " era dehreret ot aaaaaaaaaaac.??\n",
      "Average loss at step 40200: 1.542678 learning rate: 0.054976\n",
      "Average loss at step 40800: 1.544932 learning rate: 0.054976\n",
      "n political structures and co.??\n",
      "n sasitiae  salaeeaee  na eaa.??\n",
      "Average loss at step 41400: 1.545682 learning rate: 0.054976\n",
      "Average loss at step 42000: 1.550849 learning rate: 0.043980\n",
      "ercive economic instituti ana.??\n",
      "enita soet seeneenee enin ena.??\n",
      "Average loss at step 42600: 1.546176 learning rate: 0.043980\n",
      "Average loss at step 43200: 1.530951 learning rate: 0.043980\n",
      "rchism originated as a term o.??\n",
      "htiia setret ee deiteea   e  .??\n",
      "Average loss at step 43800: 1.526480 learning rate: 0.043980\n",
      "Average loss at step 44400: 1.532406 learning rate: 0.043980\n",
      "f abuse first used against ear.?\n",
      "f ecaac si si tsof enininei no.?\n",
      "Average loss at step 45000: 1.523715 learning rate: 0.035184\n",
      "Average loss at step 45600: 1.530292 learning rate: 0.035184\n",
      "ly working class radicals inc.??\n",
      "hs ynilal si si lal ni    aa.???\n",
      "Average loss at step 46200: 1.521451 learning rate: 0.035184\n",
      "Average loss at step 46800: 1.503893 learning rate: 0.035184\n",
      "luding the diggers of the.??????\n",
      "gnina eht tretret ei ehtt.??????\n",
      "Average loss at step 47400: 1.519205 learning rate: 0.035184\n",
      "Average loss at step 48000: 1.528735 learning rate: 0.028148\n",
      " english revolution and th.?????\n",
      " clilaaa noitaootoc dna et.?????\n",
      "Average loss at step 48600: 1.518481 learning rate: 0.028148\n",
      "Average loss at step 49200: 1.511850 learning rate: 0.028148\n",
      "e sans culottes of the french.??\n",
      "e saei setteeee ft eht eee ec.??\n",
      "Average loss at step 49800: 1.506777 learning rate: 0.028148\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001  \n",
    "summary_frequency = 600\n",
    "train_batches = EqualSentenceBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = EqualSentenceBatchGenerator(valid_text, 1, num_unrollings)\n",
    "initial_step = np.zeros((batch_size, vocabulary_size)) # Equivalent of GO \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized\\n')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        encoder_batches = train_batches.next()\n",
    "        decoder_batches = reverse_words(encoder_batches)\n",
    "        encoder_batches = encoder_batches[::-1]\n",
    "        weights = np.ones(len(encoder_batches))\n",
    "        weights = weights / weights.sum()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[encoder_train_inputs[i]] = encoder_batches[i]\n",
    "            feed_dict[decoder_train_inputs[i]] = decoder_batches[i]\n",
    "            feed_dict[train_weights[i]] = [weights[i]]\n",
    "\n",
    "        (_, l, predictions, lr) = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate],\n",
    "            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            summary = (step, mean_loss, lr)\n",
    "            print('Average loss at step %d: %f learning rate: %f' % summary)\n",
    "            mean_loss = 0\n",
    "        if step % (2*summary_frequency) == 0:\n",
    "            val_batches = valid_batches.next()\n",
    "            print(''.join(batches2string(val_batches)))\n",
    "            val_batches = val_batches[::-1]\n",
    "            feed_valid = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_valid[sample_inputs[i]] = val_batches[i]\n",
    "            val_prediction = session.run([sample_prediction], feed_dict=feed_valid)\n",
    "            val_prediction = val_prediction[0]\n",
    "            print(''.join(batches2string(np.array(val_prediction))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's modified our seq2seq model\n",
    "We combine word representation with lstm cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 315138], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Most common reversed words (+UNK) [['UNK', 315138], ('eht', 1061396), ('fo', 593677), ('dna', 416629), ('eno', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
      "Sample reversed data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "vocabulary_size = 2**16\n",
    "num_unrollings = 10\n",
    "words = text.split()\n",
    "reversed_words = []\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "for i in range(len(words)):\n",
    "    reversed_words.append(words[i][::-1])\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "rev_data, rev_count, dictionary_r, reverse_dictionary_r = build_dataset(reversed_words)\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Most common reversed words (+UNK)', rev_count[:5])\n",
    "\n",
    "print('Sample data', data[:10])\n",
    "print('Sample reversed data', rev_data[:10])\n",
    "\n",
    "del words, reversed_words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'floccinaucinihilipilification'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dictionary.keys(), key=lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size_seq2seq = 128\n",
    "valid_data = data[:valid_size_seq2seq]\n",
    "train_data = data[valid_size_seq2seq:]\n",
    "rev_valid_data = rev_data[:valid_size_seq2seq]\n",
    "rev_train_data = rev_data[valid_size_seq2seq:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class SequenceWordsBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._text[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array cooonsists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def seqbatches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into string\n",
    "      representation.\n",
    "      \"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [' '.join(x) for x in zip(s, [reverse_dictionary[c] for c in b])]\n",
    "    return s\n",
    "\n",
    "def seqbatches2string_reverse(batches):\n",
    "    \"\"\"Convert a sequence of batches back into string\n",
    "    representation.\n",
    "    \"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [' '.join(x) for x in zip(s, [reverse_dictionary_r[c] for c in b])]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism originated']\n",
      "[' msihcrana detanigiro']\n"
     ]
    }
   ],
   "source": [
    "#Create new train batches\n",
    "train_batches = SequenceWordsBatchGenerator(train_data, batch_size, num_unrollings)\n",
    "reverse_train_batches = SequenceWordsBatchGenerator(rev_train_data, batch_size, num_unrollings)\n",
    "valid_batches = SequenceWordsBatchGenerator(valid_data, 1, 1)\n",
    "reverse_valid_batches = SequenceWordsBatchGenerator(rev_valid_data, 1, 1)\n",
    "print(seqbatches2string(valid_batches.next()))\n",
    "print(seqbatches2string_reverse(reverse_valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b79142b6d5bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mencoder_sample_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mencoder_sample_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_encoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_embeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         encoder_sample_output, encoder_sample_state = lstm_cell_encoder(input_encoder,\n\u001b[1;32m    127\u001b[0m                                                             encoder_sample_output, encoder_sample_state)\n",
      "\u001b[0;32m/home/vitar/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \"\"\"\n\u001b[0;32m--> 504\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Tensor' object is not iterable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not iterable."
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "vocabulary_size = len(count)\n",
    "embedding_size = 128\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    encoder_train_inputs = []\n",
    "    decoder_train_inputs = []\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=False)\n",
    "    embeddings_rev = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=False)\n",
    "    \n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        encoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size]))\n",
    "        decoder_train_inputs.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size]))\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size]))\n",
    "    encoded_inputs = list()\n",
    "    for inputs in encoder_train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, tf.cast(inputs, tf.int32))\n",
    "        encoded_inputs.append(embed)\n",
    "    \n",
    "    train_inputs = encoded_inputs[:num_unrollings-1]\n",
    "    decoded_inputs = list()\n",
    "    for inputs in decoder_train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings_rev, tf.cast(inputs, tf.int32))\n",
    "        decoded_inputs.append(embed)\n",
    "    dec_train_inputs = decoded_inputs[:num_unrollings-1]\n",
    "\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    # State\n",
    "    # Bias\n",
    "    \n",
    "    #Encoder\n",
    "    x_enc = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_enc = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_enc = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    #Decoder\n",
    "    x_dec = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    m_dec = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    bias_dec = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    encoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    decoder_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    decoder_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell_encoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_enc)+ tf.matmul(o, m_enc) + bias_enc\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell_decoder(i, o, state):\n",
    "        matmul_part = tf.matmul(i, x_dec)+ tf.matmul(o, m_dec) + bias_dec\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop of encoder.\n",
    "    outputs = list()\n",
    "    for input_encoder in train_inputs:\n",
    "        encoder_output, encoder_state = lstm_cell_encoder(input_encoder, encoder_output, encoder_state)\n",
    "    \n",
    "    GO = []\n",
    "    GO.append(tf.Variable(tf.zeros([batch_size, embedding_size]), trainable=False))\n",
    "    decoder_output = encoder_output    \n",
    "    decoder_state = encoder_state    \n",
    "    for input_decoder in GO + dec_train_inputs:\n",
    "        if i == GO:\n",
    "            decoder_output, decoder_state = lstm_cell_decoder(input_decoder, encoder_output, encoder_state)\n",
    "        else:\n",
    "            decoder_output, decoder_state = lstm_cell_decoder(input_decoder, decoder_output, decoder_state)\n",
    "        outputs.append(decoder_output)\n",
    "    \n",
    "    # Classifier.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, 1], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=tf.transpose(logits)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_inputs = []\n",
    "    for _ in range(num_unrollings):\n",
    "        sample_inputs.append(tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    encoder_sample_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    encoder_sample_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    for input_encoder in sample_embeds:\n",
    "        sample_embeds = tf.nn.embedding_lookup(embeddings, input_decoder)\n",
    "        encoder_sample_output, encoder_sample_state = lstm_cell_encoder(input_encoder,\n",
    "                                                            encoder_sample_output, encoder_sample_state)\n",
    "    \n",
    "    decoder_sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "    decoder_sample_state = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    decoder_sample_output = tf.placeholder(tf.float32, shape=[1, num_nodes])\n",
    "    sample_output, sample_state = lstm_cell_decoder(decoder_sample_input, decoder_sample_output, decoder_sample_state)\n",
    " \n",
    "    with tf.control_dependencies([sample_output,\n",
    "                                sample_state]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
