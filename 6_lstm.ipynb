{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "        batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295208 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "yatheajtllatrskhmep invotlq  q pi t laf ga sahnpknylpwfl ggrqaneoe ugtpmtaf p s \n",
      "ynhvajhiovfb dowdvwpoewm d xxeewxi x pbpa  sim azf e edp tghb b dvel mxsehn e ke\n",
      "pntjis ebjdtcxto ja gpbootnceupoowghydoycijjtdnfy jkmw izminq  q vosuranleav tog\n",
      "efnuaicetxeprafgbymdnnls sdnftlteteiydgohjnpqsjh g yfsntlailngtvnznnmwraeuer swe\n",
      "i wcgkhznzsicgcobohnoqd cauhtu e eehlkfrfmixu rvlrspy  eq cscuclgcjosttr neyjvzc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.588530 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.246915 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.094018 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.996125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.929049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.904875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.858147 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.817738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.827896 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.827739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "giee romery with the sjedice as defrremic is and will americal aceipling zeno of\n",
      "ing perilev of dores of nuctrakuovinory in aolly vid groininily heasuthic mo of \n",
      "hile have when haried of the lating of ma atorions artain on noqulles nice saace\n",
      "lies the the the nature three seven his brids of e to thind to speld fiaded or t\n",
      "legis incesfibuts pose migfes notic oill minsterm to promited isels it ledery wi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.773581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.753957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.734725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.750867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1500: 1.734888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.747994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.713807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.676606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.647259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.698557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "vine of jeaged sckovemallys hindufil the bornain bungances porsed useng anstrel \n",
      "coling the eiglonoss in a dive the parouse mouse hanautity parted the amses mija\n",
      "jem heid bown discision often itserent cited resectors and tandticislad sucheish\n",
      "gorage the gutyursts condictine romield in no be devited mastom dose for pos gat\n",
      "ming caroil it slassing jost inkent zero four one retumunne desse is landugts th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.688800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.684880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.645900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400: 1.662937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.684959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.656358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.656332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.650387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.651811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.650454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "nizal langoit it is froy sounce was bould sevon fors repable officially eaulic d\n",
      "zer the farchabon recolds their one nine seven two zero one nine zero melicyoses\n",
      "x used theory that to first lape decouxtly in auscak two at innose atsinced to f\n",
      "is do gro sceet thereally reuin landstac dome cassing to with dublation puncil r\n",
      "tive two there wayed hussity of blime freazius maracian indian is cacks exa and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.626096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.647592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3300: 1.636553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.670837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.664399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.667626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.644710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.643552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.635254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.652572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "scy rophist inclasse seliding from nomels discovering todby strupate compbered a\n",
      "han one nine th end the peaces blatt he descryefts suppen butly in the burlys la\n",
      "ired lipp agastagle is discencus meir caye one johia a rume anderwaberable murni\n",
      "ver mass gamesed part quevered shoug one nine nine byhind the king by event dist\n",
      "zer and ninger ecognen alboin the georate fromszion wold metic priece to are the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4100: 1.631786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.636934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.616193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.606888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.615679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.614758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.624560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.627901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.630480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.608328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "gan mutholh called a cain through and the sage hall after litisunian howish musi\n",
      "galled between the bed than be israbling and psypheile of s swith was masis inde\n",
      "k in one nine six eight one nine one six three zero one six five years cominal p\n",
      "emer first the mikam by distriptance others that killed thig consifted ston five\n",
      "d the with main pocts for oftenray this parces for chyple gang ar often mughts w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.603532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.587737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.576345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.579209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.566709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.577041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.567916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.580292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.572056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.543825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "mill dare posedent an armet of cert seven two zero with samal the tevired an and\n",
      "af the war as dolas was all da pendital or racote restraish and sueg thewe on th\n",
      "n six jasables heablines by confinition the quanters ponitary markex on the many\n",
      "on amac brash umis treen one nine seven nine six three bholow is a conno than th\n",
      "s and arny since actival word states reflocitian heak mangy is game otceen press\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.565541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.537013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.544169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.540127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.558921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.591132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.577313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.581632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.572044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "p gen g in an and distranamor pernabial like and typical articule john tox sh th\n",
      "tha suckele such days pisw in the litelell and populate the chablisms of the mor\n",
      "pen from a hilled the replyntlening emperor milleally it series see brouly as th\n",
      "y and an emeriarch that five dregal saching it is notes iboly waves one zero zer\n",
      "zer geo claimed byughtrick ensirension on neight sinning the qutles whatt whel h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(27), Dimension(256)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296088 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "gnyruvqeoxko oiz z hw ilv bpb kx lidyvglnpfdergeahl  nfsnavviqloeqlvsatisknrtadr\n",
      "o drqklnk gz gqi uhn qiigo kudqiilajlvw  n lk nm wedhe eiecarhxsqhlnt zteojcaoef\n",
      "dvjectz fnh kirlialdyaxebmcjetm tfibcdxh k iba li vqtglbd uidsooojc cuc did e  e\n",
      "weadhedhwsnlhwtlcqrfiallfr if qncuitr ea tpbnrewsduoseiffzislcocsr kekeswkqrttly\n",
      "n sbe kan icp vcfnt v v xcid  ozniqtmosoj ejzpeim mgc agwt  joenefucpafgdzp joll\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.578006 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.241599 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.080559 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.993168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.997385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.921496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.896213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.874717 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.863912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.795488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "x astin time the biede of knotiper stages ay masses diyy the hresm the beto it c\n",
      "f was didpacomore of purbon etceol and for opter plepular avreiked to a noveb se\n",
      "ricise orrenure one one seven in foullalic guen mut one nine ofter molugagraing \n",
      "her has as woin the fishce erees the retyed the cantucate maboroly cotuence erea\n",
      "zed in thisoldwoum codnive tic as a pardicens aytelsco of in the pautef extecks \n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1100: 1.771558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1200: 1.792623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1300: 1.776950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1400: 1.749590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1500: 1.738979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1600: 1.724243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1700: 1.744537 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.706883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.715391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.719643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "mal amedia per protectional lindent showed and gill broods houlations to crablia\n",
      "jendacucty s twok daugise by the valct fing thser shaneage bald ada pherem tees \n",
      " quath vollows on kitt serouss frectional diecan these geart as an aselicated ca\n",
      "jeed albymalianca deone afterly deam korterdest jistion of he fast adea of low h\n",
      "mentsers ameriated alterentter mides ellapitions reserice be ansechesed enation \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.704503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.679675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.684407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2400: 1.686753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.704629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.678998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.694463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2800: 1.654941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.658249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.669374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "lical isolan celler arcilural compare the lands assail existed by or the seallin\n",
      "maned aunor ablected houchlored in five verter bus the dissuriture making s la t\n",
      "ffer four mathorization of where likely of twu coulling smacle servivizally remo\n",
      "ned it daierc the coulder and arterts of lecks is haltter thermour eight nine th\n",
      "figelo colonender one nine nine nine nine zero shing hulta and emplicaping bed t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.657744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.653052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.637909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.637971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3500: 1.629433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.634803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.634721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.632055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.620987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.621898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "chillowale have moracic becagarth ollogave inditheme tain medicions offertiially\n",
      "kions composbats swenberly address brigh war heasy marschled charsherazafas the \n",
      "le keeving eayal to cleasite butchade compiticiesmilm hebbolly of the sectors th\n",
      "fac is the e breatius simpute they edito appirily reganast that mased and a migl\n",
      "re three one the german also headshasels ricemm gruith found to diffine american\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.619685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4200: 1.609960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4300: 1.591466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.618586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4500: 1.626669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.630820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.597774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.583371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.599418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.623120 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "s aduative of gthinamation seccent gomania atposes the offscy r for autor sha mo\n",
      "kf evely are moxitin increas and neater a ennekhione in perlip the to s policion\n",
      "ed the each hole artiures entile one five zero sell two five in the themen of so\n",
      "ys it wotter both puldimal adchand percent for space of the estimgy computatiect\n",
      "ns actomal chant holving such the cidian regiols internota the sade incree year \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.635124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.634271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.593521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.580340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.607977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.569673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.576006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.597034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6000: 1.563030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "o countly and of troagus alo is a relates such as a quilat sudpup vari and show \n",
      "jon he sive estension light however this one nine zero zero est in s linder can \n",
      "th ocrewist that by empioration for related tanchis is ituation from me fighteld\n",
      "b were the skempon inadapped of his cantrick immootal humprish publiornes upodes\n",
      " most featives wille by macles andsce and gemer to zen the for the mode sezen pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6100: 1.585241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6200: 1.599419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.611827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6400: 1.639336 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.635514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.604675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.594729 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.573748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.572458 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.584984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "hes his terry a that prish in can the lefeet who junding incies mempose returned\n",
      "nge sient also at a buri than naturally feftical greek universy one nine six lif\n",
      "ken is d known burde of into word notecer riggh nafcer assed rengrayly amminia t\n",
      "ra filled construenter he reloger eloperes the hervy s manum curled theorits the\n",
      "ods and edeciated two the color example succks si publishe of them during offect\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 7100: 1.555530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7200: 1.592411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7300: 1.601814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7400: 1.578915 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7500: 1.562399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7600: 1.589043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7700: 1.579928 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7800: 1.581206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7900: 1.586072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8000: 1.564950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "================================================================================\n",
      "os samora s coner soon of the be time taxim jondaria a invieence febtiar on scet\n",
      "rair of alsyor the foles intension ths exalaton and for andificid p de mechanism\n",
      "zades and offendon of the alcoce o through nat no chure relational rylimbus vide\n",
      "phonal the some to kuing longs west years purnts of a pursuit nowleduen of ave t\n",
      "t ageious of several of nites two six seven elead downame which on the midnas yi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8100: 1.571132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8200: 1.582282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8300: 1.591929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8400: 1.569817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8500: 1.567689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8600: 1.551054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8700: 1.562797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 8800: 1.588976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 8900: 1.581452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9000: 1.586009 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "per consimerisma general drween anchmance if sull eight sions four seven fill wh\n",
      "re contrives which they of game india attayen polind explication unifour but ame\n",
      "eish in the cluited assertious sometimes within critives k frolloo marulata scot\n",
      "by and imple the k one eight and nocle reart teperots of the san griffer the was\n",
      "zed imbrisis minctor soud that ansible clao ziskmis sbarletdery the world raxamm\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9100: 1.590926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9200: 1.595583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9300: 1.590074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9400: 1.573143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9500: 1.592387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9600: 1.581841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9700: 1.577665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9800: 1.577118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9900: 1.580554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10000: 1.597251 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ing may went from and de the sajound out a and torad are grany of lewneng into w\n",
      "orser erlywara the demored tom use an amonging than laceman in the grapked by th\n",
      "s others after geodx struelian statforts the ground pachice work societatoospfic\n",
      "pian fegen be game by madicatimmans scate shortanes and the runtil supported its\n",
      "beh markg from the jec his creymons over famurayils hold removyble conditional f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 10100: 1.565126 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 10200: 1.590093 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10300: 1.584949 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10400: 1.582896 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10500: 1.573755 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10600: 1.583306 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10700: 1.562903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10800: 1.560848 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 10900: 1.555347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11000: 1.588585 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "roughs on the film brit perconstion to he sect form s r coduce cre often and qui\n",
      "z well verquesthic official rocumber under where has a legacy adent are offerabl\n",
      "ists augnrising its happown law overpoussia alen for spected theadine therefull \n",
      "ng anahaphies to actor show the diel those of troal intublishos fort three tho a\n",
      "pirited of some created unior the ong starcy mal yack and the but lease roy an o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11100: 1.587801 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11200: 1.579634 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11300: 1.541607 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11400: 1.569012 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11500: 1.562717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11600: 1.557078 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11700: 1.573717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11800: 1.580734 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11900: 1.601813 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12000: 1.597944 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "zed with two show allow processosia as a one nine zero zero prepequence pignorsa\n",
      "y the under pow reevishing mankex one seven chillows ugnotia year s parof it to \n",
      "vimians such is tilspos forch and leftitsour south some trittulsot of is atril l\n",
      "phanz idels without supromussed governind jurnes schalrone similating a between \n",
      "que involver on wassfal weater of histiclats reference of the with the like stro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12100: 1.575750 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12200: 1.590594 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12300: 1.569764 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12400: 1.566402 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12500: 1.560497 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12600: 1.551376 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12700: 1.589705 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12800: 1.581224 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 12900: 1.588941 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 13000: 1.549479 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "quers exportained town does of the weenned by one seven two rulation hera two co\n",
      "variex in denawion xpy in one thromengies for two did put of whomery the country\n",
      "d fine fullion clowesh force thinkfore central ugs a light into suppsorey all fi\n",
      "zen numbers and the invent at dickather martagers linking here war at show chour\n",
      "xard were lown numbers allahoph errisken suka ppuce studed in one nine seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13100: 1.593628 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13200: 1.593347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13300: 1.577060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13400: 1.592521 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13500: 1.603572 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13600: 1.579617 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13700: 1.578702 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13800: 1.590926 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13900: 1.632133 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14000: 1.615489 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "coneand theory do on hemed teachw and defins two zero zero in west a loss by the\n",
      "s rot national name in the nument shack booic recalas european pechured is parti\n",
      "m ith darined is consucunds carg these as questor statum and in corpore the empl\n",
      "ter milkning or a histam their rause ownersite one nine eight five four stat hal\n",
      "ishing mated out convlems mornenon from forg had fearcis lavach calannisult hurr\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14100: 1.596736 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14200: 1.573053 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14300: 1.580519 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14400: 1.595691 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14500: 1.600329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14600: 1.570035 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14700: 1.592610 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14800: 1.574893 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14900: 1.592432 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 15000: 1.586098 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "des as that it day ten inchodence is the untogrites and mains other anba first m\n",
      "rap purreating the meupples oft devolution yet over is parode islands it still r\n",
      "valur been particularly of the causes different of the triffed by industric nall\n",
      "nn norve dease to as also of computer finnistric commanncement production of its\n",
      "orkage ip our screepon on one nine four films ireland of these american in great\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15100: 1.573534 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15200: 1.575539 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15300: 1.570134 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15400: 1.576043 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15500: 1.600311 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15600: 1.593948 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15700: 1.570269 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15800: 1.611362 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15900: 1.617383 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16000: 1.577232 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "way are by simpley one connicife of eight nine inine were inceed s by s eck spec\n",
      "ners american rock though northan initatesely as of byology aber purrdegations o\n",
      "ley opyp that air that sources pression me concerfine two zero s autax socially \n",
      "k by the in mijornism is ethornism hur charge skater as one two is stromping ope\n",
      " faile promoties merslar institite the one four midribivies and toward dowing ol\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16100: 1.570450 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16200: 1.566831 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16300: 1.608270 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16400: 1.591806 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16500: 1.566013 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16600: 1.567127 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16700: 1.579081 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16800: 1.595272 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16900: 1.552456 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 17000: 1.557044 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "jicalling of serving of a williorical point edust clins are b overel as general \n",
      "chnorece bill an athapuria one cable clown contablepomnody contembrally dungh in\n",
      "jornwa of the low play storea ligg a wings tver sean pilined directle in eurlibe\n",
      "zer headly into s the and arrikans micronizalations in is the made in the been o\n",
      "ubluel gremack sociation over fasta absour the mocars and terman the cultural th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 17001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
