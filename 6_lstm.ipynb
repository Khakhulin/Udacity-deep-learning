{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "        batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295208 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "yatheajtllatrskhmep invotlq  q pi t laf ga sahnpknylpwfl ggrqaneoe ugtpmtaf p s \n",
      "ynhvajhiovfb dowdvwpoewm d xxeewxi x pbpa  sim azf e edp tghb b dvel mxsehn e ke\n",
      "pntjis ebjdtcxto ja gpbootnceupoowghydoycijjtdnfy jkmw izminq  q vosuranleav tog\n",
      "efnuaicetxeprafgbymdnnls sdnftlteteiydgohjnpqsjh g yfsntlailngtvnznnmwraeuer swe\n",
      "i wcgkhznzsicgcobohnoqd cauhtu e eehlkfrfmixu rvlrspy  eq cscuclgcjosttr neyjvzc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.588530 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.246915 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.094018 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.996125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.929049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.904875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.858147 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.817738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.827896 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.827739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "giee romery with the sjedice as defrremic is and will americal aceipling zeno of\n",
      "ing perilev of dores of nuctrakuovinory in aolly vid groininily heasuthic mo of \n",
      "hile have when haried of the lating of ma atorions artain on noqulles nice saace\n",
      "lies the the the nature three seven his brids of e to thind to speld fiaded or t\n",
      "legis incesfibuts pose migfes notic oill minsterm to promited isels it ledery wi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.773581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.753957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.734725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.750867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1500: 1.734888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.747994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.713807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.676606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.647259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.698557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "vine of jeaged sckovemallys hindufil the bornain bungances porsed useng anstrel \n",
      "coling the eiglonoss in a dive the parouse mouse hanautity parted the amses mija\n",
      "jem heid bown discision often itserent cited resectors and tandticislad sucheish\n",
      "gorage the gutyursts condictine romield in no be devited mastom dose for pos gat\n",
      "ming caroil it slassing jost inkent zero four one retumunne desse is landugts th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.688800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.684880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.645900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400: 1.662937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.684959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.656358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.656332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.650387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.651811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.650454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "nizal langoit it is froy sounce was bould sevon fors repable officially eaulic d\n",
      "zer the farchabon recolds their one nine seven two zero one nine zero melicyoses\n",
      "x used theory that to first lape decouxtly in auscak two at innose atsinced to f\n",
      "is do gro sceet thereally reuin landstac dome cassing to with dublation puncil r\n",
      "tive two there wayed hussity of blime freazius maracian indian is cacks exa and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.626096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.647592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3300: 1.636553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.670837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.664399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.667626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.644710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.643552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.635254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.652572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "scy rophist inclasse seliding from nomels discovering todby strupate compbered a\n",
      "han one nine th end the peaces blatt he descryefts suppen butly in the burlys la\n",
      "ired lipp agastagle is discencus meir caye one johia a rume anderwaberable murni\n",
      "ver mass gamesed part quevered shoug one nine nine byhind the king by event dist\n",
      "zer and ninger ecognen alboin the georate fromszion wold metic priece to are the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4100: 1.631786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.636934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.616193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.606888 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.615679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.614758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.624560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.627901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.630480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.608328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "gan mutholh called a cain through and the sage hall after litisunian howish musi\n",
      "galled between the bed than be israbling and psypheile of s swith was masis inde\n",
      "k in one nine six eight one nine one six three zero one six five years cominal p\n",
      "emer first the mikam by distriptance others that killed thig consifted ston five\n",
      "d the with main pocts for oftenray this parces for chyple gang ar often mughts w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.603532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.587737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.576345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.579209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.566709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.577041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.567916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.580292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.572056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.543825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "mill dare posedent an armet of cert seven two zero with samal the tevired an and\n",
      "af the war as dolas was all da pendital or racote restraish and sueg thewe on th\n",
      "n six jasables heablines by confinition the quanters ponitary markex on the many\n",
      "on amac brash umis treen one nine seven nine six three bholow is a conno than th\n",
      "s and arny since actival word states reflocitian heak mangy is game otceen press\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.565541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.537013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.544169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.540127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.558921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.591132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.577313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.581632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.572044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "p gen g in an and distranamor pernabial like and typical articule john tox sh th\n",
      "tha suckele such days pisw in the litelell and populate the chablisms of the mor\n",
      "pen from a hilled the replyntlening emperor milleally it series see brouly as th\n",
      "y and an emeriarch that five dregal saching it is notes iboly waves one zero zer\n",
      "zer geo claimed byughtrick ensirension on neight sinning the qutles whatt whel h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(27), Dimension(256)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296088 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "gnyruvqeoxko oiz z hw ilv bpb kx lidyvglnpfdergeahl  nfsnavviqloeqlvsatisknrtadr\n",
      "o drqklnk gz gqi uhn qiigo kudqiilajlvw  n lk nm wedhe eiecarhxsqhlnt zteojcaoef\n",
      "dvjectz fnh kirlialdyaxebmcjetm tfibcdxh k iba li vqtglbd uidsooojc cuc did e  e\n",
      "weadhedhwsnlhwtlcqrfiallfr if qncuitr ea tpbnrewsduoseiffzislcocsr kekeswkqrttly\n",
      "n sbe kan icp vcfnt v v xcid  ozniqtmosoj ejzpeim mgc agwt  joenefucpafgdzp joll\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.578006 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.241599 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.080559 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.993168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.997385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.921496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.896213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.874717 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.863912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.795488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "x astin time the biede of knotiper stages ay masses diyy the hresm the beto it c\n",
      "f was didpacomore of purbon etceol and for opter plepular avreiked to a noveb se\n",
      "ricise orrenure one one seven in foullalic guen mut one nine ofter molugagraing \n",
      "her has as woin the fishce erees the retyed the cantucate maboroly cotuence erea\n",
      "zed in thisoldwoum codnive tic as a pardicens aytelsco of in the pautef extecks \n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1100: 1.771558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1200: 1.792623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1300: 1.776950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1400: 1.749590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1500: 1.738979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1600: 1.724243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1700: 1.744537 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.706883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.715391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.719643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "mal amedia per protectional lindent showed and gill broods houlations to crablia\n",
      "jendacucty s twok daugise by the valct fing thser shaneage bald ada pherem tees \n",
      " quath vollows on kitt serouss frectional diecan these geart as an aselicated ca\n",
      "jeed albymalianca deone afterly deam korterdest jistion of he fast adea of low h\n",
      "mentsers ameriated alterentter mides ellapitions reserice be ansechesed enation \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.704503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.679675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.684407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2400: 1.686753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.704629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.678998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.694463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2800: 1.654941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.658249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.669374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "lical isolan celler arcilural compare the lands assail existed by or the seallin\n",
      "maned aunor ablected houchlored in five verter bus the dissuriture making s la t\n",
      "ffer four mathorization of where likely of twu coulling smacle servivizally remo\n",
      "ned it daierc the coulder and arterts of lecks is haltter thermour eight nine th\n",
      "figelo colonender one nine nine nine nine zero shing hulta and emplicaping bed t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.657744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.653052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.637909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.637971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3500: 1.629433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.634803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.634721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.632055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.620987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.621898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "chillowale have moracic becagarth ollogave inditheme tain medicions offertiially\n",
      "kions composbats swenberly address brigh war heasy marschled charsherazafas the \n",
      "le keeving eayal to cleasite butchade compiticiesmilm hebbolly of the sectors th\n",
      "fac is the e breatius simpute they edito appirily reganast that mased and a migl\n",
      "re three one the german also headshasels ricemm gruith found to diffine american\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.619685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4200: 1.609960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4300: 1.591466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.618586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4500: 1.626669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.630820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.597774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.583371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.599418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.623120 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "s aduative of gthinamation seccent gomania atposes the offscy r for autor sha mo\n",
      "kf evely are moxitin increas and neater a ennekhione in perlip the to s policion\n",
      "ed the each hole artiures entile one five zero sell two five in the themen of so\n",
      "ys it wotter both puldimal adchand percent for space of the estimgy computatiect\n",
      "ns actomal chant holving such the cidian regiols internota the sade incree year \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.635124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.634271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.593521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.580340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.607977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.569673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.576006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.597034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6000: 1.563030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "o countly and of troagus alo is a relates such as a quilat sudpup vari and show \n",
      "jon he sive estension light however this one nine zero zero est in s linder can \n",
      "th ocrewist that by empioration for related tanchis is ituation from me fighteld\n",
      "b were the skempon inadapped of his cantrick immootal humprish publiornes upodes\n",
      " most featives wille by macles andsce and gemer to zen the for the mode sezen pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6100: 1.585241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6200: 1.599419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.611827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6400: 1.639336 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.635514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.604675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.594729 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.573748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.572458 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.584984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "hes his terry a that prish in can the lefeet who junding incies mempose returned\n",
      "nge sient also at a buri than naturally feftical greek universy one nine six lif\n",
      "ken is d known burde of into word notecer riggh nafcer assed rengrayly amminia t\n",
      "ra filled construenter he reloger eloperes the hervy s manum curled theorits the\n",
      "ods and edeciated two the color example succks si publishe of them during offect\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 7100: 1.555530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7200: 1.592411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7300: 1.601814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7400: 1.578915 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7500: 1.562399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7600: 1.589043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7700: 1.579928 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7800: 1.581206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7900: 1.586072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8000: 1.564950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "================================================================================\n",
      "os samora s coner soon of the be time taxim jondaria a invieence febtiar on scet\n",
      "rair of alsyor the foles intension ths exalaton and for andificid p de mechanism\n",
      "zades and offendon of the alcoce o through nat no chure relational rylimbus vide\n",
      "phonal the some to kuing longs west years purnts of a pursuit nowleduen of ave t\n",
      "t ageious of several of nites two six seven elead downame which on the midnas yi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8100: 1.571132 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8200: 1.582282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8300: 1.591929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8400: 1.569817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8500: 1.567689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 8600: 1.551054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8700: 1.562797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 8800: 1.588976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 8900: 1.581452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9000: 1.586009 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "per consimerisma general drween anchmance if sull eight sions four seven fill wh\n",
      "re contrives which they of game india attayen polind explication unifour but ame\n",
      "eish in the cluited assertious sometimes within critives k frolloo marulata scot\n",
      "by and imple the k one eight and nocle reart teperots of the san griffer the was\n",
      "zed imbrisis minctor soud that ansible clao ziskmis sbarletdery the world raxamm\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9100: 1.590926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9200: 1.595583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9300: 1.590074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9400: 1.573143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 9500: 1.592387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 9600: 1.581841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9700: 1.577665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 9800: 1.577118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 9900: 1.580554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10000: 1.597251 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ing may went from and de the sajound out a and torad are grany of lewneng into w\n",
      "orser erlywara the demored tom use an amonging than laceman in the grapked by th\n",
      "s others after geodx struelian statforts the ground pachice work societatoospfic\n",
      "pian fegen be game by madicatimmans scate shortanes and the runtil supported its\n",
      "beh markg from the jec his creymons over famurayils hold removyble conditional f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 10100: 1.565126 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 10200: 1.590093 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10300: 1.584949 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 10400: 1.582896 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10500: 1.573755 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10600: 1.583306 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10700: 1.562903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 10800: 1.560848 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 10900: 1.555347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11000: 1.588585 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "roughs on the film brit perconstion to he sect form s r coduce cre often and qui\n",
      "z well verquesthic official rocumber under where has a legacy adent are offerabl\n",
      "ists augnrising its happown law overpoussia alen for spected theadine therefull \n",
      "ng anahaphies to actor show the diel those of troal intublishos fort three tho a\n",
      "pirited of some created unior the ong starcy mal yack and the but lease roy an o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11100: 1.587801 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11200: 1.579634 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11300: 1.541607 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 11400: 1.569012 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11500: 1.562717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11600: 1.557078 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 11700: 1.573717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11800: 1.580734 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 11900: 1.601813 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12000: 1.597944 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "zed with two show allow processosia as a one nine zero zero prepequence pignorsa\n",
      "y the under pow reevishing mankex one seven chillows ugnotia year s parof it to \n",
      "vimians such is tilspos forch and leftitsour south some trittulsot of is atril l\n",
      "phanz idels without supromussed governind jurnes schalrone similating a between \n",
      "que involver on wassfal weater of histiclats reference of the with the like stro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12100: 1.575750 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12200: 1.590594 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12300: 1.569764 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12400: 1.566402 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12500: 1.560497 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12600: 1.551376 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 12700: 1.589705 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 12800: 1.581224 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 12900: 1.588941 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 13000: 1.549479 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "quers exportained town does of the weenned by one seven two rulation hera two co\n",
      "variex in denawion xpy in one thromengies for two did put of whomery the country\n",
      "d fine fullion clowesh force thinkfore central ugs a light into suppsorey all fi\n",
      "zen numbers and the invent at dickather martagers linking here war at show chour\n",
      "xard were lown numbers allahoph errisken suka ppuce studed in one nine seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13100: 1.593628 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13200: 1.593347 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13300: 1.577060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13400: 1.592521 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13500: 1.603572 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13600: 1.579617 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13700: 1.578702 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13800: 1.590926 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13900: 1.632133 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14000: 1.615489 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "coneand theory do on hemed teachw and defins two zero zero in west a loss by the\n",
      "s rot national name in the nument shack booic recalas european pechured is parti\n",
      "m ith darined is consucunds carg these as questor statum and in corpore the empl\n",
      "ter milkning or a histam their rause ownersite one nine eight five four stat hal\n",
      "ishing mated out convlems mornenon from forg had fearcis lavach calannisult hurr\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14100: 1.596736 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14200: 1.573053 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14300: 1.580519 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14400: 1.595691 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14500: 1.600329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14600: 1.570035 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14700: 1.592610 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14800: 1.574893 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14900: 1.592432 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 15000: 1.586098 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "des as that it day ten inchodence is the untogrites and mains other anba first m\n",
      "rap purreating the meupples oft devolution yet over is parode islands it still r\n",
      "valur been particularly of the causes different of the triffed by industric nall\n",
      "nn norve dease to as also of computer finnistric commanncement production of its\n",
      "orkage ip our screepon on one nine four films ireland of these american in great\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15100: 1.573534 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15200: 1.575539 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15300: 1.570134 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15400: 1.576043 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15500: 1.600311 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15600: 1.593948 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15700: 1.570269 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15800: 1.611362 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 15900: 1.617383 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16000: 1.577232 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "way are by simpley one connicife of eight nine inine were inceed s by s eck spec\n",
      "ners american rock though northan initatesely as of byology aber purrdegations o\n",
      "ley opyp that air that sources pression me concerfine two zero s autax socially \n",
      "k by the in mijornism is ethornism hur charge skater as one two is stromping ope\n",
      " faile promoties merslar institite the one four midribivies and toward dowing ol\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16100: 1.570450 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16200: 1.566831 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16300: 1.608270 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16400: 1.591806 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16500: 1.566013 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16600: 1.567127 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16700: 1.579081 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16800: 1.595272 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 16900: 1.552456 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 17000: 1.557044 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "jicalling of serving of a williorical point edust clins are b overel as general \n",
      "chnorece bill an athapuria one cable clown contablepomnody contembrally dungh in\n",
      "jornwa of the low play storea ligg a wings tver sean pilined directle in eurlibe\n",
      "zer headly into s the and arrikans micronizalations in is the made in the been o\n",
      "ubluel gremack sociation over fasta absour the mocars and terman the cultural th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: Cause of high demension in bigram model, we will use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 20\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "        train_labels.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for inputs in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input_idx = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input_idx)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 27)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_batches.next()[1]).shape\n",
    "(train_batches.next()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299145 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "oylgmcepa z fpvtywiumju   erw iboay cqer y voa sdc nidk us ql n a rcu tpanm xoxe\n",
      "klhlvbez empyziikatzpuiorc iwc wisuirghrtljrlm isen vuely  i vaypb  eteehhzsjpil\n",
      "rzkaoww ndlpomgfeg eotyueoanetze kr fimw  dbcn ez   efoexuupletyqthm  kdyzlcccqi\n",
      "clrsgyfditku o ttvd gh e z iiejrek  reiiaknnhr ueu e hiosh yeal bk qlso iomvzco \n",
      "khcpsaa odtjissm rravggbeixsq lqoq x    jlwltsecxxgvyo ayroh fbsvee aia in etisz\n",
      "================================================================================\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 100: 2.386494 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 200: 2.050725 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 300: 1.958449 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 400: 1.878871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 500: 1.835760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 600: 1.831754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 700: 1.800248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 800: 1.785890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.776740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1000: 1.756033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "ns mattenish the jeprest betill stuagly recopitaran elephised ciethnem impledact\n",
      "ments hark zero subsout articty working instanch hotro aneted ec vied anderewark\n",
      "dest sepathends ha on cetress liesis apperies then with of the stund mintirely s\n",
      "ressiol trantive of a stider blet there styt ie apbirse sixi one nine eight eihp\n",
      " veries then newsgest prosulty orgagenstos present senize s s the sted intenitio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1100: 1.754460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1200: 1.748571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1300: 1.726775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1400: 1.726786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1500: 1.727561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.702806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.702149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.673698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.692209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.704837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "gas by fiam wheer boops above seven yark sall and name carvice are falson and pe\n",
      "sern is jager bepen but three is and the bora and name in a sharge than or sense\n",
      "wifrer lub every person indiscient civiliond leveld spelfon one five five five o\n",
      "js apports one with and seen and bressent of supprotent six canamia an cater lif\n",
      "ne he used about seven zero ze of draweverl the sirah undernace of the proveriol\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.687647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.677252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2300: 1.683569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2400: 1.657063 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.660905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.664901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.659565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2800: 1.686172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2900: 1.661634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3000: 1.686554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bell respectual take sty all peomber stagn of one thrby the mecry see groupn isl\n",
      "ond while not relique lesignas uply it no fark ana gene exi one ni sound minity \n",
      "er meam of mudyling tourining clearates mones generage only their bik ups propin\n",
      "ov sig be premicia pusse of in ho moer which he neatric from chang janding in ot\n",
      "gest is don of his awarder ciese is island unor to or ofterne of hilly were orip\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.668552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.657001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3300: 1.629718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.643223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.659296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.638719 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.652325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.662170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.675188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4000: 1.652607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "yual and old und nnturys oulltian as voll and coblurgrate intendentencoly world \n",
      "ful the most s hellva and election s bond m they anks bill is bochsails ainspeno\n",
      "bands to pennyfo mis dame in dadical laures his chers informed source a boody mi\n",
      "vicul fabq one all aptem john bradia the analy dister for ammere of caulism one \n",
      "vers and luend a seven one mit ogrover shost to five four isso notations would o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4100: 1.655894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4200: 1.664900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4300: 1.639053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4400: 1.660346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4500: 1.636049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4600: 1.631538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4700: 1.627726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4800: 1.633210 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.634437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5000: 1.623997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ching the percession be kearges as they evil gpuce ordhlitipe to trivity this kh\n",
      "saured to the courquent as asternered as popel on films and lee while zero zero \n",
      "nap refirs staracted by selvcity use in anustimple in the creat mooking the one \n",
      "nicousivels vatels a macipmda withbrorizenter one sm pxt at a cans faman commits\n",
      "dize and to a states hes pace to it adone the formanistianly the the majored the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5100: 1.612377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5200: 1.586064 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.566667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.566389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5500: 1.562027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.613164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5700: 1.580942 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.591175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.586076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.578914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "acshitle ms other ravidual subsmen published a mysting one two mistorical north \n",
      "quation educally to no commus is gremights where war one nine seensane with doon\n",
      "domoson historited and exploiswantaclates states a free fick state allizy sane s\n",
      "prous then presidents founds compleri japoubbles of this germandurche distica im\n",
      "rouse asterneger and remains themosal pregutioned rality the civilization portab\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.558227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.571269 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300: 1.567709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.597509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.557523 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.577094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.591322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.550663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6900: 1.558134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.567879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "t and abblecution and high promia and cogropan for rucotforates the memanuan rec\n",
      "ker a construation or pose barrani of these and schoor of its of the vardimed yi\n",
      "be destancing for eventatinvenger distinction some habyen in the commannitionslo\n",
      "yberved diromians by higher accords are marvist which erigary labvement one nine\n",
      "hosal to jadlar it seaver programateriding and charvi zerood douced rugh lopting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7100: 1.586794 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7200: 1.568544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7300: 1.559103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7400: 1.575451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 7500: 1.586957 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7600: 1.567902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7700: 1.564885 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7800: 1.537815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7900: 1.559151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8000: 1.570239 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "xic instructiogual with attingly about the possed one simmate stricton land shg \n",
      "hoot films had and sone their willania in the emperor sahnt van cassemates peapd\n",
      "gayans between the lays was catakes the rights if timoshics by the elow from pos\n",
      "fecturositate with a fustroco series over palarsisculus gide and cape between gr\n",
      "ked the certed a propentene amveromy uncha strease as propuge rard in the his po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis=1)\n",
    "      feed_dict[train_labels[i]] = batches[i+1]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_idx: feed.argmax(axis=1)})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_idx: b[0].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b:**  Change graph for bigrams model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['nar']\n",
      "['rch']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 32\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        print(embed.shape)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    print(len(embeds))\n",
    "    print(len(train_labels))\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289089 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.82\n",
      "================================================================================\n",
      "wzkgfrmne gvme  ehgfah ykezsac teejk r  eypo nyhtvia  xxngdwyioy t nmaaodilnnswvc\n",
      "tbiprrpjz   ztkpscan hh woawatxxheovklocr   lepnzbwva eemkaridb w  ahictra i qe z\n",
      "qcc rxvloagsuw ssaifevcjnltctcehwp ofenygtnfgzl o  zbso afcsr iuwnthczv fxofesdax\n",
      "ds bisawakfgy p  wfcpkeo     ujdt eukdoktvylrb j pg d xawdg jxe dchkfbukfvyedcsto\n",
      "ixthpk oxnqanbtvmea bwplx me gtqjpwbtn tfgoearcg xiwz lh  nps vfphah suqgdoxxtprn\n",
      "================================================================================\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 100: 2.302889 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 200: 2.027483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 1.912729 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 1.855559 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 500: 1.808225 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 600: 1.827677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 700: 1.835468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 800: 1.815775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 900: 1.796186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 1000: 1.801397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "ccuuooueouuhiiiuuiueioaieeeuuueuitueuaeeieeeuoleaouuueeiuaeheuiieuroeiiuueoueioeu\n",
      "bveoeeeeyeeeieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      " jeeuoaauaeaauuinaeeaaahooararaeoaieahaoaoa uaacaraoioelaaauaoearraelaaeleaariaoa\n",
      " voaauaaiaaaaauaaaeie eaiaaeeeaiaaaeoeaieielieaeaeeaeaeeaeeeieiaiieaeeeeeeoeeaaea\n",
      "s abiaooaahmmsihbofmisdudoiacamooatttotskcaftiioobsambmbsmbmrmieopmnebbaddpsimfok\n",
      "================================================================================\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1100: 1.776139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1200: 1.753704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 1300: 1.783106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1400: 1.755056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 1500: 1.748084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1600: 1.743098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1700: 1.736587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1800: 1.751845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1900: 1.730856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2000: 1.735253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "fy pm lnpmcpqtrrnrutnprnzn tnlnmprnrntlpwnetcprlcnpnsntrnntpcmlnpttppntr r rumpls\n",
      "hfr     eo i r  rr    io  ri i ii rro  rriier ro oriii i riiooioi  oorrr im  iiro\n",
      "zroeeieieoeeeoooeoeooeoooeoeooeeeooeeeeeoeeooooioiooeooeeieoeeiaeeeeeoooooooioooe\n",
      "nkeies e sei seeeeyi ee e  s  saeee eae se   nisiiieee   emees  oi i a  l ee eee \n",
      "tonbmm ammgmm nms s mmprirmrgs b r rs sbsombr  ml   mbi mmbrm rbyaisrmb  mmsndrms\n",
      "================================================================================\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 2100: 1.721529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2200: 1.720902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 2300: 1.749266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2400: 1.751861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2500: 1.726976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2600: 1.713863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 2700: 1.715788 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 2800: 1.716926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 2900: 1.730110 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 3000: 1.749124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "q limpgcnmsmeccwrmeodijsmwyaittmspbrmtacgptealhmhrsachlccltmhbheommrhyoiaoaihalsh\n",
      "us       i  s        iu         o    a              ii o       u         c       \n",
      "vwb    enr  o o     o       ro    h              i  o   o     ooo       i        \n",
      " innnnrmnnnnnnnnnnnnnnnnnnnnnnnnmnnnnnnnnnnnsnnsnnnnnnnnnnnnd nnnnnnntns nnnnjnnt\n",
      "fj    si c   m   o  uwm      u c wh rhr   ssu  ihs usoau m s   mss ssocsup s wu  \n",
      "================================================================================\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3100: 1.767456 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 3200: 1.753534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 3300: 1.741110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 3400: 1.738429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3500: 1.756102 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 3600: 1.756591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 3700: 1.784526 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 3800: 1.716554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 3900: 1.732743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 4000: 1.743809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "cjuaeoaaoooluiusruirluaauuuerousvuuuuuuluuuuuuluuuvfuruiuuiauusuuuuiur uuiuumuurm\n",
      "gs         t                                       i                             \n",
      "o eamavpaprsprdcvjuoiproaujhvftmrrfcorltadcbrtrlrmnmigtriwcsarlliadcemmrauhfpowsp\n",
      "gs    t           tt                                         f e                 \n",
      "oji oe aai iee iuuiaiouaaeuheeeieimooeeiieu a yaai sieiaa asiuuiaaeeuiua euueoeei\n",
      "================================================================================\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 4100: 1.745477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 4200: 1.728003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 4300: 1.745660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 4400: 1.752761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 8.14\n",
      "Average loss at step 4500: 1.744508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 4600: 1.737338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 4700: 1.701295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 4800: 1.736424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 4900: 1.739173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 5000: 1.724270 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ce mvrlplpmipr rpdlppnlprprmpprdnbrpnrlrdndpllsrrtnmdmdrrvdsrn mrpdpdrpsllvpsdrdl\n",
      "xdoeie errirerm em me reuemeuom  iu ime meee  im eeeiemmu im me  leieem   o ml mi\n",
      "kbimaaneaaaauaelaaltamaeaauaeaoaeuaaomealleura laaonloarraaaelaaaraornepaammaarar\n",
      "xmaesasaneaaaceeaalalsiaaaepaaaddeaasapasaaaaoaeeapaassdeaaaalareaaeaaaaaaadgpea \n",
      "mnoia eaensaoauoaoo auocaia  aaoooauioioaeiooooauouuooo sooaueoueoaopapwoaeoaaaeo\n",
      "================================================================================\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 5100: 1.703443 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5200: 1.678977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 5300: 1.694666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5400: 1.660934 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5500: 1.662414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 5600: 1.667233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5700: 1.654800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5800: 1.659663 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5900: 1.660582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 6000: 1.671689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "fg e  hhheho hhhh h  h hhh h  hhoh   hh hh h   heehhhhh    h   h hhse h hh eh hhh\n",
      "pd  aeaaeaeaeeaeeeeeaeaoeaeaioaoeeeoooaooieoiioooooeaieeaeoeoooeoooa oeeoooooeoao\n",
      " unnsspswsssnnsncnsnstpnssssssnnsgnsssnnscnnnssnmnnpswsnnknssnslsbnnnpnsnsnnwscsp\n",
      "ihao  aaeaaa  a a aaa  a    aaaa  a aa  ea   a aaaaae     aan   aa  a aac nara   \n",
      "vm  e  aeo    aa e   aeeae   aa     aa a eseaee i     eea  esea s me  ae sa ee ao\n",
      "================================================================================\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 6100: 1.674793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 6200: 1.687196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 6300: 1.694800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6400: 1.668747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6500: 1.634569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 6600: 1.645372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6700: 1.658347 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 6800: 1.664987 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6900: 1.665253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7000: 1.645361 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "ss isei eiiiieieeii  i ii i io iii eioiiieiii ef ae iiiei  ieaeieiiiiiie iuaiiiii\n",
      "illdiluyauudadadteyudiehrdayyelliauayiyr ds yaeyyy  uytaeuyiruttilrdayyuteeu uttu\n",
      "ux                        i     e              i            i                    \n",
      "cbilieeileaaibeebnlbmyylyyismbnlayiiilbyiiaeayueieililnriineleua aybyybiiiiioeola\n",
      "psi  oe             i e             yi     o      e                i         o   \n",
      "================================================================================\n",
      "Validation set perplexity: 7.70\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[0].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[1].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input0: b[0].argmax(axis=1),\n",
    "                                              sample_input1: b[1].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c:** Add dropout layer to output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "(64, 512)\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 128\n",
    "embedding_size = 256\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "     # Input data.\n",
    "    train_data = []\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(\n",
    "            tf.placeholder(tf.int32, shape = [batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    for index in range(num_unrollings - 1):\n",
    "        embed_first_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index])\n",
    "        embed_second_symb = tf.nn.embedding_lookup(embeddings, train_inputs[index + 1])\n",
    "        embed = tf.concat([embed_first_symb, embed_second_symb], 1)\n",
    "        print(embed.shape)\n",
    "        embeds.append(embed)\n",
    "        \n",
    "    # Parameters:\n",
    "    # Gates: input, memory, forget, output.\n",
    "    x = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # State\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    #bias\n",
    "    bias = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #extract matmul part into the single variable\n",
    "        matmul_part = tf.matmul(i, x)+ tf.matmul(o, m) + bias\n",
    "        input_gate = tf.sigmoid(matmul_part[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmul_part[:, num_nodes : 2 * num_nodes])\n",
    "        update = matmul_part[:, 2 * num_nodes: 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmul_part[:, 3 * num_nodes: 4 * num_nodes])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.5)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        input_drop = tf.nn.dropout(output, 0.5)\n",
    "        outputs.append(output)\n",
    "\n",
    "    print(len(embeds))\n",
    "    print(len(train_labels))\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input0 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    sample_input1 = (tf.placeholder(tf.int32, shape=[1]))\n",
    "    \n",
    "    embed1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input0), [1, -1])\n",
    "    embed2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input1), [1, -1])\n",
    "    sample_input_embed = tf.concat([embed1, embed2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibe']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.346701 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.41\n",
      "================================================================================\n",
      "pjctcl tf lnj btsfz a epgn cuojtdaecx yoa ateq q tepeihvtdv i  smieertby sa k his\n",
      "hmzoreef pex  ea  th sa dot qeita l cton  ul tq ek oufmzqk chyeieblbirhteg zhq zb\n",
      "gteca fbv oefdnm jtpje lei htiochtg   spdo  uznq itq e   aedq iecghat l obfhufnt \n",
      "ifjcbvetkzyctlye qtmx ntn a n ae w ugekmzmh yi ae i ra esul mgeiatvecbitoo netdpy\n",
      "jn  na flfix g o sss zex cn jie ke   dn heredy n hiyiqeo g ater r ior ewljw o tv \n",
      "================================================================================\n",
      "Validation set perplexity: 36.76\n",
      "Average loss at step 100: 2.508614 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.59\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 200: 2.208066 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300: 2.113411 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 400: 2.063644 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 500: 2.052137 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 600: 2.043986 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 700: 2.012513 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 800: 1.981432 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 900: 1.973458 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 1000: 1.957001 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "================================================================================\n",
      "rmaesd eenfeogli nse iarnutnudaizvdeu trwiav efsr atc oacnretdien ikmepdiegtu nad\n",
      "i feen kaei vbolracbrlien odn isgtai ft hyo tfeunecdl ot hsa se lsotv asq ubalbie\n",
      "knsoelnignu bozucceosc okvre mc liloinses od etdainlee ntaevnadigbhrsoc hee atre \n",
      "th en itlye xaa loev eosm imloinx unadi npoei nn iism usrisniucdhirsa mmaol eitse\n",
      "vzieeni sesr uasr ufle ionde ohrl easmtaen iwtohruimeb ym amyi nxeprrodn inevnadw\n",
      "================================================================================\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 1100: 1.951238 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 1200: 1.950721 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 1300: 1.928550 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 1400: 1.922142 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 1500: 1.917013 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1600: 1.911437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 1700: 1.915550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 1800: 1.930586 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 1900: 1.900094 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2000: 1.898056 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "ai xrhisoocnednasr ewli nt hgt twh efciocse att hookwiall oc hfetde iinnet hev am\n",
      "dizse  acclroprhil ittetso nse vto ftaintetrernadnrtatns etpihsa  hmobte attl utr\n",
      "kk ist hte rte xsos itcoatwe hhay epradt hirlsa hu nre ifrosmya srh apne gorryoet\n",
      "yf ovle xtewrii tsi nucnl iont ocuttean ifvo ftei  itoe atlii nsacth iegtee xxeod\n",
      "riatt hpernyi sbesg ipnr uorwiiaanc hnetrhaets mtesv osasr own aztiiantesf itoer \n",
      "================================================================================\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 2100: 1.906625 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 2200: 1.908398 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 2300: 1.897212 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 2400: 1.887619 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 2500: 1.869945 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 2600: 1.869007 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 2700: 1.878028 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 2800: 1.880063 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2900: 1.870763 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 3000: 1.851888 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "================================================================================\n",
      "fhe nadhi  ozuhp en imneddu tsi tsoepre viebsl imlairnya nto lbernedr eodf itn ea\n",
      "wreamrai nodniagrteulelsiessoi nbemi tse ihrese aarnec ojhe ivno natceed efl asb \n",
      "yu nfar eodmeaxm od ifsee itcialrya gaunkrenroe eedtaevre ibeotrhee iptiso st hia\n",
      "czaec ohv iodia tkh at hwo ngee irnedl os tcac wo nteedn hte iwtee tlelsy esree x\n",
      "kti ntorlialsa mmoalrieg evreisat hae nti nadt ha nadyi no fo nw ifremr ecat  obr\n",
      "================================================================================\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 3100: 1.866997 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 3200: 1.880433 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 3300: 1.848115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 3400: 1.841124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 3500: 1.851641 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 3600: 1.846148 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 3700: 1.835299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 3800: 1.835681 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 3900: 1.855404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 4000: 1.860406 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "================================================================================\n",
      "qgusa nc asnh aosr etrhaolwe nqeunzdae avlietd esrhe ifvtuhses cmoartfy eivle ico\n",
      "z efd iegn acmroycnetde lovtae pmaetdaedr emnes uun etluast hcia  btaes pmlal ii \n",
      "jlee nnedrsbhte mapnhiasr esv epripml oisc etai ftk mablilse corcuesd  pecsutrehr\n",
      "su sse qsuidtse eotpiast esdhs ejcetnhion  oinnedr eksihl iegdg een esrhaelrya ro\n",
      "djeen ot efnrmiasn ec hha lte lberta nfersa tth wse xnudae ipna nveenmoernetd isn\n",
      "================================================================================\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 4100: 1.841241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 4200: 1.840490 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 4300: 1.857821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 4400: 1.842521 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.98\n",
      "Average loss at step 4500: 1.835488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 4600: 1.827065 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 4700: 1.832611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 4800: 1.823246 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 4900: 1.801606 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 5000: 1.812211 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "================================================================================\n",
      "qbuye ahme ign opro ftiye vaendienw odde icsu st hoa sw iwto ftoll owrasm ot hte \n",
      "rwio ntisn ot hu soinneu sc oofuu sicakqiut hse ast ofuasn eavliess ofl ocmebye a\n",
      "wkaintuim eedl icloedl odn iocmt oeu paprtohsi slhasl iutsi sesr eses  hoemn iisl\n",
      "zues fmleisv eprotrhee xfulisoe vlaclhya rwiesr ogyn itpetn ot heamtohlu sbeedcet\n",
      "gxei st oersalcetrhee  sa  cmobc fb litohlmae agsy otghonngolpalcoest ot ha cakbl\n",
      "================================================================================\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 5100: 1.772910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 5200: 1.768373 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 5300: 1.785816 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 5400: 1.781310 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 5500: 1.779623 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 5600: 1.800253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 5700: 1.774873 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5800: 1.802613 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5900: 1.775997 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 6000: 1.791908 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "================================================================================\n",
      "kjwe itse taersa pfuas osl issopoh itohrize ihg rtoalwe iparp rayd eotra soi novc\n",
      "le rorwosn otb ya  yceh ecnernoosu nnionmc lpey oanb yo vt wo feedriesgor osnr ip\n",
      "njcet haesd ymaes qaurainoem odui nass bss  itae idresutr uvresttes pto ftunadye \n",
      "sr afroms tv ailteha lelye nt wo ffatra  cihs  isd adl eposlhoeu ntses eianlde in\n",
      "uosm ritaianl idtryi si st haersatnetde rasimt wy usc abpr eh etc lhi tpipei nb e\n",
      "================================================================================\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 6100: 1.764456 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 6200: 1.775840 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 6300: 1.777564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6400: 1.786593 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6500: 1.757449 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 6600: 1.751987 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6700: 1.760113 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 6800: 1.792958 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 6900: 1.802140 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 7000: 1.774216 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "vraerta lpealwyr otu pnre xi nveernacnigsi nw ide icaesn ibo anl lhitn uupb ntoec\n",
      "gsec wbalye nliot wso ntetdo fafist  esv  oplleyd ubraalns eartaito fm argihnses \n",
      "ci sashtehrveendi suidnse iyn een innigss yf rsa palniganl eo bbientee iomnetde x\n",
      "gihn moimtmrainse uwsi ts  abmla nb ran essc ehre iarmni tfwe irny ot haardyr esv\n",
      "dwe isnuim eisn alse innec akl iiarle ptaiml owrlefse emr esnet hp rdoeudnetdoyr \n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings - 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i].argmax(axis = 1)\n",
    "      feed_dict[train_inputs[i + 1]] = batches[i + 1].argmax(axis = 1)\n",
    "      feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        \n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feeds = [sample(random_distribution()),sample(random_distribution())]\n",
    "          sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input0: feeds[-1].argmax(axis = 1),\n",
    "                                                 sample_input1: feeds[-2].argmax(axis = 1)})\n",
    "            feed = sample(prediction)\n",
    "            #print(\"prediction: {0} \\n feed {1}:{2} \\n\".format(prediction, feed, characters(feed)))\n",
    "            sentence += characters(feed)[0]\n",
    "            feeds.append(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input0: b[0].argmax(axis=1),\n",
    "                                              sample_input1: b[1].argmax(axis=1)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adic and om', 'ks journals', 'g inn g is ', 'itions were', 'antiprism w', 'ragon ball ', ' for his sh', 'eference in', 'dclapped th', 'prettanikee', 'ng some str', 'le on top o', ' as the tre', 'anks buildi', ' lead in a ', ' nine nine ', 'g in the u ', 'g the tourn', ' arcology n', 'unlike toda', 'mosomes vol', 'ial plasma ', 'h the use o', 'six zero s ', 'rgagni ital', 'n the late ', 'he victim c', 'an be const', 'ism secular', ' case the s', ' maker leo ', 'ended impro', 'ut failed t', 'sed on the ', 'ed at sea s', 'ense that o', 'f march fiv', 'tor kenneth', 'horror movi', 'es use the ', 'percard as ', 'ld reaching', 'was soon ca', 'g monster a', 'overnments ', 'ing machine', 'cannot say ', 'to young wh', ' eight four', 'author john', 'learly seen', 'ed to the a', 'cement of e', 'ion the rev', 'neural netw', 'press that ', 'underground', 'o this text', ' since one ', 'her modes g', ' for other ', ' a role dra', ' zero four ', 'ight zero s']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
